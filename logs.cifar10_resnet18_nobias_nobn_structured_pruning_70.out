/home/gvignen/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
------- Setting up parameters -------
dumping parameters at  /home/gvignen/otfusion/exp_cifar10_resnet18_nobias_nobn_structured_pruning_70/configurations
The parameters are: 
 Namespace(n_epochs=300, batch_size_train=64, batch_size_test=1000, learning_rate=0.01, momentum=0.5, log_interval=100, to_download=True, disable_bias=True, dataset='Cifar10', num_models=2, model_name='resnet18_nobias_nobn', config_file=None, config_dir='/home/gvignen/otfusion/exp_cifar10_resnet18_nobias_nobn_structured_pruning_70/configurations', num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, sweep_id=90, gpu_id=0, skip_last_layer=False, skip_last_layer_type='average', debug=False, cifar_style_data=False, activation_histograms=True, act_num_samples=200, softmax_temperature=1, activation_mode='raw', options_type='generic', deprecated=None, save_result_file='sample_cifar10_resnet18_nobias_nobn_structured_pruning_70.csv', sweep_name='exp_cifar10_resnet18_nobias_nobn_structured_pruning_70', reg=0.01, reg_m=0.001, ground_metric='euclidean', ground_metric_normalize='none', not_squared=True, clip_gm=False, clip_min=0, clip_max=5, tmap_stats=False, ensemble_step=0.5, ground_metric_eff=True, retrain=300, retrain_lr_decay=-1, retrain_lr_decay_factor=None, retrain_lr_decay_epochs=None, retrain_avg_only=False, retrain_geometric_only=True, load_models='./resnet_models/', ckpt_type='best', recheck_cifar=True, recheck_acc=True, eval_aligned=False, enable_dropout=False, dump_model=False, dump_final_models=False, correction=True, activation_seed=21, weight_stats=True, sinkhorn_type='normal', geom_ensemble_type='acts', act_bug=False, standardize_acts=False, transform_acts=False, center_acts=False, prelu_acts=True, pool_acts=False, pool_relu=False, normalize_acts=False, normalize_wts=False, gromov=False, gromov_loss='square_loss', tensorboard_root='./tensorboard', tensorboard=False, same_model=-1, dist_normalize=False, update_acts=False, past_correction=True, partial_reshape=False, choice='0 2 4 6 8', diff_init=False, partition_type='labels', personal_class_idx=9, partition_dataloader=-1, personal_split_frac=0.1, exact=True, skip_personal_idx=False, prediction_wts=False, width_ratio=1, proper_marginals=False, retrain_seed=-1, no_random_trainloaders=False, reinit_trainloaders=False, second_model_name=None, print_distances=False, deterministic=False, skip_retrain=-1, importance=None, unbalanced=False, temperature=20, alpha=0.7, dist_epochs=60, handle_skips=True, prune=True, prune_frac=0.7, prune_type='structured', experiment_name='cifar10_resnet18_nobias_nobn_structured_pruning_70', timestamp='2024-01-06_15-21-56_440541', rootdir='/home/gvignen/otfusion/exp_cifar10_resnet18_nobias_nobn_structured_pruning_70', baseroot='/home/gvignen/otfusion', result_dir='/home/gvignen/otfusion/exp_cifar10_resnet18_nobias_nobn_structured_pruning_70/results', exp_name='exp_cifar10_resnet18_nobias_nobn_structured_pruning_70', csv_dir='/home/gvignen/otfusion/exp_cifar10_resnet18_nobias_nobn_structured_pruning_70/csv')
refactored get_config
------- Loading pre-trained models -------
loading cifar10 dataloaders
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
loading model with idx 0 and checkpoint_type is best
model parameters are 
 [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]
Loading model at path ./resnet_models/model_0/best.checkpoint which had accuracy 0.9310999816656113 and at epoch 181
loading model with idx 1 and checkpoint_type is best
model parameters are 
 [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]
Loading model at path ./resnet_models/model_1/best.checkpoint which had accuracy 0.9319999837875367 and at epoch 205
Done loading all the models

--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0043, Accuracy: 9311/10000 (93%)


--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0044, Accuracy: 9320/10000 (93%)

Rechecked accuracies are  [93.11, 93.2]
----------Prune the 2 Parent models now---------
---------let's see result after pruning-------------
dict_keys([])
---------let's see result after pruning-------------
dict_keys([])
--------Rechecking accuracies again!--------

--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0228, Accuracy: 1233/10000 (12%)

----- Saving Pruned model0-------

--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0232, Accuracy: 1798/10000 (18%)

----- Saving Pruned model1-------
Rechecked accuracies are  [12.33, 17.98]
layer conv1.weight has #params  1728
layer layer1.0.conv1.weight has #params  36864
layer layer1.0.conv2.weight has #params  36864
layer layer1.1.conv1.weight has #params  36864
layer layer1.1.conv2.weight has #params  36864
layer layer2.0.conv1.weight has #params  73728
layer layer2.0.conv2.weight has #params  147456
layer layer2.0.shortcut.0.weight has #params  8192
layer layer2.1.conv1.weight has #params  147456
layer layer2.1.conv2.weight has #params  147456
layer layer3.0.conv1.weight has #params  294912
layer layer3.0.conv2.weight has #params  589824
layer layer3.0.shortcut.0.weight has #params  32768
layer layer3.1.conv1.weight has #params  589824
layer layer3.1.conv2.weight has #params  589824
layer layer4.0.conv1.weight has #params  1179648
layer layer4.0.conv2.weight has #params  2359296
layer layer4.0.shortcut.0.weight has #params  131072
layer layer4.1.conv1.weight has #params  2359296
layer layer4.1.conv2.weight has #params  2359296
layer linear.weight has #params  5120
Activation Timer start
Files already downloaded and verified
Files already downloaded and verified
excluded
set forward hook for layer named:  conv1
this was continued,  bn1
this was continued,  layer1
this was continued,  layer1.0
set forward hook for layer named:  layer1.0.conv1
this was continued,  layer1.0.bn1
set forward hook for layer named:  layer1.0.conv2
this was continued,  layer1.0.shortcut
this was continued,  layer1.1
set forward hook for layer named:  layer1.1.conv1
this was continued,  layer1.1.bn1
set forward hook for layer named:  layer1.1.conv2
this was continued,  layer1.1.shortcut
this was continued,  layer2
this was continued,  layer2.0
set forward hook for layer named:  layer2.0.conv1
this was continued,  layer2.0.bn1
set forward hook for layer named:  layer2.0.conv2
this was continued,  layer2.0.shortcut
set forward hook for layer named:  layer2.0.shortcut.0
this was continued,  layer2.0.shortcut.1
this was continued,  layer2.1
set forward hook for layer named:  layer2.1.conv1
this was continued,  layer2.1.bn1
set forward hook for layer named:  layer2.1.conv2
this was continued,  layer2.1.shortcut
this was continued,  layer3
this was continued,  layer3.0
set forward hook for layer named:  layer3.0.conv1
this was continued,  layer3.0.bn1
set forward hook for layer named:  layer3.0.conv2
this was continued,  layer3.0.shortcut
set forward hook for layer named:  layer3.0.shortcut.0
this was continued,  layer3.0.shortcut.1
this was continued,  layer3.1
set forward hook for layer named:  layer3.1.conv1
this was continued,  layer3.1.bn1
set forward hook for layer named:  layer3.1.conv2
this was continued,  layer3.1.shortcut
this was continued,  layer4
this was continued,  layer4.0
set forward hook for layer named:  layer4.0.conv1
this was continued,  layer4.0.bn1
set forward hook for layer named:  layer4.0.conv2
this was continued,  layer4.0.shortcut
set forward hook for layer named:  layer4.0.shortcut.0
this was continued,  layer4.0.shortcut.1
this was continued,  layer4.1
set forward hook for layer named:  layer4.1.conv1
this was continued,  layer4.1.bn1
set forward hook for layer named:  layer4.1.conv2
this was continued,  layer4.1.shortcut
set forward hook for layer named:  linear
excluded
set forward hook for layer named:  conv1
this was continued,  bn1
this was continued,  layer1
this was continued,  layer1.0
set forward hook for layer named:  layer1.0.conv1
this was continued,  layer1.0.bn1
set forward hook for layer named:  layer1.0.conv2
this was continued,  layer1.0.shortcut
this was continued,  layer1.1
set forward hook for layer named:  layer1.1.conv1
this was continued,  layer1.1.bn1
set forward hook for layer named:  layer1.1.conv2
this was continued,  layer1.1.shortcut
this was continued,  layer2
this was continued,  layer2.0
set forward hook for layer named:  layer2.0.conv1
this was continued,  layer2.0.bn1
set forward hook for layer named:  layer2.0.conv2
this was continued,  layer2.0.shortcut
set forward hook for layer named:  layer2.0.shortcut.0
this was continued,  layer2.0.shortcut.1
this was continued,  layer2.1
set forward hook for layer named:  layer2.1.conv1
this was continued,  layer2.1.bn1
set forward hook for layer named:  layer2.1.conv2
this was continued,  layer2.1.shortcut
this was continued,  layer3
this was continued,  layer3.0
set forward hook for layer named:  layer3.0.conv1
this was continued,  layer3.0.bn1
set forward hook for layer named:  layer3.0.conv2
this was continued,  layer3.0.shortcut
set forward hook for layer named:  layer3.0.shortcut.0
this was continued,  layer3.0.shortcut.1
this was continued,  layer3.1
set forward hook for layer named:  layer3.1.conv1
this was continued,  layer3.1.bn1
set forward hook for layer named:  layer3.1.conv2
this was continued,  layer3.1.shortcut
this was continued,  layer4
this was continued,  layer4.0
set forward hook for layer named:  layer4.0.conv1
this was continued,  layer4.0.bn1
set forward hook for layer named:  layer4.0.conv2
this was continued,  layer4.0.shortcut
set forward hook for layer named:  layer4.0.shortcut.0
this was continued,  layer4.0.shortcut.1
this was continued,  layer4.1
set forward hook for layer named:  layer4.1.conv1
this was continued,  layer4.1.bn1
set forward hook for layer named:  layer4.1.conv2
this was continued,  layer4.1.shortcut
set forward hook for layer named:  linear
num_personal_idx  25
model_name is  resnet18_nobias_nobn
***********
min of act: -19.394960403442383, max: 19.856435775756836, mean: -0.000841317989397794
activations for idx 0 at layer conv1 have the following shape  torch.Size([200, 1, 64, 32, 32])
-----------
***********
min of act: -24.467409133911133, max: 15.29208755493164, mean: -0.45238053798675537
activations for idx 0 at layer layer1.0.conv1 have the following shape  torch.Size([200, 1, 64, 32, 32])
-----------
***********
min of act: -4.8935956954956055, max: 21.673065185546875, mean: 0.10969128459692001
activations for idx 0 at layer layer1.0.conv2 have the following shape  torch.Size([200, 1, 64, 32, 32])
-----------
***********
min of act: -19.705734252929688, max: 14.1870756149292, mean: -0.462127685546875
activations for idx 0 at layer layer1.1.conv1 have the following shape  torch.Size([200, 1, 64, 32, 32])
-----------
***********
min of act: -6.814763069152832, max: 18.827129364013672, mean: 0.09519819170236588
activations for idx 0 at layer layer1.1.conv2 have the following shape  torch.Size([200, 1, 64, 32, 32])
-----------
***********
min of act: -29.69502830505371, max: 26.944673538208008, mean: -0.11881692707538605
activations for idx 0 at layer layer2.0.conv1 have the following shape  torch.Size([200, 1, 128, 16, 16])
-----------
***********
min of act: -35.00494384765625, max: 20.439533233642578, mean: -0.16444171965122223
activations for idx 0 at layer layer2.0.conv2 have the following shape  torch.Size([200, 1, 128, 16, 16])
-----------
***********
min of act: -5.445998668670654, max: 8.100708961486816, mean: 0.02487022802233696
activations for idx 0 at layer layer2.0.shortcut.0 have the following shape  torch.Size([200, 1, 128, 16, 16])
-----------
***********
min of act: -21.8758602142334, max: 10.94388198852539, mean: -0.7160735130310059
activations for idx 0 at layer layer2.1.conv1 have the following shape  torch.Size([200, 1, 128, 16, 16])
-----------
***********
min of act: -4.104300022125244, max: 19.75005531311035, mean: 0.17388296127319336
activations for idx 0 at layer layer2.1.conv2 have the following shape  torch.Size([200, 1, 128, 16, 16])
-----------
***********
min of act: -21.860422134399414, max: 22.735219955444336, mean: -0.3451526165008545
activations for idx 0 at layer layer3.0.conv1 have the following shape  torch.Size([200, 1, 256, 8, 8])
-----------
***********
min of act: -15.870463371276855, max: 12.618986129760742, mean: -0.28414249420166016
activations for idx 0 at layer layer3.0.conv2 have the following shape  torch.Size([200, 1, 256, 8, 8])
-----------
***********
min of act: -6.546900749206543, max: 8.16091537475586, mean: -0.006741006392985582
activations for idx 0 at layer layer3.0.shortcut.0 have the following shape  torch.Size([200, 1, 256, 8, 8])
-----------
***********
min of act: -11.195972442626953, max: 6.166138648986816, mean: -0.4060988426208496
activations for idx 0 at layer layer3.1.conv1 have the following shape  torch.Size([200, 1, 256, 8, 8])
-----------
***********
min of act: -2.387763500213623, max: 12.343515396118164, mean: 0.052060022950172424
activations for idx 0 at layer layer3.1.conv2 have the following shape  torch.Size([200, 1, 256, 8, 8])
-----------
***********
min of act: -7.185918807983398, max: 5.930825710296631, mean: -0.16469314694404602
activations for idx 0 at layer layer4.0.conv1 have the following shape  torch.Size([200, 1, 512, 4, 4])
-----------
***********
min of act: -5.27445125579834, max: 3.59063982963562, mean: -0.1357727199792862
activations for idx 0 at layer layer4.0.conv2 have the following shape  torch.Size([200, 1, 512, 4, 4])
-----------
***********
min of act: -3.583122730255127, max: 3.59063982963562, mean: -0.04101070761680603
activations for idx 0 at layer layer4.0.shortcut.0 have the following shape  torch.Size([200, 1, 512, 4, 4])
-----------
***********
min of act: -2.191981792449951, max: 1.4189960956573486, mean: -0.04434771090745926
activations for idx 0 at layer layer4.1.conv1 have the following shape  torch.Size([200, 1, 512, 4, 4])
-----------
***********
min of act: -0.3482596278190613, max: 3.59063982963562, mean: 0.016560522839426994
activations for idx 0 at layer layer4.1.conv2 have the following shape  torch.Size([200, 1, 512, 4, 4])
-----------
***********
min of act: -1.00503671169281, max: 1.5077471733093262, mean: 0.0006376605597324669
activations for idx 0 at layer linear have the following shape  torch.Size([200, 1, 10])
-----------
***********
min of act: -19.116561889648438, max: 18.00572967529297, mean: -0.003392730839550495
activations for idx 1 at layer conv1 have the following shape  torch.Size([200, 1, 64, 32, 32])
-----------
***********
min of act: -20.105876922607422, max: 9.529386520385742, mean: -0.4398024380207062
activations for idx 1 at layer layer1.0.conv1 have the following shape  torch.Size([200, 1, 64, 32, 32])
-----------
***********
min of act: -5.436816692352295, max: 20.696443557739258, mean: 0.1373615562915802
activations for idx 1 at layer layer1.0.conv2 have the following shape  torch.Size([200, 1, 64, 32, 32])
-----------
***********
min of act: -27.046480178833008, max: 12.49283218383789, mean: -0.5505181550979614
activations for idx 1 at layer layer1.1.conv1 have the following shape  torch.Size([200, 1, 64, 32, 32])
-----------
***********
min of act: -7.377118110656738, max: 17.582286834716797, mean: 0.13041137158870697
activations for idx 1 at layer layer1.1.conv2 have the following shape  torch.Size([200, 1, 64, 32, 32])
-----------
***********
min of act: -32.746376037597656, max: 33.68923568725586, mean: -0.1328447163105011
activations for idx 1 at layer layer2.0.conv1 have the following shape  torch.Size([200, 1, 128, 16, 16])
-----------
***********
min of act: -34.209983825683594, max: 26.182497024536133, mean: -0.001294024521484971
activations for idx 1 at layer layer2.0.conv2 have the following shape  torch.Size([200, 1, 128, 16, 16])
-----------
***********
min of act: -4.849428653717041, max: 10.213042259216309, mean: 0.04624006152153015
activations for idx 1 at layer layer2.0.shortcut.0 have the following shape  torch.Size([200, 1, 128, 16, 16])
-----------
***********
min of act: -34.30434799194336, max: 15.9978666305542, mean: -0.9554839730262756
activations for idx 1 at layer layer2.1.conv1 have the following shape  torch.Size([200, 1, 128, 16, 16])
-----------
***********
min of act: -7.786315441131592, max: 26.157888412475586, mean: 0.24192187190055847
activations for idx 1 at layer layer2.1.conv2 have the following shape  torch.Size([200, 1, 128, 16, 16])
-----------
***********
min of act: -31.383352279663086, max: 22.064437866210938, mean: -0.3917657434940338
activations for idx 1 at layer layer3.0.conv1 have the following shape  torch.Size([200, 1, 256, 8, 8])
-----------
***********
min of act: -18.71464729309082, max: 20.430633544921875, mean: -0.25905370712280273
activations for idx 1 at layer layer3.0.conv2 have the following shape  torch.Size([200, 1, 256, 8, 8])
-----------
***********
min of act: -9.568388938903809, max: 12.537215232849121, mean: 0.01821373589336872
activations for idx 1 at layer layer3.0.shortcut.0 have the following shape  torch.Size([200, 1, 256, 8, 8])
-----------
***********
min of act: -16.007003784179688, max: 7.177618026733398, mean: -0.659963846206665
activations for idx 1 at layer layer3.1.conv1 have the following shape  torch.Size([200, 1, 256, 8, 8])
-----------
***********
min of act: -2.6952528953552246, max: 19.885862350463867, mean: 0.12780345976352692
activations for idx 1 at layer layer3.1.conv2 have the following shape  torch.Size([200, 1, 256, 8, 8])
-----------
***********
min of act: -12.501812934875488, max: 10.198038101196289, mean: -0.2021525651216507
activations for idx 1 at layer layer4.0.conv1 have the following shape  torch.Size([200, 1, 512, 4, 4])
-----------
***********
min of act: -8.809429168701172, max: 6.481224060058594, mean: -0.20369744300842285
activations for idx 1 at layer layer4.0.conv2 have the following shape  torch.Size([200, 1, 512, 4, 4])
-----------
***********
min of act: -3.4746503829956055, max: 6.481224060058594, mean: -0.057790230959653854
activations for idx 1 at layer layer4.0.shortcut.0 have the following shape  torch.Size([200, 1, 512, 4, 4])
-----------
***********
min of act: -3.393627405166626, max: 2.1671409606933594, mean: -0.08643773198127747
activations for idx 1 at layer layer4.1.conv1 have the following shape  torch.Size([200, 1, 512, 4, 4])
-----------
***********
min of act: -0.6196317076683044, max: 6.481224060058594, mean: 0.02994523011147976
activations for idx 1 at layer layer4.1.conv2 have the following shape  torch.Size([200, 1, 512, 4, 4])
-----------
***********
min of act: -1.6704914569854736, max: 1.5467908382415771, mean: -0.0009741295361891389
activations for idx 1 at layer linear have the following shape  torch.Size([200, 1, 10])
-----------
Activation Timer ends
------- Geometric Ensembling -------
Timer start

--------------- At layer index 0 ------------- 
 
Previous layer shape is  None
let's see the difference in layer names conv1 conv1
torch.Size([200, 1, 64, 32, 32]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer conv1.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 13.10551643371582, Mean : 4.208578109741211, Min : 0.9274705648422241, Std: 2.0215132236480713
returns a uniform measure of cardinality:  64
returns a uniform measure of cardinality:  64
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  204800
# of ground metric features in 0 is   204800
# of ground metric features in 1 is   204800
ground metric (m0) is  tensor([[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [469.7942, 469.7942, 469.7942,  ..., 469.7942, 469.7942, 469.7942],
        [672.3196, 672.3196, 672.3196,  ..., 672.3196, 672.3196, 672.3196],
        ...,
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],
       device='cuda:0')
shape of T_var is  torch.Size([64, 64])
T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        ...,
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],
       device='cuda:0')
T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403392791748047 
Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')
Here, trace is 2.9999806880950928 and matrix sum is 63.99958801269531 
Shape of aligned wt is  torch.Size([64, 3, 9])
Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])

--------------- At layer index 1 ------------- 
 
Previous layer shape is  torch.Size([64, 3, 3, 3])
let's see the difference in layer names layer1.0.conv1 layer1.0.conv1
torch.Size([200, 1, 64, 32, 32]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer1.0.conv1.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 15.640549659729004, Mean : 5.633087158203125, Min : 1.392107605934143, Std: 2.360412120819092
shape of layer: model 0 torch.Size([64, 64, 9])
shape of layer: model 1 torch.Size([64, 64, 9])
shape of activations: model 0 torch.Size([64, 32, 32, 200])
shape of activations: model 1 torch.Size([64, 32, 32, 200])
shape of previous transport map torch.Size([64, 64])
doing nothing for skips
returns a uniform measure of cardinality:  64
returns a uniform measure of cardinality:  64
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  204800
# of ground metric features in 0 is   204800
# of ground metric features in 1 is   204800
ground metric (m0) is  tensor([[   0.0000,    0.0000, 1299.7815,  ...,  897.0592, 1334.2622,
          801.8998],
        [   0.0000,    0.0000, 1299.7815,  ...,  897.0592, 1334.2622,
          801.8998],
        [   0.0000,    0.0000, 1299.7815,  ...,  897.0592, 1334.2622,
          801.8998],
        ...,
        [1072.8689, 1072.8689,  834.7766,  ..., 1090.6881, 1089.6201,
          928.2966],
        [   0.0000,    0.0000, 1299.7815,  ...,  897.0592, 1334.2622,
          801.8998],
        [   0.0000,    0.0000, 1299.7815,  ...,  897.0592, 1334.2622,
          801.8998]], device='cuda:0')
shape of T_var is  torch.Size([64, 64])
T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        ...,
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],
       device='cuda:0')
T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403393536806107 
Ratio of trace to the matrix sum:  tensor(0.0156, device='cuda:0')
Here, trace is 0.9999935626983643 and matrix sum is 63.99958801269531 
Shape of aligned wt is  torch.Size([64, 64, 9])
Shape of fc_layer0_weight_data is  torch.Size([64, 64, 9])

--------------- At layer index 2 ------------- 
 
Previous layer shape is  torch.Size([64, 64, 3, 3])
let's see the difference in layer names layer1.0.conv2 layer1.0.conv2
torch.Size([200, 1, 64, 32, 32]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer1.0.conv2.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 10.63819408416748, Mean : 2.903735399246216, Min : 0.4490102529525757, Std: 1.722285270690918
shape of layer: model 0 torch.Size([64, 64, 9])
shape of layer: model 1 torch.Size([64, 64, 9])
shape of activations: model 0 torch.Size([64, 32, 32, 200])
shape of activations: model 1 torch.Size([64, 32, 32, 200])
shape of previous transport map torch.Size([64, 64])
doing nothing for skips
returns a uniform measure of cardinality:  64
returns a uniform measure of cardinality:  64
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  204800
# of ground metric features in 0 is   204800
# of ground metric features in 1 is   204800
ground metric (m0) is  tensor([[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [369.4266, 369.4266, 369.4266,  ..., 369.4266, 369.4266, 369.4266],
        [460.9471, 460.9471, 460.9471,  ..., 460.9471, 460.9471, 460.9471],
        ...,
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [ 99.1236,  99.1236,  99.1236,  ...,  99.1236,  99.1236,  99.1236],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],
       device='cuda:0')
shape of T_var is  torch.Size([64, 64])
T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0156],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        ...,
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],
       device='cuda:0')
T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403392791748047 
Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')
Here, trace is 2.9999806880950928 and matrix sum is 63.99958801269531 
Shape of aligned wt is  torch.Size([64, 64, 9])
Shape of fc_layer0_weight_data is  torch.Size([64, 64, 9])

--------------- At layer index 3 ------------- 
 
Previous layer shape is  torch.Size([64, 64, 3, 3])
let's see the difference in layer names layer1.1.conv1 layer1.1.conv1
torch.Size([200, 1, 64, 32, 32]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer1.1.conv1.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 16.406471252441406, Mean : 6.1626739501953125, Min : 1.5762616395950317, Std: 2.4972341060638428
shape of layer: model 0 torch.Size([64, 64, 9])
shape of layer: model 1 torch.Size([64, 64, 9])
shape of activations: model 0 torch.Size([64, 32, 32, 200])
shape of activations: model 1 torch.Size([64, 32, 32, 200])
shape of previous transport map torch.Size([64, 64])
doing nothing for skips
returns a uniform measure of cardinality:  64
returns a uniform measure of cardinality:  64
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  204800
# of ground metric features in 0 is   204800
# of ground metric features in 1 is   204800
ground metric (m0) is  tensor([[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        ...,
        [785.0659, 785.0659, 785.0659,  ..., 785.0659, 785.0659, 785.0659],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],
       device='cuda:0')
shape of T_var is  torch.Size([64, 64])
T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0156],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        ...,
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],
       device='cuda:0')
T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000],
        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403393536806107 
Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')
Here, trace is 0.0 and matrix sum is 63.99958801269531 
Shape of aligned wt is  torch.Size([64, 64, 9])
Shape of fc_layer0_weight_data is  torch.Size([64, 64, 9])

--------------- At layer index 4 ------------- 
 
Previous layer shape is  torch.Size([64, 64, 3, 3])
let's see the difference in layer names layer1.1.conv2 layer1.1.conv2
torch.Size([200, 1, 64, 32, 32]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer1.1.conv2.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 10.449968338012695, Mean : 2.9233756065368652, Min : 0.4564881920814514, Std: 1.6980966329574585
shape of layer: model 0 torch.Size([64, 64, 9])
shape of layer: model 1 torch.Size([64, 64, 9])
shape of activations: model 0 torch.Size([64, 32, 32, 200])
shape of activations: model 1 torch.Size([64, 32, 32, 200])
shape of previous transport map torch.Size([64, 64])
doing nothing for skips
returns a uniform measure of cardinality:  64
returns a uniform measure of cardinality:  64
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  204800
# of ground metric features in 0 is   204800
# of ground metric features in 1 is   204800
ground metric (m0) is  tensor([[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [369.4266, 369.4266, 369.4266,  ..., 369.4266, 369.4266, 369.4266],
        [443.1281, 443.1281, 443.1281,  ..., 443.1281, 443.1281, 443.1281],
        ...,
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [ 57.6449,  57.6449,  57.6449,  ...,  57.6449,  57.6449,  57.6449],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],
       device='cuda:0')
shape of T_var is  torch.Size([64, 64])
T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0156, 0.0000]],
       device='cuda:0')
marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        ...,
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],
        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],
       device='cuda:0')
T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000]],
       device='cuda:0')
T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403393536806107 
Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')
Here, trace is 2.9999806880950928 and matrix sum is 63.99958801269531 
Shape of aligned wt is  torch.Size([64, 64, 9])
Shape of fc_layer0_weight_data is  torch.Size([64, 64, 9])

--------------- At layer index 5 ------------- 
 
Previous layer shape is  torch.Size([64, 64, 3, 3])
let's see the difference in layer names layer2.0.conv1 layer2.0.conv1
torch.Size([200, 1, 128, 16, 16]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer2.0.conv1.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 11.819778442382812, Mean : 3.9978904724121094, Min : 0.800420880317688, Std: 1.9538341760635376
shape of layer: model 0 torch.Size([128, 64, 9])
shape of layer: model 1 torch.Size([128, 64, 9])
shape of activations: model 0 torch.Size([128, 16, 16, 200])
shape of activations: model 1 torch.Size([128, 16, 16, 200])
shape of previous transport map torch.Size([64, 64])
saved skip T_var at layer 5 with shape torch.Size([128, 64, 3, 3])
shape of previous transport map now is torch.Size([64, 64])
returns a uniform measure of cardinality:  128
returns a uniform measure of cardinality:  128
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  51200
# of ground metric features in 0 is   51200
# of ground metric features in 1 is   51200
ground metric (m0) is  tensor([[488.7576, 409.5777,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [488.7576, 409.5777,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [488.7576, 409.5777,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        ...,
        [488.7576, 409.5777,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [488.7576, 409.5777,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [488.7576, 409.5777,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],
       device='cuda:0')
shape of T_var is  torch.Size([128, 128])
T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        ...,
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],
       device='cuda:0')
T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 
Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')
Here, trace is 0.9999872446060181 and matrix sum is 127.99836730957031 
Shape of aligned wt is  torch.Size([128, 64, 9])
Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])

--------------- At layer index 6 ------------- 
 
Previous layer shape is  torch.Size([128, 64, 3, 3])
let's see the difference in layer names layer2.0.conv2 layer2.0.conv2
torch.Size([200, 1, 128, 16, 16]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer2.0.conv2.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 14.345860481262207, Mean : 5.530034065246582, Min : 1.2556098699569702, Std: 2.3902370929718018
shape of layer: model 0 torch.Size([128, 128, 9])
shape of layer: model 1 torch.Size([128, 128, 9])
shape of activations: model 0 torch.Size([128, 16, 16, 200])
shape of activations: model 1 torch.Size([128, 16, 16, 200])
shape of previous transport map torch.Size([128, 128])
doing nothing for skips
returns a uniform measure of cardinality:  128
returns a uniform measure of cardinality:  128
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  51200
# of ground metric features in 0 is   51200
# of ground metric features in 1 is   51200
ground metric (m0) is  tensor([[500.1804,  39.6282, 478.1358,  ..., 153.1831,  39.6282,  72.7873],
        [513.4969,   0.0000, 479.0710,  ..., 121.9881,   0.0000,  54.6482],
        [611.5154, 328.4106, 499.0045,  ..., 384.7408, 328.4106, 344.0376],
        ...,
        [726.6454, 468.6581, 708.3759,  ..., 479.3816, 468.6581, 474.6044],
        [513.4969,   0.0000, 479.0710,  ..., 121.9881,   0.0000,  54.6482],
        [721.0419, 450.7620, 703.7440,  ..., 441.1762, 450.7620, 452.9366]],
       device='cuda:0')
shape of T_var is  torch.Size([128, 128])
T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        ...,
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],
       device='cuda:0')
T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 
Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')
Here, trace is 0.9999872446060181 and matrix sum is 127.99836730957031 
Shape of aligned wt is  torch.Size([128, 128, 9])
Shape of fc_layer0_weight_data is  torch.Size([128, 128, 9])

--------------- At layer index 7 ------------- 
 
Previous layer shape is  torch.Size([128, 128, 3, 3])
let's see the difference in layer names layer2.0.shortcut.0 layer2.0.shortcut.0
torch.Size([200, 1, 128, 16, 16]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer2.0.shortcut.0.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 2.711927890777588, Mean : 0.8683279752731323, Min : 0.19502116739749908, Std: 0.41919654607772827
shape of layer: model 0 torch.Size([128, 64, 1])
shape of layer: model 1 torch.Size([128, 64, 1])
shape of activations: model 0 torch.Size([128, 16, 16, 200])
shape of activations: model 1 torch.Size([128, 16, 16, 200])
shape of previous transport map torch.Size([128, 128])
utilizing skip T_var saved from layer layer 5 with shape torch.Size([64, 64])
shape of previous transport map now is torch.Size([64, 64])
returns a uniform measure of cardinality:  128
returns a uniform measure of cardinality:  128
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  51200
# of ground metric features in 0 is   51200
# of ground metric features in 1 is   51200
ground metric (m0) is  tensor([[ 39.6282,  39.6282, 161.9755,  ..., 153.1831,  39.6282,  72.7873],
        [  0.0000,   0.0000, 172.3240,  ..., 121.9881,   0.0000,  54.6482],
        [  0.0000,   0.0000, 172.3240,  ..., 121.9881,   0.0000,  54.6482],
        ...,
        [111.8132, 111.8132, 226.6839,  ..., 206.5353, 111.8132, 127.7626],
        [  0.0000,   0.0000, 172.3240,  ..., 121.9881,   0.0000,  54.6482],
        [  0.0000,   0.0000, 172.3240,  ..., 121.9881,   0.0000,  54.6482]],
       device='cuda:0')
shape of T_var is  torch.Size([128, 128])
T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        ...,
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],
       device='cuda:0')
T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 
Ratio of trace to the matrix sum:  tensor(0.0156, device='cuda:0')
Here, trace is 1.9999744892120361 and matrix sum is 127.99836730957031 
Shape of aligned wt is  torch.Size([128, 64, 1])
Shape of fc_layer0_weight_data is  torch.Size([128, 64, 1])

--------------- At layer index 8 ------------- 
 
Previous layer shape is  torch.Size([128, 64, 1, 1])
let's see the difference in layer names layer2.1.conv1 layer2.1.conv1
torch.Size([200, 1, 128, 16, 16]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer2.1.conv1.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 14.414194107055664, Mean : 6.492931842803955, Min : 1.9173412322998047, Std: 2.303649425506592
shape of layer: model 0 torch.Size([128, 128, 9])
shape of layer: model 1 torch.Size([128, 128, 9])
shape of activations: model 0 torch.Size([128, 16, 16, 200])
shape of activations: model 1 torch.Size([128, 16, 16, 200])
shape of previous transport map torch.Size([128, 128])
averaging multiple T_var's
returns a uniform measure of cardinality:  128
returns a uniform measure of cardinality:  128
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  51200
# of ground metric features in 0 is   51200
# of ground metric features in 1 is   51200
ground metric (m0) is  tensor([[ 824.2954,  460.5887,  905.8458,  ...,  460.5887,  460.5887,
          818.9134],
        [ 881.0075,    0.0000, 1100.2144,  ...,    0.0000,    0.0000,
          982.3207],
        [ 881.0075,    0.0000, 1100.2144,  ...,    0.0000,    0.0000,
          982.3207],
        ...,
        [ 881.0075,    0.0000, 1100.2144,  ...,    0.0000,    0.0000,
          982.3207],
        [ 728.3188,  607.9023,  754.7014,  ...,  607.9023,  607.9023,
          701.4899],
        [ 746.7720,  570.6793,  778.4789,  ...,  570.6793,  570.6793,
          694.6132]], device='cuda:0')
shape of T_var is  torch.Size([128, 128])
T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        ...,
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],
       device='cuda:0')
T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 
Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')
Here, trace is 0.9999872446060181 and matrix sum is 127.99836730957031 
Shape of aligned wt is  torch.Size([128, 128, 9])
Shape of fc_layer0_weight_data is  torch.Size([128, 128, 9])

--------------- At layer index 9 ------------- 
 
Previous layer shape is  torch.Size([128, 128, 3, 3])
let's see the difference in layer names layer2.1.conv2 layer2.1.conv2
torch.Size([200, 1, 128, 16, 16]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer2.1.conv2.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 9.479935646057129, Mean : 2.973053455352783, Min : 0.40773433446884155, Std: 1.6585015058517456
shape of layer: model 0 torch.Size([128, 128, 9])
shape of layer: model 1 torch.Size([128, 128, 9])
shape of activations: model 0 torch.Size([128, 16, 16, 200])
shape of activations: model 1 torch.Size([128, 16, 16, 200])
shape of previous transport map torch.Size([128, 128])
doing nothing for skips
returns a uniform measure of cardinality:  128
returns a uniform measure of cardinality:  128
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  51200
# of ground metric features in 0 is   51200
# of ground metric features in 1 is   51200
ground metric (m0) is  tensor([[421.8275,  39.3788, 375.0724,  ...,  42.3807, 127.2493,  47.6592],
        [439.5613,   0.0000, 382.8624,  ...,  15.9429, 101.8309,  37.6941],
        [464.9637, 277.2231, 368.8846,  ..., 277.4717, 321.8892, 276.6639],
        ...,
        [481.5460, 309.5121, 467.8957,  ..., 309.8385, 343.2535, 308.3244],
        [439.5613,   0.0000, 382.8624,  ...,  15.9429, 101.8309,  37.6941],
        [460.9651, 249.8558, 443.1860,  ..., 249.4520, 290.7737, 246.6513]],
       device='cuda:0')
shape of T_var is  torch.Size([128, 128])
T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        ...,
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],
        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],
       device='cuda:0')
T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 
Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')
Here, trace is 0.9999872446060181 and matrix sum is 127.99836730957031 
Shape of aligned wt is  torch.Size([128, 128, 9])
Shape of fc_layer0_weight_data is  torch.Size([128, 128, 9])

--------------- At layer index 10 ------------- 
 
Previous layer shape is  torch.Size([128, 128, 3, 3])
let's see the difference in layer names layer3.0.conv1 layer3.0.conv1
torch.Size([200, 1, 256, 8, 8]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer3.0.conv1.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 9.030806541442871, Mean : 3.5766072273254395, Min : 0.815983772277832, Std: 1.4968793392181396
shape of layer: model 0 torch.Size([256, 128, 9])
shape of layer: model 1 torch.Size([256, 128, 9])
shape of activations: model 0 torch.Size([256, 8, 8, 200])
shape of activations: model 1 torch.Size([256, 8, 8, 200])
shape of previous transport map torch.Size([128, 128])
saved skip T_var at layer 10 with shape torch.Size([256, 128, 3, 3])
shape of previous transport map now is torch.Size([128, 128])
returns a uniform measure of cardinality:  256
returns a uniform measure of cardinality:  256
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  12800
# of ground metric features in 0 is   12800
# of ground metric features in 1 is   12800
ground metric (m0) is  tensor([[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [245.3270, 245.3270, 245.3270,  ..., 245.3270, 245.3270, 245.3270],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        ...,
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [275.1449, 275.1449, 275.1449,  ..., 275.1449, 275.1449, 275.1449],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],
       device='cuda:0')
shape of T_var is  torch.Size([256, 256])
T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        ...,
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],
       device='cuda:0')
T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 
Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')
Here, trace is 1.9999487400054932 and matrix sum is 255.99343872070312 
Shape of aligned wt is  torch.Size([256, 128, 9])
Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])

--------------- At layer index 11 ------------- 
 
Previous layer shape is  torch.Size([256, 128, 3, 3])
let's see the difference in layer names layer3.0.conv2 layer3.0.conv2
torch.Size([200, 1, 256, 8, 8]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer3.0.conv2.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 7.9448137283325195, Mean : 3.1524271965026855, Min : 0.7985601425170898, Std: 1.266437292098999
shape of layer: model 0 torch.Size([256, 256, 9])
shape of layer: model 1 torch.Size([256, 256, 9])
shape of activations: model 0 torch.Size([256, 8, 8, 200])
shape of activations: model 1 torch.Size([256, 8, 8, 200])
shape of previous transport map torch.Size([256, 256])
doing nothing for skips
returns a uniform measure of cardinality:  256
returns a uniform measure of cardinality:  256
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  12800
# of ground metric features in 0 is   12800
# of ground metric features in 1 is   12800
ground metric (m0) is  tensor([[343.6737,   0.0000,   0.0000,  ...,   0.0000,   0.0000, 222.5558],
        [343.6737,   0.0000,   0.0000,  ...,   0.0000,   0.0000, 222.5558],
        [337.9176,  90.3596,  90.3596,  ...,  90.3596,  90.3596, 261.4898],
        ...,
        [343.6737,   0.0000,   0.0000,  ...,   0.0000,   0.0000, 222.5558],
        [343.6737,   0.0000,   0.0000,  ...,   0.0000,   0.0000, 222.5558],
        [395.8935, 101.0175, 101.0175,  ..., 101.0175, 101.0175, 253.4743]],
       device='cuda:0')
shape of T_var is  torch.Size([256, 256])
T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        ...,
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],
       device='cuda:0')
T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 
Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')
Here, trace is 0.0 and matrix sum is 255.99343872070312 
Shape of aligned wt is  torch.Size([256, 256, 9])
Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])

--------------- At layer index 12 ------------- 
 
Previous layer shape is  torch.Size([256, 256, 3, 3])
let's see the difference in layer names layer3.0.shortcut.0 layer3.0.shortcut.0
torch.Size([200, 1, 256, 8, 8]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer3.0.shortcut.0.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 3.0623788833618164, Mean : 1.1690312623977661, Min : 0.25524213910102844, Std: 0.5052023530006409
shape of layer: model 0 torch.Size([256, 128, 1])
shape of layer: model 1 torch.Size([256, 128, 1])
shape of activations: model 0 torch.Size([256, 8, 8, 200])
shape of activations: model 1 torch.Size([256, 8, 8, 200])
shape of previous transport map torch.Size([256, 256])
utilizing skip T_var saved from layer layer 10 with shape torch.Size([128, 128])
shape of previous transport map now is torch.Size([128, 128])
returns a uniform measure of cardinality:  256
returns a uniform measure of cardinality:  256
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  12800
# of ground metric features in 0 is   12800
# of ground metric features in 1 is   12800
ground metric (m0) is  tensor([[191.1395,   0.0000,   0.0000,  ...,   0.0000,   0.0000,  88.5620],
        [191.1395,   0.0000,   0.0000,  ...,   0.0000,   0.0000,  88.5620],
        [194.6903,  90.3596,  90.3596,  ...,  90.3596,  90.3596, 162.6896],
        ...,
        [191.1395,   0.0000,   0.0000,  ...,   0.0000,   0.0000,  88.5620],
        [191.1395,   0.0000,   0.0000,  ...,   0.0000,   0.0000,  88.5620],
        [260.6731, 101.0175, 101.0175,  ..., 101.0175, 101.0175, 147.1313]],
       device='cuda:0')
shape of T_var is  torch.Size([256, 256])
T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        ...,
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],
       device='cuda:0')
T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 
Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')
Here, trace is 0.9999743700027466 and matrix sum is 255.99343872070312 
Shape of aligned wt is  torch.Size([256, 128, 1])
Shape of fc_layer0_weight_data is  torch.Size([256, 128, 1])

--------------- At layer index 13 ------------- 
 
Previous layer shape is  torch.Size([256, 128, 1, 1])
let's see the difference in layer names layer3.1.conv1 layer3.1.conv1
torch.Size([200, 1, 256, 8, 8]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer3.1.conv1.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 6.722815036773682, Mean : 2.935209274291992, Min : 0.8837250471115112, Std: 1.0483142137527466
shape of layer: model 0 torch.Size([256, 256, 9])
shape of layer: model 1 torch.Size([256, 256, 9])
shape of activations: model 0 torch.Size([256, 8, 8, 200])
shape of activations: model 1 torch.Size([256, 8, 8, 200])
shape of previous transport map torch.Size([256, 256])
averaging multiple T_var's
returns a uniform measure of cardinality:  256
returns a uniform measure of cardinality:  256
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  12800
# of ground metric features in 0 is   12800
# of ground metric features in 1 is   12800
ground metric (m0) is  tensor([[  0.0000,   0.0000,   0.0000,  ...,   0.0000, 230.7539,   0.0000],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000, 230.7539,   0.0000],
        [194.6795, 194.6795, 194.6795,  ..., 194.6795, 210.3676, 194.6795],
        ...,
        [229.7474, 229.7474, 229.7474,  ..., 229.7474, 182.6075, 229.7474],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000, 230.7539,   0.0000],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000, 230.7539,   0.0000]],
       device='cuda:0')
shape of T_var is  torch.Size([256, 256])
T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        ...,
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],
       device='cuda:0')
T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 
Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')
Here, trace is 0.0 and matrix sum is 255.99343872070312 
Shape of aligned wt is  torch.Size([256, 256, 9])
Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])

--------------- At layer index 14 ------------- 
 
Previous layer shape is  torch.Size([256, 256, 3, 3])
let's see the difference in layer names layer3.1.conv2 layer3.1.conv2
torch.Size([200, 1, 256, 8, 8]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer3.1.conv2.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 4.426374912261963, Mean : 1.2669754028320312, Min : 0.17083682119846344, Std: 0.7541853189468384
shape of layer: model 0 torch.Size([256, 256, 9])
shape of layer: model 1 torch.Size([256, 256, 9])
shape of activations: model 0 torch.Size([256, 8, 8, 200])
shape of activations: model 1 torch.Size([256, 8, 8, 200])
shape of previous transport map torch.Size([256, 256])
doing nothing for skips
returns a uniform measure of cardinality:  256
returns a uniform measure of cardinality:  256
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  12800
# of ground metric features in 0 is   12800
# of ground metric features in 1 is   12800
ground metric (m0) is  tensor([[312.4454,  25.9218,  25.9218,  ...,  25.9218,  25.9218, 174.0108],
        [303.3258,   0.0000,   0.0000,  ...,   0.0000,   0.0000, 164.4625],
        [279.2938,  43.7554,  43.7554,  ...,  43.7554,  43.7554, 166.9837],
        ...,
        [303.3258,   0.0000,   0.0000,  ...,   0.0000,   0.0000, 164.4625],
        [303.3258,   0.0000,   0.0000,  ...,   0.0000,   0.0000, 164.4625],
        [303.2772,   4.9262,   4.9262,  ...,   4.9262,   4.9262, 164.2485]],
       device='cuda:0')
shape of T_var is  torch.Size([256, 256])
T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        ...,
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],
        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],
       device='cuda:0')
T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 
Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')
Here, trace is 0.0 and matrix sum is 255.99343872070312 
Shape of aligned wt is  torch.Size([256, 256, 9])
Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])

--------------- At layer index 15 ------------- 
 
Previous layer shape is  torch.Size([256, 256, 3, 3])
let's see the difference in layer names layer4.0.conv1 layer4.0.conv1
torch.Size([200, 1, 512, 4, 4]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer4.0.conv1.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 2.886652708053589, Mean : 1.1354902982711792, Min : 0.2507587969303131, Std: 0.47378402948379517
shape of layer: model 0 torch.Size([512, 256, 9])
shape of layer: model 1 torch.Size([512, 256, 9])
shape of activations: model 0 torch.Size([512, 4, 4, 200])
shape of activations: model 1 torch.Size([512, 4, 4, 200])
shape of previous transport map torch.Size([256, 256])
saved skip T_var at layer 15 with shape torch.Size([512, 256, 3, 3])
shape of previous transport map now is torch.Size([256, 256])
returns a uniform measure of cardinality:  512
returns a uniform measure of cardinality:  512
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  3200
# of ground metric features in 0 is   3200
# of ground metric features in 1 is   3200
ground metric (m0) is  tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [26.2198, 26.2198, 26.2198,  ..., 26.2198, 26.2198, 26.2198],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0')
shape of T_var is  torch.Size([512, 512])
T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        ...,
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],
       device='cuda:0')
T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.9999, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.9999, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 
Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')
Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 
Shape of aligned wt is  torch.Size([512, 256, 9])
Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])

--------------- At layer index 16 ------------- 
 
Previous layer shape is  torch.Size([512, 256, 3, 3])
let's see the difference in layer names layer4.0.conv2 layer4.0.conv2
torch.Size([200, 1, 512, 4, 4]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer4.0.conv2.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 2.141167163848877, Mean : 0.8744319081306458, Min : 0.2266039252281189, Std: 0.3343183100223541
shape of layer: model 0 torch.Size([512, 512, 9])
shape of layer: model 1 torch.Size([512, 512, 9])
shape of activations: model 0 torch.Size([512, 4, 4, 200])
shape of activations: model 1 torch.Size([512, 4, 4, 200])
shape of previous transport map torch.Size([512, 512])
doing nothing for skips
returns a uniform measure of cardinality:  512
returns a uniform measure of cardinality:  512
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  3200
# of ground metric features in 0 is   3200
# of ground metric features in 1 is   3200
ground metric (m0) is  tensor([[ 0.0000, 63.5489,  0.0000,  ..., 63.3980,  0.0000, 17.6988],
        [11.6430, 61.2443, 11.6430,  ..., 63.2758, 11.6430, 21.5250],
        [ 0.0000, 63.5489,  0.0000,  ..., 63.3980,  0.0000, 17.6988],
        ...,
        [ 0.0000, 63.5489,  0.0000,  ..., 63.3980,  0.0000, 17.6988],
        [ 0.0000, 63.5489,  0.0000,  ..., 63.3980,  0.0000, 17.6988],
        [41.2314, 46.6861, 41.2314,  ..., 70.4191, 41.2314, 43.5889]],
       device='cuda:0')
shape of T_var is  torch.Size([512, 512])
T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        ...,
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],
       device='cuda:0')
T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.9999, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 
Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')
Here, trace is 0.0 and matrix sum is 511.97381591796875 
Shape of aligned wt is  torch.Size([512, 512, 9])
Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])

--------------- At layer index 17 ------------- 
 
Previous layer shape is  torch.Size([512, 512, 3, 3])
let's see the difference in layer names layer4.0.shortcut.0 layer4.0.shortcut.0
torch.Size([200, 1, 512, 4, 4]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer4.0.shortcut.0.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 0.9234867095947266, Mean : 0.3498607873916626, Min : 0.07884921133518219, Std: 0.15091539919376373
shape of layer: model 0 torch.Size([512, 256, 1])
shape of layer: model 1 torch.Size([512, 256, 1])
shape of activations: model 0 torch.Size([512, 4, 4, 200])
shape of activations: model 1 torch.Size([512, 4, 4, 200])
shape of previous transport map torch.Size([512, 512])
utilizing skip T_var saved from layer layer 15 with shape torch.Size([256, 256])
shape of previous transport map now is torch.Size([256, 256])
returns a uniform measure of cardinality:  512
returns a uniform measure of cardinality:  512
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  3200
# of ground metric features in 0 is   3200
# of ground metric features in 1 is   3200
ground metric (m0) is  tensor([[ 0.0000, 20.8029,  0.0000,  ..., 33.4200,  0.0000, 17.6988],
        [11.6430, 20.4216, 11.6430,  ..., 32.1573, 11.6430, 21.5250],
        [ 0.0000, 20.8029,  0.0000,  ..., 33.4200,  0.0000, 17.6988],
        ...,
        [ 0.0000, 20.8029,  0.0000,  ..., 33.4200,  0.0000, 17.6988],
        [ 0.0000, 20.8029,  0.0000,  ..., 33.4200,  0.0000, 17.6988],
        [15.4939, 19.5808, 15.4939,  ..., 28.5195, 15.4939, 24.0889]],
       device='cuda:0')
shape of T_var is  torch.Size([512, 512])
T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        ...,
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],
       device='cuda:0')
T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.9999, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 
Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')
Here, trace is 0.0 and matrix sum is 511.97381591796875 
Shape of aligned wt is  torch.Size([512, 256, 1])
Shape of fc_layer0_weight_data is  torch.Size([512, 256, 1])

--------------- At layer index 18 ------------- 
 
Previous layer shape is  torch.Size([512, 256, 1, 1])
let's see the difference in layer names layer4.1.conv1 layer4.1.conv1
torch.Size([200, 1, 512, 4, 4]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer4.1.conv1.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 0.6610547304153442, Mean : 0.2810633182525635, Min : 0.08306083083152771, Std: 0.10056377947330475
shape of layer: model 0 torch.Size([512, 512, 9])
shape of layer: model 1 torch.Size([512, 512, 9])
shape of activations: model 0 torch.Size([512, 4, 4, 200])
shape of activations: model 1 torch.Size([512, 4, 4, 200])
shape of previous transport map torch.Size([512, 512])
averaging multiple T_var's
returns a uniform measure of cardinality:  512
returns a uniform measure of cardinality:  512
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  3200
# of ground metric features in 0 is   3200
# of ground metric features in 1 is   3200
ground metric (m0) is  tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, 29.2700],
        [10.8166, 10.8166, 10.8166,  ..., 10.8166, 10.8166, 22.2513],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, 29.2700],
        ...,
        [ 9.9662,  9.9662,  9.9662,  ...,  9.9662,  9.9662, 22.0773],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, 29.2700],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, 29.2700]],
       device='cuda:0')
shape of T_var is  torch.Size([512, 512])
T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        ...,
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],
       device='cuda:0')
T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.9999, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 
Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')
Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 
Shape of aligned wt is  torch.Size([512, 512, 9])
Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])

--------------- At layer index 19 ------------- 
 
Previous layer shape is  torch.Size([512, 512, 3, 3])
let's see the difference in layer names layer4.1.conv2 layer4.1.conv2
torch.Size([200, 1, 512, 4, 4]) shape of activations generally
reorder_dim is  [1, 2, 3, 0]
In layer layer4.1.conv2.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 0.8279714584350586, Mean : 0.18883773684501648, Min : 0.02317231148481369, Std: 0.1351097971200943
shape of layer: model 0 torch.Size([512, 512, 9])
shape of layer: model 1 torch.Size([512, 512, 9])
shape of activations: model 0 torch.Size([512, 4, 4, 200])
shape of activations: model 1 torch.Size([512, 4, 4, 200])
shape of previous transport map torch.Size([512, 512])
doing nothing for skips
returns a uniform measure of cardinality:  512
returns a uniform measure of cardinality:  512
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features is  3200
# of ground metric features in 0 is   3200
# of ground metric features in 1 is   3200
ground metric (m0) is  tensor([[ 0.0000,  7.0173,  0.0000,  ..., 37.4038,  0.0000, 10.0237],
        [ 6.5108, 10.2415,  6.5108,  ..., 36.3283,  6.5108, 10.8471],
        [ 0.0000,  7.0173,  0.0000,  ..., 37.4038,  0.0000, 10.0237],
        ...,
        [ 0.0000,  7.0173,  0.0000,  ..., 37.4038,  0.0000, 10.0237],
        [ 1.6526,  6.3998,  1.6526,  ..., 37.8689,  1.6526, 10.8420],
        [ 6.0142,  9.1700,  6.0142,  ..., 36.5922,  6.0142, 11.3545]],
       device='cuda:0')
shape of T_var is  torch.Size([512, 512])
T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        ...,
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],
        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],
       device='cuda:0')
T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 
Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')
Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 
Shape of aligned wt is  torch.Size([512, 512, 9])
Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])

--------------- At layer index 20 ------------- 
 
Previous layer shape is  torch.Size([512, 512, 3, 3])
let's see the difference in layer names linear linear
torch.Size([200, 1, 10]) shape of activations generally
In layer linear.weight: getting activation distance statistics
Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): 

Max : 15.516336441040039, Mean : 8.023569107055664, Min : 3.5775516033172607, Std: 3.958021640777588
shape of layer: model 0 torch.Size([10, 512])
shape of layer: model 1 torch.Size([10, 512])
shape of activations: model 0 torch.Size([10, 200])
shape of activations: model 1 torch.Size([10, 200])
shape of previous transport map torch.Size([512, 512])
returns a uniform measure of cardinality:  10
returns a uniform measure of cardinality:  10
Refactored ground metric calc
inside refactored
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
# of ground metric features in 0 is   200
# of ground metric features in 1 is   200
ground metric (m0) is  tensor([[11.8474,  6.0267,  6.6939,  5.9254,  7.6933,  4.0225, 15.9345,  7.7229,
          3.9226,  3.3181],
        [13.5301,  4.3782,  8.1637,  6.8778,  9.1548,  4.6995, 17.5557,  6.1181,
          3.2153,  2.7768],
        [ 8.8887,  9.1875,  3.6564,  4.7332,  4.6383,  4.6663, 12.3569, 10.8874,
          5.7497,  6.0392],
        [ 5.0906, 18.4593,  7.5394, 10.8373,  6.6147, 12.9378,  4.7634, 20.0088,
         14.6092, 15.0479],
        [11.0373,  7.3068,  5.5654,  5.1246,  6.4549,  4.2360, 14.4338,  9.0540,
          4.4668,  4.3606],
        [ 6.4884, 13.0304,  3.6080,  6.6964,  3.6431,  7.4100,  9.3386, 14.5347,
          9.4160,  9.7946],
        [ 7.7289, 10.8885,  2.7685,  5.1144,  3.6753,  6.1737, 10.8060, 12.5859,
          7.4848,  7.5714],
        [ 9.5299,  8.5615,  4.8371,  5.5650,  5.4633,  4.1670, 13.4820, 10.1860,
          5.2049,  5.8398],
        [16.6654,  2.1635, 11.3691,  9.5069, 12.3330,  7.0613, 20.7408,  3.7785,
          4.7383,  3.9433],
        [ 9.8552,  8.1141,  4.4559,  4.7042,  5.5275,  4.3178, 13.5303,  9.7514,
          5.0593,  4.8170]], device='cuda:0')
shape of T_var is  torch.Size([10, 10])
T_var before correction  tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.1000],
        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,
         0.0000],
        [0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000]], device='cuda:0')
marginals are  tensor([[10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,
         10.0000, 10.0000],
        [10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,
         10.0000, 10.0000],
        [10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,
         10.0000, 10.0000],
        [10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,
         10.0000, 10.0000],
        [10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,
         10.0000, 10.0000],
        [10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,
         10.0000, 10.0000],
        [10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,
         10.0000, 10.0000],
        [10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,
         10.0000, 10.0000],
        [10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,
         10.0000, 10.0000],
        [10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,
         10.0000, 10.0000]], device='cuda:0')
T_var after correction  tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         1.0000],
        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,
         0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000]], device='cuda:0')
T_var stats: max 0.9999990463256836, min 0.0, mean 0.09999990463256836, std 0.3015110492706299 
Ratio of trace to the matrix sum:  tensor(0.2000, device='cuda:0')
Here, trace is 1.9999980926513672 and matrix sum is 9.999990463256836 
Shape of aligned wt is  torch.Size([10, 512])
Shape of fc_layer0_weight_data is  torch.Size([10, 512])
using independent method
model parameters are 
 [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]

--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0230, Accuracy: 1162/10000 (12%)

len of model parameters and avg aligned layers is  21 21
len of model_state_dict is  21
len of param_list is  21

--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0231, Accuracy: 1201/10000 (12%)

Timer ends
Time taken for geometric ensembling is 15.846896512899548 seconds
------- Prediction based ensembling -------

Test set: Avg. loss: 0.0226, Accuracy: 2170/10000 (22%)

------- Naive ensembling of weights -------
[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]
torch.Size([64, 3, 3, 3])
[torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3])]
torch.Size([64, 64, 3, 3])
[torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3])]
torch.Size([64, 64, 3, 3])
[torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3])]
torch.Size([64, 64, 3, 3])
[torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3])]
torch.Size([64, 64, 3, 3])
[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]
torch.Size([128, 64, 3, 3])
[torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3])]
torch.Size([128, 128, 3, 3])
[torch.Size([128, 64, 1, 1]), torch.Size([128, 64, 1, 1])]
torch.Size([128, 64, 1, 1])
[torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3])]
torch.Size([128, 128, 3, 3])
[torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3])]
torch.Size([128, 128, 3, 3])
[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]
torch.Size([256, 128, 3, 3])
[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]
torch.Size([256, 256, 3, 3])
[torch.Size([256, 128, 1, 1]), torch.Size([256, 128, 1, 1])]
torch.Size([256, 128, 1, 1])
[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]
torch.Size([256, 256, 3, 3])
[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]
torch.Size([256, 256, 3, 3])
[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]
torch.Size([512, 256, 3, 3])
[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]
torch.Size([512, 512, 3, 3])
[torch.Size([512, 256, 1, 1]), torch.Size([512, 256, 1, 1])]
torch.Size([512, 256, 1, 1])
[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]
torch.Size([512, 512, 3, 3])
[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]
torch.Size([512, 512, 3, 3])
[torch.Size([10, 512]), torch.Size([10, 512])]
torch.Size([10, 512])
model parameters are 
 [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]

--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0230, Accuracy: 1000/10000 (10%)


--------- Testing in global mode ---------
/home/gvignen/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/gvignen/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0230, Accuracy: 1037/10000 (10%)

-------- Retraining the models ---------
Retraining model :  geometric
lr is  0.1
number of epochs would be  300
Epoch 000
accuracy: {'epoch': 0, 'value': 0.34474000001907346} ({'split': 'train'})
cross_entropy: {'epoch': 0, 'value': 1.750596578407287} ({'split': 'train'})
accuracy: {'epoch': 0, 'value': 0.4641999870538711} ({'split': 'test'})
cross_entropy: {'epoch': 0, 'value': 1.4899668967723845} ({'split': 'test'})
We have a new best! with accuracy::0.4641999870538711 and at epoch::0, let's save it!
Epoch 001
accuracy: {'epoch': 1, 'value': 0.5407800000190734} ({'split': 'train'})
cross_entropy: {'epoch': 1, 'value': 1.279368929023743} ({'split': 'train'})
accuracy: {'epoch': 1, 'value': 0.5800999850034715} ({'split': 'test'})
cross_entropy: {'epoch': 1, 'value': 1.2000798815488816} ({'split': 'test'})
We have a new best! with accuracy::0.5800999850034715 and at epoch::1, let's save it!
Epoch 002
accuracy: {'epoch': 2, 'value': 0.6711800000381472} ({'split': 'train'})
cross_entropy: {'epoch': 2, 'value': 0.944311996040345} ({'split': 'train'})
accuracy: {'epoch': 2, 'value': 0.6923999828100206} ({'split': 'test'})
cross_entropy: {'epoch': 2, 'value': 0.9250591325759889} ({'split': 'test'})
We have a new best! with accuracy::0.6923999828100206 and at epoch::2, let's save it!
Epoch 003
accuracy: {'epoch': 3, 'value': 0.7284799999999996} ({'split': 'train'})
cross_entropy: {'epoch': 3, 'value': 0.7913057182693484} ({'split': 'train'})
accuracy: {'epoch': 3, 'value': 0.7363999825716021} ({'split': 'test'})
cross_entropy: {'epoch': 3, 'value': 0.7734599593281745} ({'split': 'test'})
We have a new best! with accuracy::0.7363999825716021 and at epoch::3, let's save it!
Epoch 004
accuracy: {'epoch': 4, 'value': 0.7626600000381473} ({'split': 'train'})
cross_entropy: {'epoch': 4, 'value': 0.6914049690818787} ({'split': 'train'})
accuracy: {'epoch': 4, 'value': 0.7716999816894532} ({'split': 'test'})
cross_entropy: {'epoch': 4, 'value': 0.6840296429395677} ({'split': 'test'})
We have a new best! with accuracy::0.7716999816894532 and at epoch::4, let's save it!
Epoch 005
accuracy: {'epoch': 5, 'value': 0.7778000000381469} ({'split': 'train'})
cross_entropy: {'epoch': 5, 'value': 0.6466157233047488} ({'split': 'train'})
accuracy: {'epoch': 5, 'value': 0.7810999792814255} ({'split': 'test'})
cross_entropy: {'epoch': 5, 'value': 0.6728122144937514} ({'split': 'test'})
We have a new best! with accuracy::0.7810999792814255 and at epoch::5, let's save it!
Epoch 006
accuracy: {'epoch': 6, 'value': 0.8012800000000002} ({'split': 'train'})
cross_entropy: {'epoch': 6, 'value': 0.588669073104858} ({'split': 'train'})
accuracy: {'epoch': 6, 'value': 0.7780999779701233} ({'split': 'test'})
cross_entropy: {'epoch': 6, 'value': 0.6637768578529359} ({'split': 'test'})
Epoch 007
accuracy: {'epoch': 7, 'value': 0.8115200000572206} ({'split': 'train'})
cross_entropy: {'epoch': 7, 'value': 0.558188953857422} ({'split': 'train'})
accuracy: {'epoch': 7, 'value': 0.8018999803066252} ({'split': 'test'})
cross_entropy: {'epoch': 7, 'value': 0.5973331454396251} ({'split': 'test'})
We have a new best! with accuracy::0.8018999803066252 and at epoch::7, let's save it!
Epoch 008
accuracy: {'epoch': 8, 'value': 0.8174599999999996} ({'split': 'train'})
cross_entropy: {'epoch': 8, 'value': 0.5352611601924894} ({'split': 'train'})
accuracy: {'epoch': 8, 'value': 0.7988999778032301} ({'split': 'test'})
cross_entropy: {'epoch': 8, 'value': 0.6294508135318757} ({'split': 'test'})
Epoch 009
accuracy: {'epoch': 9, 'value': 0.8228200000572201} ({'split': 'train'})
cross_entropy: {'epoch': 9, 'value': 0.5183011244773867} ({'split': 'train'})
accuracy: {'epoch': 9, 'value': 0.811699976325035} ({'split': 'test'})
cross_entropy: {'epoch': 9, 'value': 0.57803590297699} ({'split': 'test'})
We have a new best! with accuracy::0.811699976325035 and at epoch::9, let's save it!
Epoch 010
accuracy: {'epoch': 10, 'value': 0.8342800000190738} ({'split': 'train'})
cross_entropy: {'epoch': 10, 'value': 0.48499607877731327} ({'split': 'train'})
accuracy: {'epoch': 10, 'value': 0.8206999772787092} ({'split': 'test'})
cross_entropy: {'epoch': 10, 'value': 0.551519088745117} ({'split': 'test'})
We have a new best! with accuracy::0.8206999772787092 and at epoch::10, let's save it!
Epoch 011
accuracy: {'epoch': 11, 'value': 0.8419000000000001} ({'split': 'train'})
cross_entropy: {'epoch': 11, 'value': 0.4641632537651061} ({'split': 'train'})
accuracy: {'epoch': 11, 'value': 0.8263999807834624} ({'split': 'test'})
cross_entropy: {'epoch': 11, 'value': 0.5502500262856482} ({'split': 'test'})
We have a new best! with accuracy::0.8263999807834624 and at epoch::11, let's save it!
Epoch 012
accuracy: {'epoch': 12, 'value': 0.8441200000572202} ({'split': 'train'})
cross_entropy: {'epoch': 12, 'value': 0.4574563482666016} ({'split': 'train'})
accuracy: {'epoch': 12, 'value': 0.8290999788045882} ({'split': 'test'})
cross_entropy: {'epoch': 12, 'value': 0.5261994031071663} ({'split': 'test'})
We have a new best! with accuracy::0.8290999788045882 and at epoch::12, let's save it!
Epoch 013
accuracy: {'epoch': 13, 'value': 0.8512200000572204} ({'split': 'train'})
cross_entropy: {'epoch': 13, 'value': 0.43900284816741975} ({'split': 'train'})
accuracy: {'epoch': 13, 'value': 0.8086999797821046} ({'split': 'test'})
cross_entropy: {'epoch': 13, 'value': 0.5644412732124326} ({'split': 'test'})
Epoch 014
accuracy: {'epoch': 14, 'value': 0.8517799999809261} ({'split': 'train'})
cross_entropy: {'epoch': 14, 'value': 0.4360395736312867} ({'split': 'train'})
accuracy: {'epoch': 14, 'value': 0.83309997856617} ({'split': 'test'})
cross_entropy: {'epoch': 14, 'value': 0.5059441328048708} ({'split': 'test'})
We have a new best! with accuracy::0.83309997856617 and at epoch::14, let's save it!
Epoch 015
accuracy: {'epoch': 15, 'value': 0.8589400000000004} ({'split': 'train'})
cross_entropy: {'epoch': 15, 'value': 0.41739534788131716} ({'split': 'train'})
accuracy: {'epoch': 15, 'value': 0.8401999777555464} ({'split': 'test'})
cross_entropy: {'epoch': 15, 'value': 0.4836434984207153} ({'split': 'test'})
We have a new best! with accuracy::0.8401999777555464 and at epoch::15, let's save it!
Epoch 016
accuracy: {'epoch': 16, 'value': 0.8595000000381471} ({'split': 'train'})
cross_entropy: {'epoch': 16, 'value': 0.4162656528663634} ({'split': 'train'})
accuracy: {'epoch': 16, 'value': 0.8440999788045884} ({'split': 'test'})
cross_entropy: {'epoch': 16, 'value': 0.48700405627489096} ({'split': 'test'})
We have a new best! with accuracy::0.8440999788045884 and at epoch::16, let's save it!
Epoch 017
accuracy: {'epoch': 17, 'value': 0.8626999999809263} ({'split': 'train'})
cross_entropy: {'epoch': 17, 'value': 0.40575592727661136} ({'split': 'train'})
accuracy: {'epoch': 17, 'value': 0.8438999807834628} ({'split': 'test'})
cross_entropy: {'epoch': 17, 'value': 0.4675062140822411} ({'split': 'test'})
Epoch 018
accuracy: {'epoch': 18, 'value': 0.8685600000572203} ({'split': 'train'})
cross_entropy: {'epoch': 18, 'value': 0.3898395843696595} ({'split': 'train'})
accuracy: {'epoch': 18, 'value': 0.8503999775648118} ({'split': 'test'})
cross_entropy: {'epoch': 18, 'value': 0.4544402119517326} ({'split': 'test'})
We have a new best! with accuracy::0.8503999775648118 and at epoch::18, let's save it!
Epoch 019
accuracy: {'epoch': 19, 'value': 0.8695799999999999} ({'split': 'train'})
cross_entropy: {'epoch': 19, 'value': 0.3768679733276368} ({'split': 'train'})
accuracy: {'epoch': 19, 'value': 0.8348999798297884} ({'split': 'test'})
cross_entropy: {'epoch': 19, 'value': 0.5114006128907204} ({'split': 'test'})
Epoch 020
accuracy: {'epoch': 20, 'value': 0.8708600000000005} ({'split': 'train'})
cross_entropy: {'epoch': 20, 'value': 0.3790966327571871} ({'split': 'train'})
accuracy: {'epoch': 20, 'value': 0.8413999778032306} ({'split': 'test'})
cross_entropy: {'epoch': 20, 'value': 0.47614466220140444} ({'split': 'test'})
Epoch 021
accuracy: {'epoch': 21, 'value': 0.8717200000381471} ({'split': 'train'})
cross_entropy: {'epoch': 21, 'value': 0.37911160324096665} ({'split': 'train'})
accuracy: {'epoch': 21, 'value': 0.8431999802589418} ({'split': 'test'})
cross_entropy: {'epoch': 21, 'value': 0.4963132956624035} ({'split': 'test'})
Epoch 022
accuracy: {'epoch': 22, 'value': 0.874279999980926} ({'split': 'train'})
cross_entropy: {'epoch': 22, 'value': 0.3657837286949157} ({'split': 'train'})
accuracy: {'epoch': 22, 'value': 0.8559999787807466} ({'split': 'test'})
cross_entropy: {'epoch': 22, 'value': 0.4433172486722468} ({'split': 'test'})
We have a new best! with accuracy::0.8559999787807466 and at epoch::22, let's save it!
Epoch 023
accuracy: {'epoch': 23, 'value': 0.8789600000000002} ({'split': 'train'})
cross_entropy: {'epoch': 23, 'value': 0.3526362662315369} ({'split': 'train'})
accuracy: {'epoch': 23, 'value': 0.8468999797105792} ({'split': 'test'})
cross_entropy: {'epoch': 23, 'value': 0.47114825546741473} ({'split': 'test'})
Epoch 024
accuracy: {'epoch': 24, 'value': 0.8775800000381466} ({'split': 'train'})
cross_entropy: {'epoch': 24, 'value': 0.36028878585338614} ({'split': 'train'})
accuracy: {'epoch': 24, 'value': 0.8349999779462814} ({'split': 'test'})
cross_entropy: {'epoch': 24, 'value': 0.5139921560883521} ({'split': 'test'})
Epoch 025
accuracy: {'epoch': 25, 'value': 0.879400000038147} ({'split': 'train'})
cross_entropy: {'epoch': 25, 'value': 0.3520790160655975} ({'split': 'train'})
accuracy: {'epoch': 25, 'value': 0.838299978375435} ({'split': 'test'})
cross_entropy: {'epoch': 25, 'value': 0.49797043934464474} ({'split': 'test'})
Epoch 026
accuracy: {'epoch': 26, 'value': 0.8786599999999998} ({'split': 'train'})
cross_entropy: {'epoch': 26, 'value': 0.35148799096107486} ({'split': 'train'})
accuracy: {'epoch': 26, 'value': 0.8452999788522723} ({'split': 'test'})
cross_entropy: {'epoch': 26, 'value': 0.4672301113605499} ({'split': 'test'})
Epoch 027
accuracy: {'epoch': 27, 'value': 0.8831600000190739} ({'split': 'train'})
cross_entropy: {'epoch': 27, 'value': 0.3436804714584352} ({'split': 'train'})
accuracy: {'epoch': 27, 'value': 0.8540999817848206} ({'split': 'test'})
cross_entropy: {'epoch': 27, 'value': 0.4510639923810959} ({'split': 'test'})
Epoch 028
accuracy: {'epoch': 28, 'value': 0.8850000000190735} ({'split': 'train'})
cross_entropy: {'epoch': 28, 'value': 0.33918623503685014} ({'split': 'train'})
accuracy: {'epoch': 28, 'value': 0.851099978685379} ({'split': 'test'})
cross_entropy: {'epoch': 28, 'value': 0.45617707222700116} ({'split': 'test'})
Epoch 029
accuracy: {'epoch': 29, 'value': 0.88434} ({'split': 'train'})
cross_entropy: {'epoch': 29, 'value': 0.33625883560180664} ({'split': 'train'})
accuracy: {'epoch': 29, 'value': 0.8296999782323837} ({'split': 'test'})
cross_entropy: {'epoch': 29, 'value': 0.5126275917887686} ({'split': 'test'})
Epoch 030
accuracy: {'epoch': 30, 'value': 0.8880599999809262} ({'split': 'train'})
cross_entropy: {'epoch': 30, 'value': 0.32596370794296275} ({'split': 'train'})
accuracy: {'epoch': 30, 'value': 0.8630999797582627} ({'split': 'test'})
cross_entropy: {'epoch': 30, 'value': 0.42695045322179787} ({'split': 'test'})
We have a new best! with accuracy::0.8630999797582627 and at epoch::30, let's save it!
Epoch 031
accuracy: {'epoch': 31, 'value': 0.8905400000000001} ({'split': 'train'})
cross_entropy: {'epoch': 31, 'value': 0.31936501066207895} ({'split': 'train'})
accuracy: {'epoch': 31, 'value': 0.8493999773263933} ({'split': 'test'})
cross_entropy: {'epoch': 31, 'value': 0.46901117190718655} ({'split': 'test'})
Epoch 032
accuracy: {'epoch': 32, 'value': 0.8877000000190731} ({'split': 'train'})
cross_entropy: {'epoch': 32, 'value': 0.32985712498664865} ({'split': 'train'})
accuracy: {'epoch': 32, 'value': 0.8491999781131747} ({'split': 'test'})
cross_entropy: {'epoch': 32, 'value': 0.4625296282768249} ({'split': 'test'})
Epoch 033
accuracy: {'epoch': 33, 'value': 0.8920599999809263} ({'split': 'train'})
cross_entropy: {'epoch': 33, 'value': 0.3177941372299196} ({'split': 'train'})
accuracy: {'epoch': 33, 'value': 0.8482999789714816} ({'split': 'test'})
cross_entropy: {'epoch': 33, 'value': 0.49458125084638593} ({'split': 'test'})
Epoch 034
accuracy: {'epoch': 34, 'value': 0.8929400000381469} ({'split': 'train'})
cross_entropy: {'epoch': 34, 'value': 0.3135458566188811} ({'split': 'train'})
accuracy: {'epoch': 34, 'value': 0.8478999787569046} ({'split': 'test'})
cross_entropy: {'epoch': 34, 'value': 0.47466352313756965} ({'split': 'test'})
Epoch 035
accuracy: {'epoch': 35, 'value': 0.8956799999809265} ({'split': 'train'})
cross_entropy: {'epoch': 35, 'value': 0.30782963361263266} ({'split': 'train'})
accuracy: {'epoch': 35, 'value': 0.8553999793529511} ({'split': 'test'})
cross_entropy: {'epoch': 35, 'value': 0.48481595367193225} ({'split': 'test'})
Epoch 036
accuracy: {'epoch': 36, 'value': 0.8917200000381474} ({'split': 'train'})
cross_entropy: {'epoch': 36, 'value': 0.3174195602226258} ({'split': 'train'})
accuracy: {'epoch': 36, 'value': 0.8470999795198442} ({'split': 'test'})
cross_entropy: {'epoch': 36, 'value': 0.4848967152833937} ({'split': 'test'})
Epoch 037
accuracy: {'epoch': 37, 'value': 0.8927000000381469} ({'split': 'train'})
cross_entropy: {'epoch': 37, 'value': 0.3139429945182799} ({'split': 'train'})
accuracy: {'epoch': 37, 'value': 0.8448999792337416} ({'split': 'test'})
cross_entropy: {'epoch': 37, 'value': 0.49042340710759136} ({'split': 'test'})
Epoch 038
accuracy: {'epoch': 38, 'value': 0.8932800000381471} ({'split': 'train'})
cross_entropy: {'epoch': 38, 'value': 0.309324552268982} ({'split': 'train'})
accuracy: {'epoch': 38, 'value': 0.8511999833583832} ({'split': 'test'})
cross_entropy: {'epoch': 38, 'value': 0.47817422479391103} ({'split': 'test'})
Epoch 039
accuracy: {'epoch': 39, 'value': 0.8938000000381471} ({'split': 'train'})
cross_entropy: {'epoch': 39, 'value': 0.31314526404380794} ({'split': 'train'})
accuracy: {'epoch': 39, 'value': 0.8544999772310257} ({'split': 'test'})
cross_entropy: {'epoch': 39, 'value': 0.4547914509475231} ({'split': 'test'})
Epoch 040
accuracy: {'epoch': 40, 'value': 0.8972200000381472} ({'split': 'train'})
cross_entropy: {'epoch': 40, 'value': 0.30315577676773076} ({'split': 'train'})
accuracy: {'epoch': 40, 'value': 0.8620999771356583} ({'split': 'test'})
cross_entropy: {'epoch': 40, 'value': 0.436079378426075} ({'split': 'test'})
Epoch 041
accuracy: {'epoch': 41, 'value': 0.8966799999809266} ({'split': 'train'})
cross_entropy: {'epoch': 41, 'value': 0.30043848418712604} ({'split': 'train'})
accuracy: {'epoch': 41, 'value': 0.850399978160858} ({'split': 'test'})
cross_entropy: {'epoch': 41, 'value': 0.46481590777635584} ({'split': 'test'})
Epoch 042
accuracy: {'epoch': 42, 'value': 0.8945799999999997} ({'split': 'train'})
cross_entropy: {'epoch': 42, 'value': 0.30981303532600385} ({'split': 'train'})
accuracy: {'epoch': 42, 'value': 0.8648999798297882} ({'split': 'test'})
cross_entropy: {'epoch': 42, 'value': 0.4307980030775072} ({'split': 'test'})
We have a new best! with accuracy::0.8648999798297882 and at epoch::42, let's save it!
Epoch 043
accuracy: {'epoch': 43, 'value': 0.8991400000000004} ({'split': 'train'})
cross_entropy: {'epoch': 43, 'value': 0.29594069093704223} ({'split': 'train'})
accuracy: {'epoch': 43, 'value': 0.8557999789714812} ({'split': 'test'})
cross_entropy: {'epoch': 43, 'value': 0.47583567976951596} ({'split': 'test'})
Epoch 044
accuracy: {'epoch': 44, 'value': 0.8988400000381468} ({'split': 'train'})
cross_entropy: {'epoch': 44, 'value': 0.2965616845893857} ({'split': 'train'})
accuracy: {'epoch': 44, 'value': 0.8523999804258344} ({'split': 'test'})
cross_entropy: {'epoch': 44, 'value': 0.47709380745887736} ({'split': 'test'})
Epoch 045
accuracy: {'epoch': 45, 'value': 0.9002600000190735} ({'split': 'train'})
cross_entropy: {'epoch': 45, 'value': 0.2913303571891785} ({'split': 'train'})
accuracy: {'epoch': 45, 'value': 0.8638999783992766} ({'split': 'test'})
cross_entropy: {'epoch': 45, 'value': 0.4329663395881653} ({'split': 'test'})
Epoch 046
accuracy: {'epoch': 46, 'value': 0.8982800000572204} ({'split': 'train'})
cross_entropy: {'epoch': 46, 'value': 0.2976142915773394} ({'split': 'train'})
accuracy: {'epoch': 46, 'value': 0.8557999777793887} ({'split': 'test'})
cross_entropy: {'epoch': 46, 'value': 0.4592494010925294} ({'split': 'test'})
Epoch 047
accuracy: {'epoch': 47, 'value': 0.8996} ({'split': 'train'})
cross_entropy: {'epoch': 47, 'value': 0.29025129616737355} ({'split': 'train'})
accuracy: {'epoch': 47, 'value': 0.8609999793767933} ({'split': 'test'})
cross_entropy: {'epoch': 47, 'value': 0.441131020486355} ({'split': 'test'})
Epoch 048
accuracy: {'epoch': 48, 'value': 0.9000400000572206} ({'split': 'train'})
cross_entropy: {'epoch': 48, 'value': 0.28878677653312695} ({'split': 'train'})
accuracy: {'epoch': 48, 'value': 0.8754999810457227} ({'split': 'test'})
cross_entropy: {'epoch': 48, 'value': 0.39734101966023444} ({'split': 'test'})
We have a new best! with accuracy::0.8754999810457227 and at epoch::48, let's save it!
Epoch 049
accuracy: {'epoch': 49, 'value': 0.9021200000381465} ({'split': 'train'})
cross_entropy: {'epoch': 49, 'value': 0.28570371465206146} ({'split': 'train'})
accuracy: {'epoch': 49, 'value': 0.8588999789953232} ({'split': 'test'})
cross_entropy: {'epoch': 49, 'value': 0.43987339735031133} ({'split': 'test'})
Epoch 050
accuracy: {'epoch': 50, 'value': 0.9025599999999999} ({'split': 'train'})
cross_entropy: {'epoch': 50, 'value': 0.28693885583877565} ({'split': 'train'})
accuracy: {'epoch': 50, 'value': 0.8506999814510346} ({'split': 'test'})
cross_entropy: {'epoch': 50, 'value': 0.4577771446108819} ({'split': 'test'})
Epoch 051
accuracy: {'epoch': 51, 'value': 0.9056399999809266} ({'split': 'train'})
cross_entropy: {'epoch': 51, 'value': 0.2740801557350157} ({'split': 'train'})
accuracy: {'epoch': 51, 'value': 0.8560999810695646} ({'split': 'test'})
cross_entropy: {'epoch': 51, 'value': 0.4539178000390529} ({'split': 'test'})
Epoch 052
accuracy: {'epoch': 52, 'value': 0.9064199999809265} ({'split': 'train'})
cross_entropy: {'epoch': 52, 'value': 0.2806938990807535} ({'split': 'train'})
accuracy: {'epoch': 52, 'value': 0.8581999796628953} ({'split': 'test'})
cross_entropy: {'epoch': 52, 'value': 0.4497307188808919} ({'split': 'test'})
Epoch 053
accuracy: {'epoch': 53, 'value': 0.9039800000190739} ({'split': 'train'})
cross_entropy: {'epoch': 53, 'value': 0.28072328773021715} ({'split': 'train'})
accuracy: {'epoch': 53, 'value': 0.857299981117249} ({'split': 'test'})
cross_entropy: {'epoch': 53, 'value': 0.45681821361184116} ({'split': 'test'})
Epoch 054
accuracy: {'epoch': 54, 'value': 0.902500000038147} ({'split': 'train'})
cross_entropy: {'epoch': 54, 'value': 0.2825798158168794} ({'split': 'train'})
accuracy: {'epoch': 54, 'value': 0.8541999787092209} ({'split': 'test'})
cross_entropy: {'epoch': 54, 'value': 0.4774675780534744} ({'split': 'test'})
Epoch 055
accuracy: {'epoch': 55, 'value': 0.9021000000000002} ({'split': 'train'})
cross_entropy: {'epoch': 55, 'value': 0.2828183746051788} ({'split': 'train'})
accuracy: {'epoch': 55, 'value': 0.862199980020523} ({'split': 'test'})
cross_entropy: {'epoch': 55, 'value': 0.4358440798521042} ({'split': 'test'})
Epoch 056
accuracy: {'epoch': 56, 'value': 0.9043000000000004} ({'split': 'train'})
cross_entropy: {'epoch': 56, 'value': 0.28092510154724115} ({'split': 'train'})
accuracy: {'epoch': 56, 'value': 0.8692999809980394} ({'split': 'test'})
cross_entropy: {'epoch': 56, 'value': 0.4253805834054946} ({'split': 'test'})
Epoch 057
accuracy: {'epoch': 57, 'value': 0.9058199999809268} ({'split': 'train'})
cross_entropy: {'epoch': 57, 'value': 0.2764056512355804} ({'split': 'train'})
accuracy: {'epoch': 57, 'value': 0.8518999773263929} ({'split': 'test'})
cross_entropy: {'epoch': 57, 'value': 0.4568331071734428} ({'split': 'test'})
Epoch 058
accuracy: {'epoch': 58, 'value': 0.9065400000381472} ({'split': 'train'})
cross_entropy: {'epoch': 58, 'value': 0.2720897341394426} ({'split': 'train'})
accuracy: {'epoch': 58, 'value': 0.8607999813556672} ({'split': 'test'})
cross_entropy: {'epoch': 58, 'value': 0.44828571140766166} ({'split': 'test'})
Epoch 059
accuracy: {'epoch': 59, 'value': 0.9022000000572201} ({'split': 'train'})
cross_entropy: {'epoch': 59, 'value': 0.2869349355602263} ({'split': 'train'})
accuracy: {'epoch': 59, 'value': 0.8708999830484391} ({'split': 'test'})
cross_entropy: {'epoch': 59, 'value': 0.4201925563812257} ({'split': 'test'})
Epoch 060
accuracy: {'epoch': 60, 'value': 0.90772} ({'split': 'train'})
cross_entropy: {'epoch': 60, 'value': 0.26771905602931967} ({'split': 'train'})
accuracy: {'epoch': 60, 'value': 0.8566999787092208} ({'split': 'test'})
cross_entropy: {'epoch': 60, 'value': 0.45452702134847645} ({'split': 'test'})
Epoch 061
accuracy: {'epoch': 61, 'value': 0.9078600000381472} ({'split': 'train'})
cross_entropy: {'epoch': 61, 'value': 0.2687121044921877} ({'split': 'train'})
accuracy: {'epoch': 61, 'value': 0.8653999757766723} ({'split': 'test'})
cross_entropy: {'epoch': 61, 'value': 0.41796377867460255} ({'split': 'test'})
Epoch 062
accuracy: {'epoch': 62, 'value': 0.9049999999809264} ({'split': 'train'})
cross_entropy: {'epoch': 62, 'value': 0.27908768525123606} ({'split': 'train'})
accuracy: {'epoch': 62, 'value': 0.8659999805688862} ({'split': 'test'})
cross_entropy: {'epoch': 62, 'value': 0.43983553752303134} ({'split': 'test'})
Epoch 063
accuracy: {'epoch': 63, 'value': 0.9076400000190733} ({'split': 'train'})
cross_entropy: {'epoch': 63, 'value': 0.26858105613231675} ({'split': 'train'})
accuracy: {'epoch': 63, 'value': 0.856999980211258} ({'split': 'test'})
cross_entropy: {'epoch': 63, 'value': 0.4707016226649284} ({'split': 'test'})
Epoch 064
accuracy: {'epoch': 64, 'value': 0.9097999999809265} ({'split': 'train'})
cross_entropy: {'epoch': 64, 'value': 0.26202196934700017} ({'split': 'train'})
accuracy: {'epoch': 64, 'value': 0.8662999778985976} ({'split': 'test'})
cross_entropy: {'epoch': 64, 'value': 0.4594796994328499} ({'split': 'test'})
Epoch 065
accuracy: {'epoch': 65, 'value': 0.9088400000572208} ({'split': 'train'})
cross_entropy: {'epoch': 65, 'value': 0.2692481049251556} ({'split': 'train'})
accuracy: {'epoch': 65, 'value': 0.8663999813795089} ({'split': 'test'})
cross_entropy: {'epoch': 65, 'value': 0.444481352120638} ({'split': 'test'})
Epoch 066
accuracy: {'epoch': 66, 'value': 0.9062599999809264} ({'split': 'train'})
cross_entropy: {'epoch': 66, 'value': 0.26986537788391113} ({'split': 'train'})
accuracy: {'epoch': 66, 'value': 0.8656999808549878} ({'split': 'test'})
cross_entropy: {'epoch': 66, 'value': 0.42784491643309575} ({'split': 'test'})
Epoch 067
accuracy: {'epoch': 67, 'value': 0.9089599999809266} ({'split': 'train'})
cross_entropy: {'epoch': 67, 'value': 0.26940454111099243} ({'split': 'train'})
accuracy: {'epoch': 67, 'value': 0.8504999804496765} ({'split': 'test'})
cross_entropy: {'epoch': 67, 'value': 0.46133630856871616} ({'split': 'test'})
Epoch 068
accuracy: {'epoch': 68, 'value': 0.9082799999809265} ({'split': 'train'})
cross_entropy: {'epoch': 68, 'value': 0.26647349486351013} ({'split': 'train'})
accuracy: {'epoch': 68, 'value': 0.8681999832391741} ({'split': 'test'})
cross_entropy: {'epoch': 68, 'value': 0.4230138272047042} ({'split': 'test'})
Epoch 069
accuracy: {'epoch': 69, 'value': 0.9075400000381472} ({'split': 'train'})
cross_entropy: {'epoch': 69, 'value': 0.26481896059989923} ({'split': 'train'})
accuracy: {'epoch': 69, 'value': 0.8609999799728395} ({'split': 'test'})
cross_entropy: {'epoch': 69, 'value': 0.4288109056651592} ({'split': 'test'})
Epoch 070
accuracy: {'epoch': 70, 'value': 0.9099400000381471} ({'split': 'train'})
cross_entropy: {'epoch': 70, 'value': 0.26320908260822296} ({'split': 'train'})
accuracy: {'epoch': 70, 'value': 0.85289998292923} ({'split': 'test'})
cross_entropy: {'epoch': 70, 'value': 0.4693955899775028} ({'split': 'test'})
Epoch 071
accuracy: {'epoch': 71, 'value': 0.91420000005722} ({'split': 'train'})
cross_entropy: {'epoch': 71, 'value': 0.25089743328571323} ({'split': 'train'})
accuracy: {'epoch': 71, 'value': 0.8645999801158907} ({'split': 'test'})
cross_entropy: {'epoch': 71, 'value': 0.44417626023292545} ({'split': 'test'})
Epoch 072
accuracy: {'epoch': 72, 'value': 0.9113000000190736} ({'split': 'train'})
cross_entropy: {'epoch': 72, 'value': 0.25727576941490177} ({'split': 'train'})
accuracy: {'epoch': 72, 'value': 0.8696999788284303} ({'split': 'test'})
cross_entropy: {'epoch': 72, 'value': 0.4374035903811457} ({'split': 'test'})
Epoch 073
accuracy: {'epoch': 73, 'value': 0.9096399999809266} ({'split': 'train'})
cross_entropy: {'epoch': 73, 'value': 0.26585685928344727} ({'split': 'train'})
accuracy: {'epoch': 73, 'value': 0.8599999791383741} ({'split': 'test'})
cross_entropy: {'epoch': 73, 'value': 0.459281601011753} ({'split': 'test'})
Epoch 074
accuracy: {'epoch': 74, 'value': 0.9097999999999999} ({'split': 'train'})
cross_entropy: {'epoch': 74, 'value': 0.26325862312793696} ({'split': 'train'})
accuracy: {'epoch': 74, 'value': 0.8637999796867369} ({'split': 'test'})
cross_entropy: {'epoch': 74, 'value': 0.43445496216416357} ({'split': 'test'})
Epoch 075
accuracy: {'epoch': 75, 'value': 0.9133399999999997} ({'split': 'train'})
cross_entropy: {'epoch': 75, 'value': 0.25377684988975546} ({'split': 'train'})
accuracy: {'epoch': 75, 'value': 0.861599979996681} ({'split': 'test'})
cross_entropy: {'epoch': 75, 'value': 0.45309910073876397} ({'split': 'test'})
Epoch 076
accuracy: {'epoch': 76, 'value': 0.9136000000190736} ({'split': 'train'})
cross_entropy: {'epoch': 76, 'value': 0.2532813443279266} ({'split': 'train'})
accuracy: {'epoch': 76, 'value': 0.8626999783515932} ({'split': 'test'})
cross_entropy: {'epoch': 76, 'value': 0.4403577998280526} ({'split': 'test'})
Epoch 077
accuracy: {'epoch': 77, 'value': 0.9137199999809267} ({'split': 'train'})
cross_entropy: {'epoch': 77, 'value': 0.2554656265830993} ({'split': 'train'})
accuracy: {'epoch': 77, 'value': 0.8655999797582625} ({'split': 'test'})
cross_entropy: {'epoch': 77, 'value': 0.41981020018458354} ({'split': 'test'})
Epoch 078
accuracy: {'epoch': 78, 'value': 0.9116200000381471} ({'split': 'train'})
cross_entropy: {'epoch': 78, 'value': 0.2594167004680634} ({'split': 'train'})
accuracy: {'epoch': 78, 'value': 0.8641999810934068} ({'split': 'test'})
cross_entropy: {'epoch': 78, 'value': 0.4527736595273018} ({'split': 'test'})
Epoch 079
accuracy: {'epoch': 79, 'value': 0.9149} ({'split': 'train'})
cross_entropy: {'epoch': 79, 'value': 0.2559409370088577} ({'split': 'train'})
accuracy: {'epoch': 79, 'value': 0.8556999784708023} ({'split': 'test'})
cross_entropy: {'epoch': 79, 'value': 0.49178431898355496} ({'split': 'test'})
Epoch 080
accuracy: {'epoch': 80, 'value': 0.9145799999809263} ({'split': 'train'})
cross_entropy: {'epoch': 80, 'value': 0.24981653718948338} ({'split': 'train'})
accuracy: {'epoch': 80, 'value': 0.862899979352951} ({'split': 'test'})
cross_entropy: {'epoch': 80, 'value': 0.4470517319440843} ({'split': 'test'})
Epoch 081
accuracy: {'epoch': 81, 'value': 0.9127400000381467} ({'split': 'train'})
cross_entropy: {'epoch': 81, 'value': 0.2539499540281295} ({'split': 'train'})
accuracy: {'epoch': 81, 'value': 0.8660999792814255} ({'split': 'test'})
cross_entropy: {'epoch': 81, 'value': 0.4234448812901974} ({'split': 'test'})
Epoch 082
accuracy: {'epoch': 82, 'value': 0.9120999999999998} ({'split': 'train'})
cross_entropy: {'epoch': 82, 'value': 0.2552369438552857} ({'split': 'train'})
accuracy: {'epoch': 82, 'value': 0.8691999787092208} ({'split': 'test'})
cross_entropy: {'epoch': 82, 'value': 0.41057437241077427} ({'split': 'test'})
Epoch 083
accuracy: {'epoch': 83, 'value': 0.9171600000381465} ({'split': 'train'})
cross_entropy: {'epoch': 83, 'value': 0.24256989784479135} ({'split': 'train'})
accuracy: {'epoch': 83, 'value': 0.864899978041649} ({'split': 'test'})
cross_entropy: {'epoch': 83, 'value': 0.43016155183315274} ({'split': 'test'})
Epoch 084
accuracy: {'epoch': 84, 'value': 0.9119400000190736} ({'split': 'train'})
cross_entropy: {'epoch': 84, 'value': 0.255351415271759} ({'split': 'train'})
accuracy: {'epoch': 84, 'value': 0.872899980545044} ({'split': 'test'})
cross_entropy: {'epoch': 84, 'value': 0.4055715763568877} ({'split': 'test'})
Epoch 085
accuracy: {'epoch': 85, 'value': 0.9147000000381466} ({'split': 'train'})
cross_entropy: {'epoch': 85, 'value': 0.2498748885154725} ({'split': 'train'})
accuracy: {'epoch': 85, 'value': 0.8692999798059463} ({'split': 'test'})
cross_entropy: {'epoch': 85, 'value': 0.4165671621263027} ({'split': 'test'})
Epoch 086
accuracy: {'epoch': 86, 'value': 0.917960000038147} ({'split': 'train'})
cross_entropy: {'epoch': 86, 'value': 0.23879562903404233} ({'split': 'train'})
accuracy: {'epoch': 86, 'value': 0.8604999804496763} ({'split': 'test'})
cross_entropy: {'epoch': 86, 'value': 0.43904620990157117} ({'split': 'test'})
Epoch 087
accuracy: {'epoch': 87, 'value': 0.9169800000000001} ({'split': 'train'})
cross_entropy: {'epoch': 87, 'value': 0.24242991075515744} ({'split': 'train'})
accuracy: {'epoch': 87, 'value': 0.8646999782323839} ({'split': 'test'})
cross_entropy: {'epoch': 87, 'value': 0.4315330693125724} ({'split': 'test'})
Epoch 088
accuracy: {'epoch': 88, 'value': 0.9136399999809264} ({'split': 'train'})
cross_entropy: {'epoch': 88, 'value': 0.2539282874727247} ({'split': 'train'})
accuracy: {'epoch': 88, 'value': 0.8624999809265136} ({'split': 'test'})
cross_entropy: {'epoch': 88, 'value': 0.42175542637705793} ({'split': 'test'})
Epoch 089
accuracy: {'epoch': 89, 'value': 0.9176799999999995} ({'split': 'train'})
cross_entropy: {'epoch': 89, 'value': 0.24088000239849094} ({'split': 'train'})
accuracy: {'epoch': 89, 'value': 0.8713999813795089} ({'split': 'test'})
cross_entropy: {'epoch': 89, 'value': 0.4176638491451738} ({'split': 'test'})
Epoch 090
accuracy: {'epoch': 90, 'value': 0.9148799999809267} ({'split': 'train'})
cross_entropy: {'epoch': 90, 'value': 0.2447270876932145} ({'split': 'train'})
accuracy: {'epoch': 90, 'value': 0.8671999788284305} ({'split': 'test'})
cross_entropy: {'epoch': 90, 'value': 0.436984591782093} ({'split': 'test'})
Epoch 091
accuracy: {'epoch': 91, 'value': 0.9153800000572202} ({'split': 'train'})
cross_entropy: {'epoch': 91, 'value': 0.24824415689468388} ({'split': 'train'})
accuracy: {'epoch': 91, 'value': 0.8586999768018723} ({'split': 'test'})
cross_entropy: {'epoch': 91, 'value': 0.4757486142218112} ({'split': 'test'})
Epoch 092
accuracy: {'epoch': 92, 'value': 0.9144800000572205} ({'split': 'train'})
cross_entropy: {'epoch': 92, 'value': 0.25194429127693174} ({'split': 'train'})
accuracy: {'epoch': 92, 'value': 0.8590999823808666} ({'split': 'test'})
cross_entropy: {'epoch': 92, 'value': 0.46480034276843074} ({'split': 'test'})
Epoch 093
accuracy: {'epoch': 93, 'value': 0.9138000000381472} ({'split': 'train'})
cross_entropy: {'epoch': 93, 'value': 0.2543686753940582} ({'split': 'train'})
accuracy: {'epoch': 93, 'value': 0.8637999784946443} ({'split': 'test'})
cross_entropy: {'epoch': 93, 'value': 0.43208897605538366} ({'split': 'test'})
Epoch 094
accuracy: {'epoch': 94, 'value': 0.9148199999999999} ({'split': 'train'})
cross_entropy: {'epoch': 94, 'value': 0.2507895878982546} ({'split': 'train'})
accuracy: {'epoch': 94, 'value': 0.865299977064133} ({'split': 'test'})
cross_entropy: {'epoch': 94, 'value': 0.4351564107835293} ({'split': 'test'})
Epoch 095
accuracy: {'epoch': 95, 'value': 0.9171399999809263} ({'split': 'train'})
cross_entropy: {'epoch': 95, 'value': 0.24507889717102047} ({'split': 'train'})
accuracy: {'epoch': 95, 'value': 0.8532999759912492} ({'split': 'test'})
cross_entropy: {'epoch': 95, 'value': 0.46907044008374227} ({'split': 'test'})
Epoch 096
accuracy: {'epoch': 96, 'value': 0.9171399999809262} ({'split': 'train'})
cross_entropy: {'epoch': 96, 'value': 0.24745151659488673} ({'split': 'train'})
accuracy: {'epoch': 96, 'value': 0.8583999800682063} ({'split': 'test'})
cross_entropy: {'epoch': 96, 'value': 0.45878871083259565} ({'split': 'test'})
Epoch 097
accuracy: {'epoch': 97, 'value': 0.9149000000572198} ({'split': 'train'})
cross_entropy: {'epoch': 97, 'value': 0.24709812440872186} ({'split': 'train'})
accuracy: {'epoch': 97, 'value': 0.861599979400635} ({'split': 'test'})
cross_entropy: {'epoch': 97, 'value': 0.4276675632596017} ({'split': 'test'})
Epoch 098
accuracy: {'epoch': 98, 'value': 0.9151200000000002} ({'split': 'train'})
cross_entropy: {'epoch': 98, 'value': 0.24594826040267947} ({'split': 'train'})
accuracy: {'epoch': 98, 'value': 0.8697999799251558} ({'split': 'test'})
cross_entropy: {'epoch': 98, 'value': 0.4389871118962763} ({'split': 'test'})
Epoch 099
accuracy: {'epoch': 99, 'value': 0.9166200000572208} ({'split': 'train'})
cross_entropy: {'epoch': 99, 'value': 0.24644103837013245} ({'split': 'train'})
accuracy: {'epoch': 99, 'value': 0.8675999826192853} ({'split': 'test'})
cross_entropy: {'epoch': 99, 'value': 0.43124112829565997} ({'split': 'test'})
Epoch 100
accuracy: {'epoch': 100, 'value': 0.9124200000000002} ({'split': 'train'})
cross_entropy: {'epoch': 100, 'value': 0.25607147064685837} ({'split': 'train'})
accuracy: {'epoch': 100, 'value': 0.8731999820470809} ({'split': 'test'})
cross_entropy: {'epoch': 100, 'value': 0.41423366546630863} ({'split': 'test'})
Epoch 101
accuracy: {'epoch': 101, 'value': 0.9196400000381473} ({'split': 'train'})
cross_entropy: {'epoch': 101, 'value': 0.234565237903595} ({'split': 'train'})
accuracy: {'epoch': 101, 'value': 0.8661999809741975} ({'split': 'test'})
cross_entropy: {'epoch': 101, 'value': 0.47198581129312517} ({'split': 'test'})
Epoch 102
accuracy: {'epoch': 102, 'value': 0.9167799999809265} ({'split': 'train'})
cross_entropy: {'epoch': 102, 'value': 0.24287786146163942} ({'split': 'train'})
accuracy: {'epoch': 102, 'value': 0.8552999740839007} ({'split': 'test'})
cross_entropy: {'epoch': 102, 'value': 0.4763692502677442} ({'split': 'test'})
Epoch 103
accuracy: {'epoch': 103, 'value': 0.9175799999809264} ({'split': 'train'})
cross_entropy: {'epoch': 103, 'value': 0.24283517833232876} ({'split': 'train'})
accuracy: {'epoch': 103, 'value': 0.8595999795198442} ({'split': 'test'})
cross_entropy: {'epoch': 103, 'value': 0.46740271285176277} ({'split': 'test'})
Epoch 104
accuracy: {'epoch': 104, 'value': 0.9175799999809264} ({'split': 'train'})
cross_entropy: {'epoch': 104, 'value': 0.24038531556129447} ({'split': 'train'})
accuracy: {'epoch': 104, 'value': 0.8730999791622163} ({'split': 'test'})
cross_entropy: {'epoch': 104, 'value': 0.40860340356826763} ({'split': 'test'})
Epoch 105
accuracy: {'epoch': 105, 'value': 0.9170800000381466} ({'split': 'train'})
cross_entropy: {'epoch': 105, 'value': 0.23746812049865718} ({'split': 'train'})
accuracy: {'epoch': 105, 'value': 0.86299997985363} ({'split': 'test'})
cross_entropy: {'epoch': 105, 'value': 0.45884993746876723} ({'split': 'test'})
Epoch 106
accuracy: {'epoch': 106, 'value': 0.9179799999809264} ({'split': 'train'})
cross_entropy: {'epoch': 106, 'value': 0.24156409118175506} ({'split': 'train'})
accuracy: {'epoch': 106, 'value': 0.8636999809741975} ({'split': 'test'})
cross_entropy: {'epoch': 106, 'value': 0.449006900936365} ({'split': 'test'})
Epoch 107
accuracy: {'epoch': 107, 'value': 0.9197400000381469} ({'split': 'train'})
cross_entropy: {'epoch': 107, 'value': 0.23406001557350156} ({'split': 'train'})
accuracy: {'epoch': 107, 'value': 0.867199980020523} ({'split': 'test'})
cross_entropy: {'epoch': 107, 'value': 0.43221421301364904} ({'split': 'test'})
Epoch 108
accuracy: {'epoch': 108, 'value': 0.9201399999999997} ({'split': 'train'})
cross_entropy: {'epoch': 108, 'value': 0.23557118845939634} ({'split': 'train'})
accuracy: {'epoch': 108, 'value': 0.8536999791860581} ({'split': 'test'})
cross_entropy: {'epoch': 108, 'value': 0.47633531630039216} ({'split': 'test'})
Epoch 109
accuracy: {'epoch': 109, 'value': 0.9174000000190735} ({'split': 'train'})
cross_entropy: {'epoch': 109, 'value': 0.24206224815368654} ({'split': 'train'})
accuracy: {'epoch': 109, 'value': 0.8577999800443651} ({'split': 'test'})
cross_entropy: {'epoch': 109, 'value': 0.49605088591575597} ({'split': 'test'})
Epoch 110
accuracy: {'epoch': 110, 'value': 0.92240000005722} ({'split': 'train'})
cross_entropy: {'epoch': 110, 'value': 0.22952003057479858} ({'split': 'train'})
accuracy: {'epoch': 110, 'value': 0.8731999802589414} ({'split': 'test'})
cross_entropy: {'epoch': 110, 'value': 0.4256032143533229} ({'split': 'test'})
Epoch 111
accuracy: {'epoch': 111, 'value': 0.9141599999809263} ({'split': 'train'})
cross_entropy: {'epoch': 111, 'value': 0.2517928748798369} ({'split': 'train'})
accuracy: {'epoch': 111, 'value': 0.85719997882843} ({'split': 'test'})
cross_entropy: {'epoch': 111, 'value': 0.48104068219661694} ({'split': 'test'})
Epoch 112
accuracy: {'epoch': 112, 'value': 0.9171199999809269} ({'split': 'train'})
cross_entropy: {'epoch': 112, 'value': 0.2399327030706406} ({'split': 'train'})
accuracy: {'epoch': 112, 'value': 0.8659999799728394} ({'split': 'test'})
cross_entropy: {'epoch': 112, 'value': 0.42747415959835044} ({'split': 'test'})
Epoch 113
accuracy: {'epoch': 113, 'value': 0.9220200000190739} ({'split': 'train'})
cross_entropy: {'epoch': 113, 'value': 0.23196456715106956} ({'split': 'train'})
accuracy: {'epoch': 113, 'value': 0.8508999794721602} ({'split': 'test'})
cross_entropy: {'epoch': 113, 'value': 0.4598741887509823} ({'split': 'test'})
Epoch 114
accuracy: {'epoch': 114, 'value': 0.9180800000190735} ({'split': 'train'})
cross_entropy: {'epoch': 114, 'value': 0.2405107201004028} ({'split': 'train'})
accuracy: {'epoch': 114, 'value': 0.8718999779224397} ({'split': 'test'})
cross_entropy: {'epoch': 114, 'value': 0.42328569576144215} ({'split': 'test'})
Epoch 115
accuracy: {'epoch': 115, 'value': 0.9184400000572206} ({'split': 'train'})
cross_entropy: {'epoch': 115, 'value': 0.23982222243309026} ({'split': 'train'})
accuracy: {'epoch': 115, 'value': 0.8690999788045881} ({'split': 'test'})
cross_entropy: {'epoch': 115, 'value': 0.43497677922248834} ({'split': 'test'})
Epoch 116
accuracy: {'epoch': 116, 'value': 0.921440000038147} ({'split': 'train'})
cross_entropy: {'epoch': 116, 'value': 0.22994716662883757} ({'split': 'train'})
accuracy: {'epoch': 116, 'value': 0.8681999772787095} ({'split': 'test'})
cross_entropy: {'epoch': 116, 'value': 0.425246345847845} ({'split': 'test'})
Epoch 117
accuracy: {'epoch': 117, 'value': 0.9197199999809266} ({'split': 'train'})
cross_entropy: {'epoch': 117, 'value': 0.23398925065994258} ({'split': 'train'})
accuracy: {'epoch': 117, 'value': 0.8742999827861785} ({'split': 'test'})
cross_entropy: {'epoch': 117, 'value': 0.4158074025809764} ({'split': 'test'})
Epoch 118
accuracy: {'epoch': 118, 'value': 0.9217999999809262} ({'split': 'train'})
cross_entropy: {'epoch': 118, 'value': 0.23323445445060728} ({'split': 'train'})
accuracy: {'epoch': 118, 'value': 0.8726999813318255} ({'split': 'test'})
cross_entropy: {'epoch': 118, 'value': 0.43519254863262163} ({'split': 'test'})
Epoch 119
accuracy: {'epoch': 119, 'value': 0.9206199999809267} ({'split': 'train'})
cross_entropy: {'epoch': 119, 'value': 0.23106423334121692} ({'split': 'train'})
accuracy: {'epoch': 119, 'value': 0.8703999799489975} ({'split': 'test'})
cross_entropy: {'epoch': 119, 'value': 0.4289629945158959} ({'split': 'test'})
Epoch 120
accuracy: {'epoch': 120, 'value': 0.91888} ({'split': 'train'})
cross_entropy: {'epoch': 120, 'value': 0.23491609703540794} ({'split': 'train'})
accuracy: {'epoch': 120, 'value': 0.8551999801397323} ({'split': 'test'})
cross_entropy: {'epoch': 120, 'value': 0.4805252972245216} ({'split': 'test'})
Epoch 121
accuracy: {'epoch': 121, 'value': 0.9194000000572206} ({'split': 'train'})
cross_entropy: {'epoch': 121, 'value': 0.2355180704021455} ({'split': 'train'})
accuracy: {'epoch': 121, 'value': 0.8680999797582628} ({'split': 'test'})
cross_entropy: {'epoch': 121, 'value': 0.4517888222634792} ({'split': 'test'})
Epoch 122
accuracy: {'epoch': 122, 'value': 0.9180199999809268} ({'split': 'train'})
cross_entropy: {'epoch': 122, 'value': 0.24191809310436255} ({'split': 'train'})
accuracy: {'epoch': 122, 'value': 0.8670999795198441} ({'split': 'test'})
cross_entropy: {'epoch': 122, 'value': 0.46985594063997266} ({'split': 'test'})
Epoch 123
accuracy: {'epoch': 123, 'value': 0.9197400000381469} ({'split': 'train'})
cross_entropy: {'epoch': 123, 'value': 0.23727717176437377} ({'split': 'train'})
accuracy: {'epoch': 123, 'value': 0.8686999791860578} ({'split': 'test'})
cross_entropy: {'epoch': 123, 'value': 0.424404076486826} ({'split': 'test'})
Epoch 124
accuracy: {'epoch': 124, 'value': 0.9176799999809262} ({'split': 'train'})
cross_entropy: {'epoch': 124, 'value': 0.2390250045680999} ({'split': 'train'})
accuracy: {'epoch': 124, 'value': 0.8705999785661698} ({'split': 'test'})
cross_entropy: {'epoch': 124, 'value': 0.39803691327571855} ({'split': 'test'})
Epoch 125
accuracy: {'epoch': 125, 'value': 0.9203399999809264} ({'split': 'train'})
cross_entropy: {'epoch': 125, 'value': 0.2339997072219849} ({'split': 'train'})
accuracy: {'epoch': 125, 'value': 0.8711999791860576} ({'split': 'test'})
cross_entropy: {'epoch': 125, 'value': 0.3922172272205351} ({'split': 'test'})
Epoch 126
accuracy: {'epoch': 126, 'value': 0.9232000000381471} ({'split': 'train'})
cross_entropy: {'epoch': 126, 'value': 0.22514732182502742} ({'split': 'train'})
accuracy: {'epoch': 126, 'value': 0.8729999786615372} ({'split': 'test'})
cross_entropy: {'epoch': 126, 'value': 0.41610081553459166} ({'split': 'test'})
Epoch 127
accuracy: {'epoch': 127, 'value': 0.9206799999809263} ({'split': 'train'})
cross_entropy: {'epoch': 127, 'value': 0.2305199477815628} ({'split': 'train'})
accuracy: {'epoch': 127, 'value': 0.8636999773979184} ({'split': 'test'})
cross_entropy: {'epoch': 127, 'value': 0.45245092362165457} ({'split': 'test'})
Epoch 128
accuracy: {'epoch': 128, 'value': 0.9189000000190738} ({'split': 'train'})
cross_entropy: {'epoch': 128, 'value': 0.23644019790649426} ({'split': 'train'})
accuracy: {'epoch': 128, 'value': 0.8543999814987183} ({'split': 'test'})
cross_entropy: {'epoch': 128, 'value': 0.47781895011663433} ({'split': 'test'})
Epoch 129
accuracy: {'epoch': 129, 'value': 0.9218400000000002} ({'split': 'train'})
cross_entropy: {'epoch': 129, 'value': 0.22678259043693544} ({'split': 'train'})
accuracy: {'epoch': 129, 'value': 0.864699977636337} ({'split': 'test'})
cross_entropy: {'epoch': 129, 'value': 0.4313933984935285} ({'split': 'test'})
Epoch 130
accuracy: {'epoch': 130, 'value': 0.9190999999999997} ({'split': 'train'})
cross_entropy: {'epoch': 130, 'value': 0.2410029630851745} ({'split': 'train'})
accuracy: {'epoch': 130, 'value': 0.8743999791145324} ({'split': 'test'})
cross_entropy: {'epoch': 130, 'value': 0.40249978706240624} ({'split': 'test'})
Epoch 131
accuracy: {'epoch': 131, 'value': 0.9211600000381467} ({'split': 'train'})
cross_entropy: {'epoch': 131, 'value': 0.23033281712055212} ({'split': 'train'})
accuracy: {'epoch': 131, 'value': 0.8676999819278716} ({'split': 'test'})
cross_entropy: {'epoch': 131, 'value': 0.43064008206129056} ({'split': 'test'})
Epoch 132
accuracy: {'epoch': 132, 'value': 0.9201000000381466} ({'split': 'train'})
cross_entropy: {'epoch': 132, 'value': 0.23382065386772155} ({'split': 'train'})
accuracy: {'epoch': 132, 'value': 0.8717999821901324} ({'split': 'test'})
cross_entropy: {'epoch': 132, 'value': 0.4382103176414966} ({'split': 'test'})
Epoch 133
accuracy: {'epoch': 133, 'value': 0.919400000038147} ({'split': 'train'})
cross_entropy: {'epoch': 133, 'value': 0.23230684828758238} ({'split': 'train'})
accuracy: {'epoch': 133, 'value': 0.876499981880188} ({'split': 'test'})
cross_entropy: {'epoch': 133, 'value': 0.4240482495725154} ({'split': 'test'})
We have a new best! with accuracy::0.876499981880188 and at epoch::133, let's save it!
Epoch 134
accuracy: {'epoch': 134, 'value': 0.9197000000000002} ({'split': 'train'})
cross_entropy: {'epoch': 134, 'value': 0.23338117297172548} ({'split': 'train'})
accuracy: {'epoch': 134, 'value': 0.8592999804019928} ({'split': 'test'})
cross_entropy: {'epoch': 134, 'value': 0.48968952849507325} ({'split': 'test'})
Epoch 135
accuracy: {'epoch': 135, 'value': 0.9212399999809273} ({'split': 'train'})
cross_entropy: {'epoch': 135, 'value': 0.23300209599018099} ({'split': 'train'})
accuracy: {'epoch': 135, 'value': 0.8641999781131742} ({'split': 'test'})
cross_entropy: {'epoch': 135, 'value': 0.44275530561804777} ({'split': 'test'})
Epoch 136
accuracy: {'epoch': 136, 'value': 0.9223800000190732} ({'split': 'train'})
cross_entropy: {'epoch': 136, 'value': 0.22718821806430822} ({'split': 'train'})
accuracy: {'epoch': 136, 'value': 0.8467999804019927} ({'split': 'test'})
cross_entropy: {'epoch': 136, 'value': 0.5251957121491432} ({'split': 'test'})
Epoch 137
accuracy: {'epoch': 137, 'value': 0.9199400000190733} ({'split': 'train'})
cross_entropy: {'epoch': 137, 'value': 0.23528509082317348} ({'split': 'train'})
accuracy: {'epoch': 137, 'value': 0.876899979710579} ({'split': 'test'})
cross_entropy: {'epoch': 137, 'value': 0.40195892244577414} ({'split': 'test'})
We have a new best! with accuracy::0.876899979710579 and at epoch::137, let's save it!
Epoch 138
accuracy: {'epoch': 138, 'value': 0.9223400000572203} ({'split': 'train'})
cross_entropy: {'epoch': 138, 'value': 0.22377762113571162} ({'split': 'train'})
accuracy: {'epoch': 138, 'value': 0.8734999829530717} ({'split': 'test'})
cross_entropy: {'epoch': 138, 'value': 0.4248794901371004} ({'split': 'test'})
Epoch 139
accuracy: {'epoch': 139, 'value': 0.9217600000000002} ({'split': 'train'})
cross_entropy: {'epoch': 139, 'value': 0.23121657915115357} ({'split': 'train'})
accuracy: {'epoch': 139, 'value': 0.8683999782800675} ({'split': 'test'})
cross_entropy: {'epoch': 139, 'value': 0.41571404144167884} ({'split': 'test'})
Epoch 140
accuracy: {'epoch': 140, 'value': 0.9237200000190734} ({'split': 'train'})
cross_entropy: {'epoch': 140, 'value': 0.2244068699741364} ({'split': 'train'})
accuracy: {'epoch': 140, 'value': 0.8719999778270717} ({'split': 'test'})
cross_entropy: {'epoch': 140, 'value': 0.4273400363326073} ({'split': 'test'})
Epoch 141
accuracy: {'epoch': 141, 'value': 0.9199800000572207} ({'split': 'train'})
cross_entropy: {'epoch': 141, 'value': 0.23289299349784856} ({'split': 'train'})
accuracy: {'epoch': 141, 'value': 0.870199982523918} ({'split': 'test'})
cross_entropy: {'epoch': 141, 'value': 0.41999446585774414} ({'split': 'test'})
Epoch 142
accuracy: {'epoch': 142, 'value': 0.9264399999999999} ({'split': 'train'})
cross_entropy: {'epoch': 142, 'value': 0.2186777959918976} ({'split': 'train'})
accuracy: {'epoch': 142, 'value': 0.8571999824047091} ({'split': 'test'})
cross_entropy: {'epoch': 142, 'value': 0.4594060850143432} ({'split': 'test'})
Epoch 143
accuracy: {'epoch': 143, 'value': 0.9244200000381466} ({'split': 'train'})
cross_entropy: {'epoch': 143, 'value': 0.221292748222351} ({'split': 'train'})
accuracy: {'epoch': 143, 'value': 0.8690999788045883} ({'split': 'test'})
cross_entropy: {'epoch': 143, 'value': 0.40471979334950453} ({'split': 'test'})
Epoch 144
accuracy: {'epoch': 144, 'value': 0.9231000000572205} ({'split': 'train'})
cross_entropy: {'epoch': 144, 'value': 0.2252389735984802} ({'split': 'train'})
accuracy: {'epoch': 144, 'value': 0.8666999804973602} ({'split': 'test'})
cross_entropy: {'epoch': 144, 'value': 0.4367763330042362} ({'split': 'test'})
Epoch 145
accuracy: {'epoch': 145, 'value': 0.9269800000572208} ({'split': 'train'})
cross_entropy: {'epoch': 145, 'value': 0.21442153866767877} ({'split': 'train'})
accuracy: {'epoch': 145, 'value': 0.8708999776840209} ({'split': 'test'})
cross_entropy: {'epoch': 145, 'value': 0.4480184487998485} ({'split': 'test'})
Epoch 146
accuracy: {'epoch': 146, 'value': 0.9219200000000003} ({'split': 'train'})
cross_entropy: {'epoch': 146, 'value': 0.2271494612979889} ({'split': 'train'})
accuracy: {'epoch': 146, 'value': 0.861299979686737} ({'split': 'test'})
cross_entropy: {'epoch': 146, 'value': 0.45625026017427456} ({'split': 'test'})
Epoch 147
accuracy: {'epoch': 147, 'value': 0.9255800000381471} ({'split': 'train'})
cross_entropy: {'epoch': 147, 'value': 0.21630773021459582} ({'split': 'train'})
accuracy: {'epoch': 147, 'value': 0.8723999828100206} ({'split': 'test'})
cross_entropy: {'epoch': 147, 'value': 0.433188559859991} ({'split': 'test'})
Epoch 148
accuracy: {'epoch': 148, 'value': 0.926} ({'split': 'train'})
cross_entropy: {'epoch': 148, 'value': 0.2179893345499038} ({'split': 'train'})
accuracy: {'epoch': 148, 'value': 0.8672999811172485} ({'split': 'test'})
cross_entropy: {'epoch': 148, 'value': 0.44772177979350103} ({'split': 'test'})
Epoch 149
accuracy: {'epoch': 149, 'value': 0.9239800000190732} ({'split': 'train'})
cross_entropy: {'epoch': 149, 'value': 0.2238351197624207} ({'split': 'train'})
accuracy: {'epoch': 149, 'value': 0.8631999802589417} ({'split': 'test'})
cross_entropy: {'epoch': 149, 'value': 0.43889192774891844} ({'split': 'test'})
Epoch 150
accuracy: {'epoch': 150, 'value': 0.9543800000000002} ({'split': 'train'})
cross_entropy: {'epoch': 150, 'value': 0.13514925626277913} ({'split': 'train'})
accuracy: {'epoch': 150, 'value': 0.8949999791383743} ({'split': 'test'})
cross_entropy: {'epoch': 150, 'value': 0.35119584389030933} ({'split': 'test'})
We have a new best! with accuracy::0.8949999791383743 and at epoch::150, let's save it!
Epoch 151
accuracy: {'epoch': 151, 'value': 0.96496} ({'split': 'train'})
cross_entropy: {'epoch': 151, 'value': 0.10131762576580045} ({'split': 'train'})
accuracy: {'epoch': 151, 'value': 0.8994999802112579} ({'split': 'test'})
cross_entropy: {'epoch': 151, 'value': 0.3531878639012575} ({'split': 'test'})
We have a new best! with accuracy::0.8994999802112579 and at epoch::151, let's save it!
Epoch 152
accuracy: {'epoch': 152, 'value': 0.968220000038147} ({'split': 'train'})
cross_entropy: {'epoch': 152, 'value': 0.09255573899030686} ({'split': 'train'})
accuracy: {'epoch': 152, 'value': 0.9004999828338622} ({'split': 'test'})
cross_entropy: {'epoch': 152, 'value': 0.34719198726117617} ({'split': 'test'})
We have a new best! with accuracy::0.9004999828338622 and at epoch::152, let's save it!
Epoch 153
accuracy: {'epoch': 153, 'value': 0.9708800000190733} ({'split': 'train'})
cross_entropy: {'epoch': 153, 'value': 0.08226120112001896} ({'split': 'train'})
accuracy: {'epoch': 153, 'value': 0.9036999803781509} ({'split': 'test'})
cross_entropy: {'epoch': 153, 'value': 0.3526391910016536} ({'split': 'test'})
We have a new best! with accuracy::0.9036999803781509 and at epoch::153, let's save it!
Epoch 154
accuracy: {'epoch': 154, 'value': 0.9717600000572205} ({'split': 'train'})
cross_entropy: {'epoch': 154, 'value': 0.07960650401592248} ({'split': 'train'})
accuracy: {'epoch': 154, 'value': 0.9016999799013139} ({'split': 'test'})
cross_entropy: {'epoch': 154, 'value': 0.3591848449409007} ({'split': 'test'})
Epoch 155
accuracy: {'epoch': 155, 'value': 0.9736800000000001} ({'split': 'train'})
cross_entropy: {'epoch': 155, 'value': 0.07442844779610638} ({'split': 'train'})
accuracy: {'epoch': 155, 'value': 0.9050999808311461} ({'split': 'test'})
cross_entropy: {'epoch': 155, 'value': 0.3512879977375269} ({'split': 'test'})
We have a new best! with accuracy::0.9050999808311461 and at epoch::155, let's save it!
Epoch 156
accuracy: {'epoch': 156, 'value': 0.9759200000572207} ({'split': 'train'})
cross_entropy: {'epoch': 156, 'value': 0.06812494734287262} ({'split': 'train'})
accuracy: {'epoch': 156, 'value': 0.9047999823093412} ({'split': 'test'})
cross_entropy: {'epoch': 156, 'value': 0.35454834640026084} ({'split': 'test'})
Epoch 157
accuracy: {'epoch': 157, 'value': 0.9763200000381469} ({'split': 'train'})
cross_entropy: {'epoch': 157, 'value': 0.06537703195333482} ({'split': 'train'})
accuracy: {'epoch': 157, 'value': 0.9046999835968017} ({'split': 'test'})
cross_entropy: {'epoch': 157, 'value': 0.36388093546032896} ({'split': 'test'})
Epoch 158
accuracy: {'epoch': 158, 'value': 0.9770800000572203} ({'split': 'train'})
cross_entropy: {'epoch': 158, 'value': 0.06494261215686799} ({'split': 'train'})
accuracy: {'epoch': 158, 'value': 0.906499981880188} ({'split': 'test'})
cross_entropy: {'epoch': 158, 'value': 0.36752479732036597} ({'split': 'test'})
We have a new best! with accuracy::0.906499981880188 and at epoch::158, let's save it!
Epoch 159
accuracy: {'epoch': 159, 'value': 0.9804599999809265} ({'split': 'train'})
cross_entropy: {'epoch': 159, 'value': 0.058540691952705386} ({'split': 'train'})
accuracy: {'epoch': 159, 'value': 0.907199981212616} ({'split': 'test'})
cross_entropy: {'epoch': 159, 'value': 0.36556736573576926} ({'split': 'test'})
We have a new best! with accuracy::0.907199981212616 and at epoch::159, let's save it!
Epoch 160
accuracy: {'epoch': 160, 'value': 0.9807} ({'split': 'train'})
cross_entropy: {'epoch': 160, 'value': 0.05466321080893277} ({'split': 'train'})
accuracy: {'epoch': 160, 'value': 0.9066999822854996} ({'split': 'test'})
cross_entropy: {'epoch': 160, 'value': 0.36647631607949727} ({'split': 'test'})
Epoch 161
accuracy: {'epoch': 161, 'value': 0.9811999999999997} ({'split': 'train'})
cross_entropy: {'epoch': 161, 'value': 0.05419887935578821} ({'split': 'train'})
accuracy: {'epoch': 161, 'value': 0.907399981021881} ({'split': 'test'})
cross_entropy: {'epoch': 161, 'value': 0.36800637640058986} ({'split': 'test'})
We have a new best! with accuracy::0.907399981021881 and at epoch::161, let's save it!
Epoch 162
accuracy: {'epoch': 162, 'value': 0.9814400000572208} ({'split': 'train'})
cross_entropy: {'epoch': 162, 'value': 0.05316897674083708} ({'split': 'train'})
accuracy: {'epoch': 162, 'value': 0.9096999830007553} ({'split': 'test'})
cross_entropy: {'epoch': 162, 'value': 0.370244625583291} ({'split': 'test'})
We have a new best! with accuracy::0.9096999830007553 and at epoch::162, let's save it!
Epoch 163
accuracy: {'epoch': 163, 'value': 0.982180000038147} ({'split': 'train'})
cross_entropy: {'epoch': 163, 'value': 0.05123050425529481} ({'split': 'train'})
accuracy: {'epoch': 163, 'value': 0.907599982023239} ({'split': 'test'})
cross_entropy: {'epoch': 163, 'value': 0.36840142771601686} ({'split': 'test'})
Epoch 164
accuracy: {'epoch': 164, 'value': 0.9823000000572204} ({'split': 'train'})
cross_entropy: {'epoch': 164, 'value': 0.049103960344791385} ({'split': 'train'})
accuracy: {'epoch': 164, 'value': 0.9088999825716018} ({'split': 'test'})
cross_entropy: {'epoch': 164, 'value': 0.37695671185851104} ({'split': 'test'})
Epoch 165
accuracy: {'epoch': 165, 'value': 0.983580000019073} ({'split': 'train'})
cross_entropy: {'epoch': 165, 'value': 0.047312500157952304} ({'split': 'train'})
accuracy: {'epoch': 165, 'value': 0.9108999866247177} ({'split': 'test'})
cross_entropy: {'epoch': 165, 'value': 0.36771880909800525} ({'split': 'test'})
We have a new best! with accuracy::0.9108999866247177 and at epoch::165, let's save it!
Epoch 166
accuracy: {'epoch': 166, 'value': 0.9847400000381471} ({'split': 'train'})
cross_entropy: {'epoch': 166, 'value': 0.04345057827472688} ({'split': 'train'})
accuracy: {'epoch': 166, 'value': 0.9092999827861787} ({'split': 'test'})
cross_entropy: {'epoch': 166, 'value': 0.38636822678148747} ({'split': 'test'})
Epoch 167
accuracy: {'epoch': 167, 'value': 0.9846600000000001} ({'split': 'train'})
cross_entropy: {'epoch': 167, 'value': 0.04383123848199846} ({'split': 'train'})
accuracy: {'epoch': 167, 'value': 0.9094999819993972} ({'split': 'test'})
cross_entropy: {'epoch': 167, 'value': 0.38588980242609977} ({'split': 'test'})
Epoch 168
accuracy: {'epoch': 168, 'value': 0.9851600000572204} ({'split': 'train'})
cross_entropy: {'epoch': 168, 'value': 0.0427782496356964} ({'split': 'train'})
accuracy: {'epoch': 168, 'value': 0.908799982666969} ({'split': 'test'})
cross_entropy: {'epoch': 168, 'value': 0.3830504440516234} ({'split': 'test'})
Epoch 169
accuracy: {'epoch': 169, 'value': 0.98542} ({'split': 'train'})
cross_entropy: {'epoch': 169, 'value': 0.04043231822907924} ({'split': 'train'})
accuracy: {'epoch': 169, 'value': 0.907899981737137} ({'split': 'test'})
cross_entropy: {'epoch': 169, 'value': 0.3907038486003876} ({'split': 'test'})
Epoch 170
accuracy: {'epoch': 170, 'value': 0.9862400000572206} ({'split': 'train'})
cross_entropy: {'epoch': 170, 'value': 0.039935243992805486} ({'split': 'train'})
accuracy: {'epoch': 170, 'value': 0.9112999814748767} ({'split': 'test'})
cross_entropy: {'epoch': 170, 'value': 0.3945640087127686} ({'split': 'test'})
We have a new best! with accuracy::0.9112999814748767 and at epoch::170, let's save it!
Epoch 171
accuracy: {'epoch': 171, 'value': 0.9866400000190731} ({'split': 'train'})
cross_entropy: {'epoch': 171, 'value': 0.039030954135656375} ({'split': 'train'})
accuracy: {'epoch': 171, 'value': 0.9087999832630157} ({'split': 'test'})
cross_entropy: {'epoch': 171, 'value': 0.4081948494911194} ({'split': 'test'})
Epoch 172
accuracy: {'epoch': 172, 'value': 0.987139999980926} ({'split': 'train'})
cross_entropy: {'epoch': 172, 'value': 0.03645821736812591} ({'split': 'train'})
accuracy: {'epoch': 172, 'value': 0.9110999810695651} ({'split': 'test'})
cross_entropy: {'epoch': 172, 'value': 0.40458992019295675} ({'split': 'test'})
Epoch 173
accuracy: {'epoch': 173, 'value': 0.9866399999999999} ({'split': 'train'})
cross_entropy: {'epoch': 173, 'value': 0.03787618007600308} ({'split': 'train'})
accuracy: {'epoch': 173, 'value': 0.9111999803781509} ({'split': 'test'})
cross_entropy: {'epoch': 173, 'value': 0.3911990951746702} ({'split': 'test'})
Epoch 174
accuracy: {'epoch': 174, 'value': 0.98826} ({'split': 'train'})
cross_entropy: {'epoch': 174, 'value': 0.0332638277256489} ({'split': 'train'})
accuracy: {'epoch': 174, 'value': 0.9117999792099002} ({'split': 'test'})
cross_entropy: {'epoch': 174, 'value': 0.4059120734035968} ({'split': 'test'})
We have a new best! with accuracy::0.9117999792099002 and at epoch::174, let's save it!
Epoch 175
accuracy: {'epoch': 175, 'value': 0.9874400000190736} ({'split': 'train'})
cross_entropy: {'epoch': 175, 'value': 0.034573396282196064} ({'split': 'train'})
accuracy: {'epoch': 175, 'value': 0.9115999817848204} ({'split': 'test'})
cross_entropy: {'epoch': 175, 'value': 0.40324547037482267} ({'split': 'test'})
Epoch 176
accuracy: {'epoch': 176, 'value': 0.9889600000381471} ({'split': 'train'})
cross_entropy: {'epoch': 176, 'value': 0.03301120076179502} ({'split': 'train'})
accuracy: {'epoch': 176, 'value': 0.9115999817848207} ({'split': 'test'})
cross_entropy: {'epoch': 176, 'value': 0.4058815275132655} ({'split': 'test'})
Epoch 177
accuracy: {'epoch': 177, 'value': 0.9886599999999998} ({'split': 'train'})
cross_entropy: {'epoch': 177, 'value': 0.03226149027496579} ({'split': 'train'})
accuracy: {'epoch': 177, 'value': 0.9109999835491178} ({'split': 'test'})
cross_entropy: {'epoch': 177, 'value': 0.4058057223260403} ({'split': 'test'})
Epoch 178
accuracy: {'epoch': 178, 'value': 0.9892400000000002} ({'split': 'train'})
cross_entropy: {'epoch': 178, 'value': 0.03141256673589349} ({'split': 'train'})
accuracy: {'epoch': 178, 'value': 0.912899983525276} ({'split': 'test'})
cross_entropy: {'epoch': 178, 'value': 0.4116283266991377} ({'split': 'test'})
We have a new best! with accuracy::0.912899983525276 and at epoch::178, let's save it!
Epoch 179
accuracy: {'epoch': 179, 'value': 0.98978} ({'split': 'train'})
cross_entropy: {'epoch': 179, 'value': 0.029650742630437012} ({'split': 'train'})
accuracy: {'epoch': 179, 'value': 0.9111999827623369} ({'split': 'test'})
cross_entropy: {'epoch': 179, 'value': 0.41387804344296447} ({'split': 'test'})
Epoch 180
accuracy: {'epoch': 180, 'value': 0.9895200000572208} ({'split': 'train'})
cross_entropy: {'epoch': 180, 'value': 0.0297914236330986} ({'split': 'train'})
accuracy: {'epoch': 180, 'value': 0.9128999805450437} ({'split': 'test'})
cross_entropy: {'epoch': 180, 'value': 0.41571012496948234} ({'split': 'test'})
Epoch 181
accuracy: {'epoch': 181, 'value': 0.9896600000190735} ({'split': 'train'})
cross_entropy: {'epoch': 181, 'value': 0.03025929133892059} ({'split': 'train'})
accuracy: {'epoch': 181, 'value': 0.910899980068207} ({'split': 'test'})
cross_entropy: {'epoch': 181, 'value': 0.4186477147042751} ({'split': 'test'})
Epoch 182
accuracy: {'epoch': 182, 'value': 0.99054} ({'split': 'train'})
cross_entropy: {'epoch': 182, 'value': 0.02739770252525807} ({'split': 'train'})
accuracy: {'epoch': 182, 'value': 0.9117999815940857} ({'split': 'test'})
cross_entropy: {'epoch': 182, 'value': 0.4239272519946097} ({'split': 'test'})
Epoch 183
accuracy: {'epoch': 183, 'value': 0.9906800000190733} ({'split': 'train'})
cross_entropy: {'epoch': 183, 'value': 0.02753387029767036} ({'split': 'train'})
accuracy: {'epoch': 183, 'value': 0.909899982213974} ({'split': 'test'})
cross_entropy: {'epoch': 183, 'value': 0.4258793336898089} ({'split': 'test'})
Epoch 184
accuracy: {'epoch': 184, 'value': 0.9910799999999998} ({'split': 'train'})
cross_entropy: {'epoch': 184, 'value': 0.025550249236226074} ({'split': 'train'})
accuracy: {'epoch': 184, 'value': 0.910099983215332} ({'split': 'test'})
cross_entropy: {'epoch': 184, 'value': 0.4317829114198687} ({'split': 'test'})
Epoch 185
accuracy: {'epoch': 185, 'value': 0.99184} ({'split': 'train'})
cross_entropy: {'epoch': 185, 'value': 0.024525099737048146} ({'split': 'train'})
accuracy: {'epoch': 185, 'value': 0.9127999824285509} ({'split': 'test'})
cross_entropy: {'epoch': 185, 'value': 0.43682368159294116} ({'split': 'test'})
Epoch 186
accuracy: {'epoch': 186, 'value': 0.99114} ({'split': 'train'})
cross_entropy: {'epoch': 186, 'value': 0.025417537712305795} ({'split': 'train'})
accuracy: {'epoch': 186, 'value': 0.9075999826192854} ({'split': 'test'})
cross_entropy: {'epoch': 186, 'value': 0.45313924953341483} ({'split': 'test'})
Epoch 187
accuracy: {'epoch': 187, 'value': 0.9908800000190734} ({'split': 'train'})
cross_entropy: {'epoch': 187, 'value': 0.02628410438776016} ({'split': 'train'})
accuracy: {'epoch': 187, 'value': 0.9107999795675277} ({'split': 'test'})
cross_entropy: {'epoch': 187, 'value': 0.43808679461479183} ({'split': 'test'})
Epoch 188
accuracy: {'epoch': 188, 'value': 0.9920200000000005} ({'split': 'train'})
cross_entropy: {'epoch': 188, 'value': 0.02312277493804692} ({'split': 'train'})
accuracy: {'epoch': 188, 'value': 0.9114999824762345} ({'split': 'test'})
cross_entropy: {'epoch': 188, 'value': 0.44458633102476597} ({'split': 'test'})
Epoch 189
accuracy: {'epoch': 189, 'value': 0.9923000000000007} ({'split': 'train'})
cross_entropy: {'epoch': 189, 'value': 0.02283101689696311} ({'split': 'train'})
accuracy: {'epoch': 189, 'value': 0.9117999815940858} ({'split': 'test'})
cross_entropy: {'epoch': 189, 'value': 0.45355341091752044} ({'split': 'test'})
Epoch 190
accuracy: {'epoch': 190, 'value': 0.9922000000190733} ({'split': 'train'})
cross_entropy: {'epoch': 190, 'value': 0.022783376874923707} ({'split': 'train'})
accuracy: {'epoch': 190, 'value': 0.9144999814033509} ({'split': 'test'})
cross_entropy: {'epoch': 190, 'value': 0.4461587540060283} ({'split': 'test'})
We have a new best! with accuracy::0.9144999814033509 and at epoch::190, let's save it!
Epoch 191
accuracy: {'epoch': 191, 'value': 0.9927199999999997} ({'split': 'train'})
cross_entropy: {'epoch': 191, 'value': 0.02149363799095155} ({'split': 'train'})
accuracy: {'epoch': 191, 'value': 0.9129999798536302} ({'split': 'test'})
cross_entropy: {'epoch': 191, 'value': 0.46012210443615903} ({'split': 'test'})
Epoch 192
accuracy: {'epoch': 192, 'value': 0.9928199999999999} ({'split': 'train'})
cross_entropy: {'epoch': 192, 'value': 0.020835854599773888} ({'split': 'train'})
accuracy: {'epoch': 192, 'value': 0.9120999771356579} ({'split': 'test'})
cross_entropy: {'epoch': 192, 'value': 0.45880016043782235} ({'split': 'test'})
Epoch 193
accuracy: {'epoch': 193, 'value': 0.99218} ({'split': 'train'})
cross_entropy: {'epoch': 193, 'value': 0.021993319421149786} ({'split': 'train'})
accuracy: {'epoch': 193, 'value': 0.9149999815225599} ({'split': 'test'})
cross_entropy: {'epoch': 193, 'value': 0.45401532657444477} ({'split': 'test'})
We have a new best! with accuracy::0.9149999815225599 and at epoch::193, let's save it!
Epoch 194
accuracy: {'epoch': 194, 'value': 0.993260000038147} ({'split': 'train'})
cross_entropy: {'epoch': 194, 'value': 0.020336707869768135} ({'split': 'train'})
accuracy: {'epoch': 194, 'value': 0.9119999808073046} ({'split': 'test'})
cross_entropy: {'epoch': 194, 'value': 0.45831330798566344} ({'split': 'test'})
Epoch 195
accuracy: {'epoch': 195, 'value': 0.9923200000190736} ({'split': 'train'})
cross_entropy: {'epoch': 195, 'value': 0.022439476896524427} ({'split': 'train'})
accuracy: {'epoch': 195, 'value': 0.9124999809265136} ({'split': 'test'})
cross_entropy: {'epoch': 195, 'value': 0.4520565015077591} ({'split': 'test'})
Epoch 196
accuracy: {'epoch': 196, 'value': 0.9928000000190736} ({'split': 'train'})
cross_entropy: {'epoch': 196, 'value': 0.02015094543457031} ({'split': 'train'})
accuracy: {'epoch': 196, 'value': 0.9124999815225605} ({'split': 'test'})
cross_entropy: {'epoch': 196, 'value': 0.45462140351533886} ({'split': 'test'})
Epoch 197
accuracy: {'epoch': 197, 'value': 0.9930800000190736} ({'split': 'train'})
cross_entropy: {'epoch': 197, 'value': 0.019417400053739556} ({'split': 'train'})
accuracy: {'epoch': 197, 'value': 0.9112999820709232} ({'split': 'test'})
cross_entropy: {'epoch': 197, 'value': 0.4641501887142658} ({'split': 'test'})
Epoch 198
accuracy: {'epoch': 198, 'value': 0.9935200000000004} ({'split': 'train'})
cross_entropy: {'epoch': 198, 'value': 0.019807528547048578} ({'split': 'train'})
accuracy: {'epoch': 198, 'value': 0.9125999826192858} ({'split': 'test'})
cross_entropy: {'epoch': 198, 'value': 0.4585738530755043} ({'split': 'test'})
Epoch 199
accuracy: {'epoch': 199, 'value': 0.9933600000190738} ({'split': 'train'})
cross_entropy: {'epoch': 199, 'value': 0.019309309139251715} ({'split': 'train'})
accuracy: {'epoch': 199, 'value': 0.9112999814748766} ({'split': 'test'})
cross_entropy: {'epoch': 199, 'value': 0.47031256146728984} ({'split': 'test'})
Epoch 200
accuracy: {'epoch': 200, 'value': 0.9934399999999999} ({'split': 'train'})
cross_entropy: {'epoch': 200, 'value': 0.01852397802054882} ({'split': 'train'})
accuracy: {'epoch': 200, 'value': 0.9125999826192854} ({'split': 'test'})
cross_entropy: {'epoch': 200, 'value': 0.4711278182268145} ({'split': 'test'})
Epoch 201
accuracy: {'epoch': 201, 'value': 0.9939799999999998} ({'split': 'train'})
cross_entropy: {'epoch': 201, 'value': 0.018312729237303125} ({'split': 'train'})
accuracy: {'epoch': 201, 'value': 0.9113999801874162} ({'split': 'test'})
cross_entropy: {'epoch': 201, 'value': 0.4527870415896178} ({'split': 'test'})
Epoch 202
accuracy: {'epoch': 202, 'value': 0.9939200000000004} ({'split': 'train'})
cross_entropy: {'epoch': 202, 'value': 0.017787283636331565} ({'split': 'train'})
accuracy: {'epoch': 202, 'value': 0.9123999786376953} ({'split': 'test'})
cross_entropy: {'epoch': 202, 'value': 0.4663372972980141} ({'split': 'test'})
Epoch 203
accuracy: {'epoch': 203, 'value': 0.993300000019073} ({'split': 'train'})
cross_entropy: {'epoch': 203, 'value': 0.01924952651798725} ({'split': 'train'})
accuracy: {'epoch': 203, 'value': 0.9126999831199648} ({'split': 'test'})
cross_entropy: {'epoch': 203, 'value': 0.4649669724702836} ({'split': 'test'})
Epoch 204
accuracy: {'epoch': 204, 'value': 0.9948200000000001} ({'split': 'train'})
cross_entropy: {'epoch': 204, 'value': 0.015518347726017241} ({'split': 'train'})
accuracy: {'epoch': 204, 'value': 0.9115999811887743} ({'split': 'test'})
cross_entropy: {'epoch': 204, 'value': 0.47168919421732414} ({'split': 'test'})
Epoch 205
accuracy: {'epoch': 205, 'value': 0.9945199999999996} ({'split': 'train'})
cross_entropy: {'epoch': 205, 'value': 0.01575170279234648} ({'split': 'train'})
accuracy: {'epoch': 205, 'value': 0.913299981355667} ({'split': 'test'})
cross_entropy: {'epoch': 205, 'value': 0.4735560885071754} ({'split': 'test'})
Epoch 206
accuracy: {'epoch': 206, 'value': 0.9948600000000001} ({'split': 'train'})
cross_entropy: {'epoch': 206, 'value': 0.015685214126110073} ({'split': 'train'})
accuracy: {'epoch': 206, 'value': 0.9118999791145322} ({'split': 'test'})
cross_entropy: {'epoch': 206, 'value': 0.4714611081033946} ({'split': 'test'})
Epoch 207
accuracy: {'epoch': 207, 'value': 0.9939800000000003} ({'split': 'train'})
cross_entropy: {'epoch': 207, 'value': 0.016795229088738553} ({'split': 'train'})
accuracy: {'epoch': 207, 'value': 0.9126999819278719} ({'split': 'test'})
cross_entropy: {'epoch': 207, 'value': 0.47126788422465327} ({'split': 'test'})
Epoch 208
accuracy: {'epoch': 208, 'value': 0.9947600000190733} ({'split': 'train'})
cross_entropy: {'epoch': 208, 'value': 0.015501502099037164} ({'split': 'train'})
accuracy: {'epoch': 208, 'value': 0.9138999789953229} ({'split': 'test'})
cross_entropy: {'epoch': 208, 'value': 0.48575971759855746} ({'split': 'test'})
Epoch 209
accuracy: {'epoch': 209, 'value': 0.9949600000190738} ({'split': 'train'})
cross_entropy: {'epoch': 209, 'value': 0.015456198557019235} ({'split': 'train'})
accuracy: {'epoch': 209, 'value': 0.912099980711937} ({'split': 'test'})
cross_entropy: {'epoch': 209, 'value': 0.4703638183325529} ({'split': 'test'})
Epoch 210
accuracy: {'epoch': 210, 'value': 0.9949600000190734} ({'split': 'train'})
cross_entropy: {'epoch': 210, 'value': 0.01463880511254072} ({'split': 'train'})
accuracy: {'epoch': 210, 'value': 0.9111999791860582} ({'split': 'test'})
cross_entropy: {'epoch': 210, 'value': 0.5075862230360507} ({'split': 'test'})
Epoch 211
accuracy: {'epoch': 211, 'value': 0.99438} ({'split': 'train'})
cross_entropy: {'epoch': 211, 'value': 0.016723626319635667} ({'split': 'train'})
accuracy: {'epoch': 211, 'value': 0.9129999804496762} ({'split': 'test'})
cross_entropy: {'epoch': 211, 'value': 0.4831979432702065} ({'split': 'test'})
Epoch 212
accuracy: {'epoch': 212, 'value': 0.9952199999999999} ({'split': 'train'})
cross_entropy: {'epoch': 212, 'value': 0.014911109209060669} ({'split': 'train'})
accuracy: {'epoch': 212, 'value': 0.9147999829053879} ({'split': 'test'})
cross_entropy: {'epoch': 212, 'value': 0.482663090378046} ({'split': 'test'})
Epoch 213
accuracy: {'epoch': 213, 'value': 0.9952199999999999} ({'split': 'train'})
cross_entropy: {'epoch': 213, 'value': 0.013799219200909148} ({'split': 'train'})
accuracy: {'epoch': 213, 'value': 0.9120999813079836} ({'split': 'test'})
cross_entropy: {'epoch': 213, 'value': 0.49039600748568773} ({'split': 'test'})
Epoch 214
accuracy: {'epoch': 214, 'value': 0.9953400000572203} ({'split': 'train'})
cross_entropy: {'epoch': 214, 'value': 0.013908233616948122} ({'split': 'train'})
accuracy: {'epoch': 214, 'value': 0.912399982213974} ({'split': 'test'})
cross_entropy: {'epoch': 214, 'value': 0.5025252402201295} ({'split': 'test'})
Epoch 215
accuracy: {'epoch': 215, 'value': 0.9945000000190739} ({'split': 'train'})
cross_entropy: {'epoch': 215, 'value': 0.015724493075609202} ({'split': 'train'})
accuracy: {'epoch': 215, 'value': 0.912899982333183} ({'split': 'test'})
cross_entropy: {'epoch': 215, 'value': 0.4992835461348297} ({'split': 'test'})
Epoch 216
accuracy: {'epoch': 216, 'value': 0.9947199999999999} ({'split': 'train'})
cross_entropy: {'epoch': 216, 'value': 0.015013849419355397} ({'split': 'train'})
accuracy: {'epoch': 216, 'value': 0.9102999818325043} ({'split': 'test'})
cross_entropy: {'epoch': 216, 'value': 0.4793822894245387} ({'split': 'test'})
Epoch 217
accuracy: {'epoch': 217, 'value': 0.9960000000190737} ({'split': 'train'})
cross_entropy: {'epoch': 217, 'value': 0.012501406010389333} ({'split': 'train'})
accuracy: {'epoch': 217, 'value': 0.9133999812602996} ({'split': 'test'})
cross_entropy: {'epoch': 217, 'value': 0.4895114899426697} ({'split': 'test'})
Epoch 218
accuracy: {'epoch': 218, 'value': 0.9953799999999998} ({'split': 'train'})
cross_entropy: {'epoch': 218, 'value': 0.014129052099287515} ({'split': 'train'})
accuracy: {'epoch': 218, 'value': 0.9124999797344209} ({'split': 'test'})
cross_entropy: {'epoch': 218, 'value': 0.4969658703356984} ({'split': 'test'})
Epoch 219
accuracy: {'epoch': 219, 'value': 0.9954600000000001} ({'split': 'train'})
cross_entropy: {'epoch': 219, 'value': 0.013674779160916807} ({'split': 'train'})
accuracy: {'epoch': 219, 'value': 0.9106999802589418} ({'split': 'test'})
cross_entropy: {'epoch': 219, 'value': 0.4894993385672568} ({'split': 'test'})
Epoch 220
accuracy: {'epoch': 220, 'value': 0.9962600000190737} ({'split': 'train'})
cross_entropy: {'epoch': 220, 'value': 0.012011013627648353} ({'split': 'train'})
accuracy: {'epoch': 220, 'value': 0.9117999792099} ({'split': 'test'})
cross_entropy: {'epoch': 220, 'value': 0.4939879609644413} ({'split': 'test'})
Epoch 221
accuracy: {'epoch': 221, 'value': 0.9960400000190736} ({'split': 'train'})
cross_entropy: {'epoch': 221, 'value': 0.01223541018486023} ({'split': 'train'})
accuracy: {'epoch': 221, 'value': 0.9115999799966813} ({'split': 'test'})
cross_entropy: {'epoch': 221, 'value': 0.5092807118594644} ({'split': 'test'})
Epoch 222
accuracy: {'epoch': 222, 'value': 0.9959400000190736} ({'split': 'train'})
cross_entropy: {'epoch': 222, 'value': 0.012183177609443666} ({'split': 'train'})
accuracy: {'epoch': 222, 'value': 0.9148999798297884} ({'split': 'test'})
cross_entropy: {'epoch': 222, 'value': 0.4956973553448916} ({'split': 'test'})
Epoch 223
accuracy: {'epoch': 223, 'value': 0.99626} ({'split': 'train'})
cross_entropy: {'epoch': 223, 'value': 0.012057625409588218} ({'split': 'train'})
accuracy: {'epoch': 223, 'value': 0.9133999794721605} ({'split': 'test'})
cross_entropy: {'epoch': 223, 'value': 0.5040696897357703} ({'split': 'test'})
Epoch 224
accuracy: {'epoch': 224, 'value': 0.9962} ({'split': 'train'})
cross_entropy: {'epoch': 224, 'value': 0.01203956063687801} ({'split': 'train'})
accuracy: {'epoch': 224, 'value': 0.913999979496002} ({'split': 'test'})
cross_entropy: {'epoch': 224, 'value': 0.4915034850686787} ({'split': 'test'})
Epoch 225
accuracy: {'epoch': 225, 'value': 0.9955599999999999} ({'split': 'train'})
cross_entropy: {'epoch': 225, 'value': 0.013175797520279886} ({'split': 'train'})
accuracy: {'epoch': 225, 'value': 0.9135999828577043} ({'split': 'test'})
cross_entropy: {'epoch': 225, 'value': 0.4925763154029846} ({'split': 'test'})
Epoch 226
accuracy: {'epoch': 226, 'value': 0.99618} ({'split': 'train'})
cross_entropy: {'epoch': 226, 'value': 0.012244429494440554} ({'split': 'train'})
accuracy: {'epoch': 226, 'value': 0.9142999821901321} ({'split': 'test'})
cross_entropy: {'epoch': 226, 'value': 0.47819215252995484} ({'split': 'test'})
Epoch 227
accuracy: {'epoch': 227, 'value': 0.99634} ({'split': 'train'})
cross_entropy: {'epoch': 227, 'value': 0.011648231994211668} ({'split': 'train'})
accuracy: {'epoch': 227, 'value': 0.9148999810218812} ({'split': 'test'})
cross_entropy: {'epoch': 227, 'value': 0.48621941052377227} ({'split': 'test'})
Epoch 228
accuracy: {'epoch': 228, 'value': 0.9959200000000002} ({'split': 'train'})
cross_entropy: {'epoch': 228, 'value': 0.012074941117167478} ({'split': 'train'})
accuracy: {'epoch': 228, 'value': 0.9154999804496767} ({'split': 'test'})
cross_entropy: {'epoch': 228, 'value': 0.4785658692568542} ({'split': 'test'})
We have a new best! with accuracy::0.9154999804496767 and at epoch::228, let's save it!
Epoch 229
accuracy: {'epoch': 229, 'value': 0.9962799999999995} ({'split': 'train'})
cross_entropy: {'epoch': 229, 'value': 0.010993601544797421} ({'split': 'train'})
accuracy: {'epoch': 229, 'value': 0.9138999795913696} ({'split': 'test'})
cross_entropy: {'epoch': 229, 'value': 0.5013601861149068} ({'split': 'test'})
Epoch 230
accuracy: {'epoch': 230, 'value': 0.9963199999999997} ({'split': 'train'})
cross_entropy: {'epoch': 230, 'value': 0.010826656501889229} ({'split': 'train'})
accuracy: {'epoch': 230, 'value': 0.9149999809265137} ({'split': 'test'})
cross_entropy: {'epoch': 230, 'value': 0.48900472588837135} ({'split': 'test'})
Epoch 231
accuracy: {'epoch': 231, 'value': 0.9972399999999996} ({'split': 'train'})
cross_entropy: {'epoch': 231, 'value': 0.009096496720612055} ({'split': 'train'})
accuracy: {'epoch': 231, 'value': 0.914099979996681} ({'split': 'test'})
cross_entropy: {'epoch': 231, 'value': 0.5102006343007087} ({'split': 'test'})
Epoch 232
accuracy: {'epoch': 232, 'value': 0.9968800000000001} ({'split': 'train'})
cross_entropy: {'epoch': 232, 'value': 0.009950757800191641} ({'split': 'train'})
accuracy: {'epoch': 232, 'value': 0.9131999808549879} ({'split': 'test'})
cross_entropy: {'epoch': 232, 'value': 0.49396739274263407} ({'split': 'test'})
Epoch 233
accuracy: {'epoch': 233, 'value': 0.9962600000000003} ({'split': 'train'})
cross_entropy: {'epoch': 233, 'value': 0.011253443310484291} ({'split': 'train'})
accuracy: {'epoch': 233, 'value': 0.9159999805688861} ({'split': 'test'})
cross_entropy: {'epoch': 233, 'value': 0.4873638001084327} ({'split': 'test'})
We have a new best! with accuracy::0.9159999805688861 and at epoch::233, let's save it!
Epoch 234
accuracy: {'epoch': 234, 'value': 0.99638} ({'split': 'train'})
cross_entropy: {'epoch': 234, 'value': 0.0108844036755152} ({'split': 'train'})
accuracy: {'epoch': 234, 'value': 0.9123999798297885} ({'split': 'test'})
cross_entropy: {'epoch': 234, 'value': 0.49286193087697044} ({'split': 'test'})
Epoch 235
accuracy: {'epoch': 235, 'value': 0.9966399999999999} ({'split': 'train'})
cross_entropy: {'epoch': 235, 'value': 0.010722669482864444} ({'split': 'train'})
accuracy: {'epoch': 235, 'value': 0.9142999792099} ({'split': 'test'})
cross_entropy: {'epoch': 235, 'value': 0.5038904239982368} ({'split': 'test'})
Epoch 236
accuracy: {'epoch': 236, 'value': 0.9966000000000002} ({'split': 'train'})
cross_entropy: {'epoch': 236, 'value': 0.009987388020604854} ({'split': 'train'})
accuracy: {'epoch': 236, 'value': 0.9139999783039096} ({'split': 'test'})
cross_entropy: {'epoch': 236, 'value': 0.508393933624029} ({'split': 'test'})
Epoch 237
accuracy: {'epoch': 237, 'value': 0.9965800000000002} ({'split': 'train'})
cross_entropy: {'epoch': 237, 'value': 0.010496561672613026} ({'split': 'train'})
accuracy: {'epoch': 237, 'value': 0.9132999807596207} ({'split': 'test'})
cross_entropy: {'epoch': 237, 'value': 0.5128248573839663} ({'split': 'test'})
Epoch 238
accuracy: {'epoch': 238, 'value': 0.9964800000000001} ({'split': 'train'})
cross_entropy: {'epoch': 238, 'value': 0.010527448778599499} ({'split': 'train'})
accuracy: {'epoch': 238, 'value': 0.9149999809265136} ({'split': 'test'})
cross_entropy: {'epoch': 238, 'value': 0.5274596024677155} ({'split': 'test'})
Epoch 239
accuracy: {'epoch': 239, 'value': 0.9964199999999996} ({'split': 'train'})
cross_entropy: {'epoch': 239, 'value': 0.010205155358687044} ({'split': 'train'})
accuracy: {'epoch': 239, 'value': 0.9152999806404114} ({'split': 'test'})
cross_entropy: {'epoch': 239, 'value': 0.5146956834197045} ({'split': 'test'})
Epoch 240
accuracy: {'epoch': 240, 'value': 0.9967200000190729} ({'split': 'train'})
cross_entropy: {'epoch': 240, 'value': 0.010005332039594653} ({'split': 'train'})
accuracy: {'epoch': 240, 'value': 0.9149999815225601} ({'split': 'test'})
cross_entropy: {'epoch': 240, 'value': 0.50815453954041} ({'split': 'test'})
Epoch 241
accuracy: {'epoch': 241, 'value': 0.9970799999999999} ({'split': 'train'})
cross_entropy: {'epoch': 241, 'value': 0.009599275797605512} ({'split': 'train'})
accuracy: {'epoch': 241, 'value': 0.9144999802112578} ({'split': 'test'})
cross_entropy: {'epoch': 241, 'value': 0.5137064193189144} ({'split': 'test'})
Epoch 242
accuracy: {'epoch': 242, 'value': 0.9969000000000001} ({'split': 'train'})
cross_entropy: {'epoch': 242, 'value': 0.009775455544553694} ({'split': 'train'})
accuracy: {'epoch': 242, 'value': 0.9144999790191651} ({'split': 'test'})
cross_entropy: {'epoch': 242, 'value': 0.5086157694458961} ({'split': 'test'})
Epoch 243
accuracy: {'epoch': 243, 'value': 0.9971400000000004} ({'split': 'train'})
cross_entropy: {'epoch': 243, 'value': 0.009221322567313915} ({'split': 'train'})
accuracy: {'epoch': 243, 'value': 0.9125999814271929} ({'split': 'test'})
cross_entropy: {'epoch': 243, 'value': 0.5106432373821735} ({'split': 'test'})
Epoch 244
accuracy: {'epoch': 244, 'value': 0.9977999999999998} ({'split': 'train'})
cross_entropy: {'epoch': 244, 'value': 0.008138598896302284} ({'split': 'train'})
accuracy: {'epoch': 244, 'value': 0.9133999758958817} ({'split': 'test'})
cross_entropy: {'epoch': 244, 'value': 0.5340872891247274} ({'split': 'test'})
Epoch 245
accuracy: {'epoch': 245, 'value': 0.9975199999999999} ({'split': 'train'})
cross_entropy: {'epoch': 245, 'value': 0.008578303047977385} ({'split': 'train'})
accuracy: {'epoch': 245, 'value': 0.9144999796152118} ({'split': 'test'})
cross_entropy: {'epoch': 245, 'value': 0.5195769256353379} ({'split': 'test'})
Epoch 246
accuracy: {'epoch': 246, 'value': 0.9970799999999995} ({'split': 'train'})
cross_entropy: {'epoch': 246, 'value': 0.00920917590022087} ({'split': 'train'})
accuracy: {'epoch': 246, 'value': 0.9135999810695646} ({'split': 'test'})
cross_entropy: {'epoch': 246, 'value': 0.5145977941155433} ({'split': 'test'})
Epoch 247
accuracy: {'epoch': 247, 'value': 0.9965400000190734} ({'split': 'train'})
cross_entropy: {'epoch': 247, 'value': 0.009876724682897327} ({'split': 'train'})
accuracy: {'epoch': 247, 'value': 0.9153999787569044} ({'split': 'test'})
cross_entropy: {'epoch': 247, 'value': 0.5068810061365365} ({'split': 'test'})
Epoch 248
accuracy: {'epoch': 248, 'value': 0.9972399999999999} ({'split': 'train'})
cross_entropy: {'epoch': 248, 'value': 0.00943543666087091} ({'split': 'train'})
accuracy: {'epoch': 248, 'value': 0.9150999808311462} ({'split': 'test'})
cross_entropy: {'epoch': 248, 'value': 0.5132384640723469} ({'split': 'test'})
Epoch 249
accuracy: {'epoch': 249, 'value': 0.9971800000000001} ({'split': 'train'})
cross_entropy: {'epoch': 249, 'value': 0.008815981899201871} ({'split': 'train'})
accuracy: {'epoch': 249, 'value': 0.9134999805688859} ({'split': 'test'})
cross_entropy: {'epoch': 249, 'value': 0.5019523607194419} ({'split': 'test'})
Epoch 250
accuracy: {'epoch': 250, 'value': 0.99792} ({'split': 'train'})
cross_entropy: {'epoch': 250, 'value': 0.007106984944045542} ({'split': 'train'})
accuracy: {'epoch': 250, 'value': 0.9150999784469603} ({'split': 'test'})
cross_entropy: {'epoch': 250, 'value': 0.5013994381576776} ({'split': 'test'})
Epoch 251
accuracy: {'epoch': 251, 'value': 0.9979599999999998} ({'split': 'train'})
cross_entropy: {'epoch': 251, 'value': 0.007340279549807313} ({'split': 'train'})
accuracy: {'epoch': 251, 'value': 0.9151999801397324} ({'split': 'test'})
cross_entropy: {'epoch': 251, 'value': 0.5006959418952466} ({'split': 'test'})
Epoch 252
accuracy: {'epoch': 252, 'value': 0.9983000000000003} ({'split': 'train'})
cross_entropy: {'epoch': 252, 'value': 0.006705481906905771} ({'split': 'train'})
accuracy: {'epoch': 252, 'value': 0.914999978542328} ({'split': 'test'})
cross_entropy: {'epoch': 252, 'value': 0.5044510293006897} ({'split': 'test'})
Epoch 253
accuracy: {'epoch': 253, 'value': 0.9982599999999999} ({'split': 'train'})
cross_entropy: {'epoch': 253, 'value': 0.0061990204231441} ({'split': 'train'})
accuracy: {'epoch': 253, 'value': 0.9144999796152116} ({'split': 'test'})
cross_entropy: {'epoch': 253, 'value': 0.50643738925457} ({'split': 'test'})
Epoch 254
accuracy: {'epoch': 254, 'value': 0.9981799999999998} ({'split': 'train'})
cross_entropy: {'epoch': 254, 'value': 0.006155429378524423} ({'split': 'train'})
accuracy: {'epoch': 254, 'value': 0.9156999784708028} ({'split': 'test'})
cross_entropy: {'epoch': 254, 'value': 0.5074294698238372} ({'split': 'test'})
Epoch 255
accuracy: {'epoch': 255, 'value': 0.9983599999999999} ({'split': 'train'})
cross_entropy: {'epoch': 255, 'value': 0.006517670218944552} ({'split': 'train'})
accuracy: {'epoch': 255, 'value': 0.9155999791622164} ({'split': 'test'})
cross_entropy: {'epoch': 255, 'value': 0.5088095078617335} ({'split': 'test'})
Epoch 256
accuracy: {'epoch': 256, 'value': 0.9980599999999997} ({'split': 'train'})
cross_entropy: {'epoch': 256, 'value': 0.006373689700700344} ({'split': 'train'})
accuracy: {'epoch': 256, 'value': 0.9151999795436857} ({'split': 'test'})
cross_entropy: {'epoch': 256, 'value': 0.5092658837884666} ({'split': 'test'})
Epoch 257
accuracy: {'epoch': 257, 'value': 0.9983799999999999} ({'split': 'train'})
cross_entropy: {'epoch': 257, 'value': 0.0058571913023293025} ({'split': 'train'})
accuracy: {'epoch': 257, 'value': 0.915399977564812} ({'split': 'test'})
cross_entropy: {'epoch': 257, 'value': 0.5111576808989049} ({'split': 'test'})
Epoch 258
accuracy: {'epoch': 258, 'value': 0.9985600000190737} ({'split': 'train'})
cross_entropy: {'epoch': 258, 'value': 0.005433963533043859} ({'split': 'train'})
accuracy: {'epoch': 258, 'value': 0.9149999791383743} ({'split': 'test'})
cross_entropy: {'epoch': 258, 'value': 0.515546067431569} ({'split': 'test'})
Epoch 259
accuracy: {'epoch': 259, 'value': 0.99816} ({'split': 'train'})
cross_entropy: {'epoch': 259, 'value': 0.005948388398960224} ({'split': 'train'})
accuracy: {'epoch': 259, 'value': 0.9149999791383743} ({'split': 'test'})
cross_entropy: {'epoch': 259, 'value': 0.5154599180817606} ({'split': 'test'})
Epoch 260
accuracy: {'epoch': 260, 'value': 0.99852} ({'split': 'train'})
cross_entropy: {'epoch': 260, 'value': 0.005019310706406833} ({'split': 'train'})
accuracy: {'epoch': 260, 'value': 0.9150999790430069} ({'split': 'test'})
cross_entropy: {'epoch': 260, 'value': 0.5157264760136604} ({'split': 'test'})
Epoch 261
accuracy: {'epoch': 261, 'value': 0.9983999999999998} ({'split': 'train'})
cross_entropy: {'epoch': 261, 'value': 0.005475209193481135} ({'split': 'train'})
accuracy: {'epoch': 261, 'value': 0.9158999788761142} ({'split': 'test'})
cross_entropy: {'epoch': 261, 'value': 0.5164797125011681} ({'split': 'test'})
Epoch 262
accuracy: {'epoch': 262, 'value': 0.9984799999999998} ({'split': 'train'})
cross_entropy: {'epoch': 262, 'value': 0.0054960018616914755} ({'split': 'train'})
accuracy: {'epoch': 262, 'value': 0.9165999794006348} ({'split': 'test'})
cross_entropy: {'epoch': 262, 'value': 0.5189467940479515} ({'split': 'test'})
We have a new best! with accuracy::0.9165999794006348 and at epoch::262, let's save it!
Epoch 263
accuracy: {'epoch': 263, 'value': 0.9984800000000003} ({'split': 'train'})
cross_entropy: {'epoch': 263, 'value': 0.005671044936031101} ({'split': 'train'})
accuracy: {'epoch': 263, 'value': 0.9169999808073044} ({'split': 'test'})
cross_entropy: {'epoch': 263, 'value': 0.5208279228955507} ({'split': 'test'})
We have a new best! with accuracy::0.9169999808073044 and at epoch::263, let's save it!
Epoch 264
accuracy: {'epoch': 264, 'value': 0.9982000000000002} ({'split': 'train'})
cross_entropy: {'epoch': 264, 'value': 0.005567418680936099} ({'split': 'train'})
accuracy: {'epoch': 264, 'value': 0.9165999799966813} ({'split': 'test'})
cross_entropy: {'epoch': 264, 'value': 0.5205106119066477} ({'split': 'test'})
Epoch 265
accuracy: {'epoch': 265, 'value': 0.9987600000000004} ({'split': 'train'})
cross_entropy: {'epoch': 265, 'value': 0.005003947273194793} ({'split': 'train'})
accuracy: {'epoch': 265, 'value': 0.9160999804735184} ({'split': 'test'})
cross_entropy: {'epoch': 265, 'value': 0.5206538248807194} ({'split': 'test'})
Epoch 266
accuracy: {'epoch': 266, 'value': 0.9986200000000002} ({'split': 'train'})
cross_entropy: {'epoch': 266, 'value': 0.004993497767300578} ({'split': 'train'})
accuracy: {'epoch': 266, 'value': 0.9157999801635746} ({'split': 'test'})
cross_entropy: {'epoch': 266, 'value': 0.5220138307660815} ({'split': 'test'})
Epoch 267
accuracy: {'epoch': 267, 'value': 0.9986400000000001} ({'split': 'train'})
cross_entropy: {'epoch': 267, 'value': 0.005008474789932368} ({'split': 'train'})
accuracy: {'epoch': 267, 'value': 0.9172999805212024} ({'split': 'test'})
cross_entropy: {'epoch': 267, 'value': 0.5221663795411589} ({'split': 'test'})
We have a new best! with accuracy::0.9172999805212024 and at epoch::267, let's save it!
Epoch 268
accuracy: {'epoch': 268, 'value': 0.9982799999999997} ({'split': 'train'})
cross_entropy: {'epoch': 268, 'value': 0.005901283810287712} ({'split': 'train'})
accuracy: {'epoch': 268, 'value': 0.9164999812841413} ({'split': 'test'})
cross_entropy: {'epoch': 268, 'value': 0.5202707502245904} ({'split': 'test'})
Epoch 269
accuracy: {'epoch': 269, 'value': 0.9986200000000004} ({'split': 'train'})
cross_entropy: {'epoch': 269, 'value': 0.004942120442837483} ({'split': 'train'})
accuracy: {'epoch': 269, 'value': 0.9159999781847} ({'split': 'test'})
cross_entropy: {'epoch': 269, 'value': 0.5211796555668117} ({'split': 'test'})
Epoch 270
accuracy: {'epoch': 270, 'value': 0.9986800000190738} ({'split': 'train'})
cross_entropy: {'epoch': 270, 'value': 0.0047857168667763475} ({'split': 'train'})
accuracy: {'epoch': 270, 'value': 0.9158999794721603} ({'split': 'test'})
cross_entropy: {'epoch': 270, 'value': 0.5210316862165927} ({'split': 'test'})
Epoch 271
accuracy: {'epoch': 271, 'value': 0.9984199999999996} ({'split': 'train'})
cross_entropy: {'epoch': 271, 'value': 0.005630040501970794} ({'split': 'train'})
accuracy: {'epoch': 271, 'value': 0.9168999791145327} ({'split': 'test'})
cross_entropy: {'epoch': 271, 'value': 0.5227843132615089} ({'split': 'test'})
Epoch 272
accuracy: {'epoch': 272, 'value': 0.9983999999999996} ({'split': 'train'})
cross_entropy: {'epoch': 272, 'value': 0.005383231771737336} ({'split': 'train'})
accuracy: {'epoch': 272, 'value': 0.9163999801874162} ({'split': 'test'})
cross_entropy: {'epoch': 272, 'value': 0.5237025925517083} ({'split': 'test'})
Epoch 273
accuracy: {'epoch': 273, 'value': 0.9986800000190738} ({'split': 'train'})
cross_entropy: {'epoch': 273, 'value': 0.004793394480198623} ({'split': 'train'})
accuracy: {'epoch': 273, 'value': 0.9157999807596208} ({'split': 'test'})
cross_entropy: {'epoch': 273, 'value': 0.5243853473663335} ({'split': 'test'})
Epoch 274
accuracy: {'epoch': 274, 'value': 0.9987000000000001} ({'split': 'train'})
cross_entropy: {'epoch': 274, 'value': 0.005069130859076978} ({'split': 'train'})
accuracy: {'epoch': 274, 'value': 0.9153999811410901} ({'split': 'test'})
cross_entropy: {'epoch': 274, 'value': 0.5266352552175524} ({'split': 'test'})
Epoch 275
accuracy: {'epoch': 275, 'value': 0.9984399999999999} ({'split': 'train'})
cross_entropy: {'epoch': 275, 'value': 0.005299557029791176} ({'split': 'train'})
accuracy: {'epoch': 275, 'value': 0.9150999808311461} ({'split': 'test'})
cross_entropy: {'epoch': 275, 'value': 0.5250218989700081} ({'split': 'test'})
Epoch 276
accuracy: {'epoch': 276, 'value': 0.9985} ({'split': 'train'})
cross_entropy: {'epoch': 276, 'value': 0.005000440710280088} ({'split': 'train'})
accuracy: {'epoch': 276, 'value': 0.9156999808549884} ({'split': 'test'})
cross_entropy: {'epoch': 276, 'value': 0.5251886212825774} ({'split': 'test'})
Epoch 277
accuracy: {'epoch': 277, 'value': 0.9984000000190731} ({'split': 'train'})
cross_entropy: {'epoch': 277, 'value': 0.004974487113505603} ({'split': 'train'})
accuracy: {'epoch': 277, 'value': 0.9159999793767931} ({'split': 'test'})
cross_entropy: {'epoch': 277, 'value': 0.5253953725099564} ({'split': 'test'})
Epoch 278
accuracy: {'epoch': 278, 'value': 0.9986999999999998} ({'split': 'train'})
cross_entropy: {'epoch': 278, 'value': 0.004701158867701885} ({'split': 'train'})
accuracy: {'epoch': 278, 'value': 0.9159999811649324} ({'split': 'test'})
cross_entropy: {'epoch': 278, 'value': 0.52814439304173} ({'split': 'test'})
Epoch 279
accuracy: {'epoch': 279, 'value': 0.9987400000000006} ({'split': 'train'})
cross_entropy: {'epoch': 279, 'value': 0.004515876332744956} ({'split': 'train'})
accuracy: {'epoch': 279, 'value': 0.9155999797582626} ({'split': 'test'})
cross_entropy: {'epoch': 279, 'value': 0.5289632932841779} ({'split': 'test'})
Epoch 280
accuracy: {'epoch': 280, 'value': 0.9985600000190735} ({'split': 'train'})
cross_entropy: {'epoch': 280, 'value': 0.004795014024972917} ({'split': 'train'})
accuracy: {'epoch': 280, 'value': 0.9158999782800675} ({'split': 'test'})
cross_entropy: {'epoch': 280, 'value': 0.5300716716796159} ({'split': 'test'})
Epoch 281
accuracy: {'epoch': 281, 'value': 0.9988599999999995} ({'split': 'train'})
cross_entropy: {'epoch': 281, 'value': 0.0042924901897460266} ({'split': 'train'})
accuracy: {'epoch': 281, 'value': 0.9168999814987181} ({'split': 'test'})
cross_entropy: {'epoch': 281, 'value': 0.5308018159866333} ({'split': 'test'})
Epoch 282
accuracy: {'epoch': 282, 'value': 0.9986599999999999} ({'split': 'train'})
cross_entropy: {'epoch': 282, 'value': 0.004989415273293851} ({'split': 'train'})
accuracy: {'epoch': 282, 'value': 0.9159999811649322} ({'split': 'test'})
cross_entropy: {'epoch': 282, 'value': 0.5289966464042662} ({'split': 'test'})
Epoch 283
accuracy: {'epoch': 283, 'value': 0.9986200000000002} ({'split': 'train'})
cross_entropy: {'epoch': 283, 'value': 0.005055519739147277} ({'split': 'train'})
accuracy: {'epoch': 283, 'value': 0.9161999797821044} ({'split': 'test'})
cross_entropy: {'epoch': 283, 'value': 0.527322431281209} ({'split': 'test'})
Epoch 284
accuracy: {'epoch': 284, 'value': 0.9986799999999998} ({'split': 'train'})
cross_entropy: {'epoch': 284, 'value': 0.0045431399809569084} ({'split': 'train'})
accuracy: {'epoch': 284, 'value': 0.9158999800682069} ({'split': 'test'})
cross_entropy: {'epoch': 284, 'value': 0.5284159402549266} ({'split': 'test'})
Epoch 285
accuracy: {'epoch': 285, 'value': 0.9986600000000001} ({'split': 'train'})
cross_entropy: {'epoch': 285, 'value': 0.004839943972900509} ({'split': 'train'})
accuracy: {'epoch': 285, 'value': 0.9155999815464021} ({'split': 'test'})
cross_entropy: {'epoch': 285, 'value': 0.5312095689773559} ({'split': 'test'})
Epoch 286
accuracy: {'epoch': 286, 'value': 0.9984399999999998} ({'split': 'train'})
cross_entropy: {'epoch': 286, 'value': 0.004902692455090584} ({'split': 'train'})
accuracy: {'epoch': 286, 'value': 0.915899980068207} ({'split': 'test'})
cross_entropy: {'epoch': 286, 'value': 0.5287458745390178} ({'split': 'test'})
Epoch 287
accuracy: {'epoch': 287, 'value': 0.9987599999999999} ({'split': 'train'})
cross_entropy: {'epoch': 287, 'value': 0.004757236986402422} ({'split': 'train'})
accuracy: {'epoch': 287, 'value': 0.9152999824285509} ({'split': 'test'})
cross_entropy: {'epoch': 287, 'value': 0.5295768610388042} ({'split': 'test'})
Epoch 288
accuracy: {'epoch': 288, 'value': 0.9986399999999999} ({'split': 'train'})
cross_entropy: {'epoch': 288, 'value': 0.004896155255734921} ({'split': 'train'})
accuracy: {'epoch': 288, 'value': 0.9154999822378156} ({'split': 'test'})
cross_entropy: {'epoch': 288, 'value': 0.5342694230377673} ({'split': 'test'})
Epoch 289
accuracy: {'epoch': 289, 'value': 0.9986600000000004} ({'split': 'train'})
cross_entropy: {'epoch': 289, 'value': 0.004569228749200702} ({'split': 'train'})
accuracy: {'epoch': 289, 'value': 0.9158999806642534} ({'split': 'test'})
cross_entropy: {'epoch': 289, 'value': 0.5340149033814667} ({'split': 'test'})
Epoch 290
accuracy: {'epoch': 290, 'value': 0.99868} ({'split': 'train'})
cross_entropy: {'epoch': 290, 'value': 0.004569373945184052} ({'split': 'train'})
accuracy: {'epoch': 290, 'value': 0.9161999821662905} ({'split': 'test'})
cross_entropy: {'epoch': 290, 'value': 0.5338122612982988} ({'split': 'test'})
Epoch 291
accuracy: {'epoch': 291, 'value': 0.99882} ({'split': 'train'})
cross_entropy: {'epoch': 291, 'value': 0.00467676331821829} ({'split': 'train'})
accuracy: {'epoch': 291, 'value': 0.9155999815464017} ({'split': 'test'})
cross_entropy: {'epoch': 291, 'value': 0.5351979176700116} ({'split': 'test'})
Epoch 292
accuracy: {'epoch': 292, 'value': 0.9988399999999994} ({'split': 'train'})
cross_entropy: {'epoch': 292, 'value': 0.0047598506192490465} ({'split': 'train'})
accuracy: {'epoch': 292, 'value': 0.9159999811649321} ({'split': 'test'})
cross_entropy: {'epoch': 292, 'value': 0.5364763507246969} ({'split': 'test'})
Epoch 293
accuracy: {'epoch': 293, 'value': 0.9989799999999995} ({'split': 'train'})
cross_entropy: {'epoch': 293, 'value': 0.0042055595048982675} ({'split': 'train'})
accuracy: {'epoch': 293, 'value': 0.9167999815940856} ({'split': 'test'})
cross_entropy: {'epoch': 293, 'value': 0.5363633988797665} ({'split': 'test'})
Epoch 294
accuracy: {'epoch': 294, 'value': 0.9988600000000006} ({'split': 'train'})
cross_entropy: {'epoch': 294, 'value': 0.0046958571961987754} ({'split': 'train'})
accuracy: {'epoch': 294, 'value': 0.9164999806880948} ({'split': 'test'})
cross_entropy: {'epoch': 294, 'value': 0.5355125328898427} ({'split': 'test'})
Epoch 295
accuracy: {'epoch': 295, 'value': 0.9985800000000001} ({'split': 'train'})
cross_entropy: {'epoch': 295, 'value': 0.004711108890026809} ({'split': 'train'})
accuracy: {'epoch': 295, 'value': 0.9162999802827834} ({'split': 'test'})
cross_entropy: {'epoch': 295, 'value': 0.5376726041734218} ({'split': 'test'})
Epoch 296
accuracy: {'epoch': 296, 'value': 0.9990400000000002} ({'split': 'train'})
cross_entropy: {'epoch': 296, 'value': 0.004044641036968677} ({'split': 'train'})
accuracy: {'epoch': 296, 'value': 0.9158999812602998} ({'split': 'test'})
cross_entropy: {'epoch': 296, 'value': 0.5390877503156665} ({'split': 'test'})
Epoch 297
accuracy: {'epoch': 297, 'value': 0.9989199999999997} ({'split': 'train'})
cross_entropy: {'epoch': 297, 'value': 0.004357108434308319} ({'split': 'train'})
accuracy: {'epoch': 297, 'value': 0.9170999807119367} ({'split': 'test'})
cross_entropy: {'epoch': 297, 'value': 0.539081924855709} ({'split': 'test'})
Epoch 298
accuracy: {'epoch': 298, 'value': 0.9987800000000001} ({'split': 'train'})
cross_entropy: {'epoch': 298, 'value': 0.004470842141080649} ({'split': 'train'})
accuracy: {'epoch': 298, 'value': 0.9161999815702435} ({'split': 'test'})
cross_entropy: {'epoch': 298, 'value': 0.5407300832122565} ({'split': 'test'})
Epoch 299
accuracy: {'epoch': 299, 'value': 0.9987600000000003} ({'split': 'train'})
cross_entropy: {'epoch': 299, 'value': 0.00410845255829394} ({'split': 'train'})
accuracy: {'epoch': 299, 'value': 0.9175999802350996} ({'split': 'test'})
cross_entropy: {'epoch': 299, 'value': 0.5430128628015519} ({'split': 'test'})
We have a new best! with accuracy::0.9175999802350996 and at epoch::299, let's save it!
----- Saved results at sample_cifar10_resnet18_nobias_nobn_structured_pruning_70.csv ------
{'exp_name': 'exp_cifar10_resnet18_nobias_nobn_structured_pruning_70', 'model0_acc': 93.10999816656113, 'model1_acc': 93.19999837875366, 'geometric_acc': 12.01, 'prediction_acc': 21.7, 'naive_acc': 10.37, 'geometric_gain': -81.18999837875366, 'geometric_gain_%': -87.11373368141831, 'prediction_gain': -71.49999837875366, 'prediction_gain_%': -76.71673779240447, 'relative_loss_wrt_prediction': 10.396995889013837, 'geometric_time': 15.846896512899548, 'retrain_geometric_best': 91.75999802350997, 'retrain_naive_best': -100, 'retrain_model0_best': -100, 'retrain_model1_best': -100, 'retrain_epochs': 300}
FYI: the parameters were: 
 Namespace(n_epochs=300, batch_size_train=64, batch_size_test=1000, learning_rate=0.01, momentum=0.5, log_interval=100, to_download=True, disable_bias=True, dataset='Cifar10', num_models=2, model_name='resnet18_nobias_nobn', config_file=None, config_dir='/home/gvignen/otfusion/exp_cifar10_resnet18_nobias_nobn_structured_pruning_70/configurations', num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, sweep_id=90, gpu_id=0, skip_last_layer=False, skip_last_layer_type='average', debug=False, cifar_style_data=False, activation_histograms=True, act_num_samples=200, softmax_temperature=1, activation_mode='raw', options_type='generic', deprecated=None, save_result_file='sample_cifar10_resnet18_nobias_nobn_structured_pruning_70.csv', sweep_name='exp_cifar10_resnet18_nobias_nobn_structured_pruning_70', reg=0.01, reg_m=0.001, ground_metric='euclidean', ground_metric_normalize='none', not_squared=True, clip_gm=False, clip_min=0, clip_max=5, tmap_stats=False, ensemble_step=0.5, ground_metric_eff=True, retrain=300, retrain_lr_decay=-1, retrain_lr_decay_factor=None, retrain_lr_decay_epochs=None, retrain_avg_only=False, retrain_geometric_only=True, load_models='./resnet_models/', ckpt_type='best', recheck_cifar=True, recheck_acc=True, eval_aligned=False, enable_dropout=False, dump_model=False, dump_final_models=False, correction=True, activation_seed=21, weight_stats=True, sinkhorn_type='normal', geom_ensemble_type='acts', act_bug=False, standardize_acts=False, transform_acts=False, center_acts=False, prelu_acts=True, pool_acts=False, pool_relu=False, normalize_acts=False, normalize_wts=False, gromov=False, gromov_loss='square_loss', tensorboard_root='./tensorboard', tensorboard=False, same_model=-1, dist_normalize=False, update_acts=False, past_correction=True, partial_reshape=False, choice='0 2 4 6 8', diff_init=False, partition_type='labels', personal_class_idx=9, partition_dataloader=-1, personal_split_frac=0.1, exact=True, skip_personal_idx=False, prediction_wts=False, width_ratio=1, proper_marginals=False, retrain_seed=-1, no_random_trainloaders=False, reinit_trainloaders=False, second_model_name=None, print_distances=False, deterministic=False, skip_retrain=-1, importance=None, unbalanced=False, temperature=20, alpha=0.7, dist_epochs=60, handle_skips=True, prune=True, prune_frac=0.7, prune_type='structured', experiment_name='cifar10_resnet18_nobias_nobn_structured_pruning_70', timestamp='2024-01-06_15-21-56_440541', rootdir='/home/gvignen/otfusion/exp_cifar10_resnet18_nobias_nobn_structured_pruning_70', baseroot='/home/gvignen/otfusion', result_dir='/home/gvignen/otfusion/exp_cifar10_resnet18_nobias_nobn_structured_pruning_70/results', exp_name='exp_cifar10_resnet18_nobias_nobn_structured_pruning_70', csv_dir='/home/gvignen/otfusion/exp_cifar10_resnet18_nobias_nobn_structured_pruning_70/csv', config={'dataset': 'Cifar10', 'model': 'resnet18_nobias_nobn', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [150, 250], 'optimizer_decay_with_factor': 10.0, 'optimizer_learning_rate': 0.1, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0001, 'batch_size': 256, 'num_epochs': 300, 'seed': 42, 'nick': 'geometric_21', 'start_acc': 12.01}, second_config={'dataset': 'Cifar10', 'model': 'resnet18_nobias_nobn', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [150, 250], 'optimizer_decay_with_factor': 10.0, 'optimizer_learning_rate': 0.1, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0001, 'batch_size': 256, 'num_epochs': 300, 'seed': 42, 'nick': 'geometric_21', 'start_acc': 12.01}, cifar_init_lr=0.1, num_personal_idx=25, activation_time=4.96339526725933, params_model_0=11164352, params_model_1=11164352, geometric_time=15.846896512899548, params_geometric=11164352, retrain_geometric_best=0.9175999802350996, retrain_naive_best=-1, retrain_model0_best=-1, retrain_model1_best=-1, **{'trace_sum_ratio_conv1.weight': 0.046875, 'trace_sum_ratio_layer1.0.conv1.weight': 0.015625, 'trace_sum_ratio_layer1.0.conv2.weight': 0.046875, 'trace_sum_ratio_layer1.1.conv1.weight': 0.0, 'trace_sum_ratio_layer1.1.conv2.weight': 0.046875, 'trace_sum_ratio_layer2.0.conv1.weight': 0.0078125, 'trace_sum_ratio_layer2.0.conv2.weight': 0.0078125, 'trace_sum_ratio_layer2.0.shortcut.0.weight': 0.015625, 'trace_sum_ratio_layer2.1.conv1.weight': 0.0078125, 'trace_sum_ratio_layer2.1.conv2.weight': 0.0078125, 'trace_sum_ratio_layer3.0.conv1.weight': 0.0078125, 'trace_sum_ratio_layer3.0.conv2.weight': 0.0, 'trace_sum_ratio_layer3.0.shortcut.0.weight': 0.00390625, 'trace_sum_ratio_layer3.1.conv1.weight': 0.0, 'trace_sum_ratio_layer3.1.conv2.weight': 0.0, 'trace_sum_ratio_layer4.0.conv1.weight': 0.001953125, 'trace_sum_ratio_layer4.0.conv2.weight': 0.0, 'trace_sum_ratio_layer4.0.shortcut.0.weight': 0.0, 'trace_sum_ratio_layer4.1.conv1.weight': 0.001953125, 'trace_sum_ratio_layer4.1.conv2.weight': 0.001953125, 'trace_sum_ratio_linear.weight': 0.20000000298023224})
