/home/thmeier/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
------- Setting up parameters -------
dumping parameters at  /home/thmeier/otfusion/exp_sample_pruned_30_structured_vgg11/configurations
The parameters are: 
 Namespace(n_epochs=300, batch_size_train=64, batch_size_test=1000, learning_rate=0.01, momentum=0.5, log_interval=100, to_download=False, disable_bias=True, dataset='Cifar10', num_models=2, model_name='vgg11_nobias', config_file=None, config_dir='/home/thmeier/otfusion/exp_sample_pruned_30_structured_vgg11/configurations', num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, sweep_id=90, gpu_id=0, skip_last_layer=False, skip_last_layer_type='average', debug=False, cifar_style_data=False, activation_histograms=False, act_num_samples=100, softmax_temperature=1, activation_mode=None, options_type='generic', deprecated=None, save_result_file='sample_pruned_30_structured_vgg11.csv', sweep_name='exp_sample_pruned_30_structured_vgg11', reg=0.01, reg_m=0.001, ground_metric='euclidean', ground_metric_normalize='none', not_squared=True, clip_gm=False, clip_min=0, clip_max=5, tmap_stats=False, ensemble_step=0.5, ground_metric_eff=True, retrain=0, retrain_lr_decay=-1, retrain_lr_decay_factor=None, retrain_lr_decay_epochs=None, retrain_avg_only=False, retrain_geometric_only=False, load_models='./cifar_models/', ckpt_type='best', recheck_cifar=True, recheck_acc=False, eval_aligned=False, enable_dropout=False, dump_model=False, dump_final_models=False, correction=True, activation_seed=21, weight_stats=True, sinkhorn_type='normal', geom_ensemble_type='wts', act_bug=False, standardize_acts=False, transform_acts=False, center_acts=False, prelu_acts=True, pool_acts=False, pool_relu=False, normalize_acts=False, normalize_wts=False, gromov=False, gromov_loss='square_loss', tensorboard_root='./tensorboard', tensorboard=False, same_model=-1, dist_normalize=False, update_acts=False, past_correction=True, partial_reshape=False, choice='0 2 4 6 8', diff_init=False, partition_type='labels', personal_class_idx=9, partition_dataloader=-1, personal_split_frac=0.1, exact=True, skip_personal_idx=False, prediction_wts=False, width_ratio=1, proper_marginals=False, retrain_seed=-1, no_random_trainloaders=False, reinit_trainloaders=False, second_model_name=None, print_distances=False, deterministic=False, skip_retrain=-1, importance=None, unbalanced=False, temperature=20, alpha=0.7, dist_epochs=60, handle_skips=False, prune_frac=0.3, prune_type='structured', timestamp='2024-01-05_16-44-06_059512', rootdir='/home/thmeier/otfusion/exp_sample_pruned_30_structured_vgg11', baseroot='/home/thmeier/otfusion', result_dir='/home/thmeier/otfusion/exp_sample_pruned_30_structured_vgg11/results', exp_name='exp_2024-01-05_16-44-06_059512', csv_dir='/home/thmeier/otfusion/exp_sample_pruned_30_structured_vgg11/csv')
refactored get_config
------- Loading pre-trained models -------
loading cifar10 dataloaders
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
loading model with idx 0 and checkpoint_type is best
in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]
Relu Inplace is  False
model parameters are 
 [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]
Loading model at path ./cifar_models/model_0/best.checkpoint which had accuracy 0.9030999821424489 and at epoch 127
loading model with idx 1 and checkpoint_type is best
in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]
Relu Inplace is  False
model parameters are 
 [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]
Loading model at path ./cifar_models/model_1/best.checkpoint which had accuracy 0.9049999803304669 and at epoch 134
Done loading all the models

--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0041, Accuracy: 9031/10000 (90%)


--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0039, Accuracy: 9050/10000 (90%)

Rechecked accuracies are  [90.31, 90.5]
----------Prune the 2 Parent models now---------
---------let's see result after pruning-------------
dict_keys([])
---------let's see result after pruning-------------
dict_keys([])
--------Rechecking accuracies again!--------

--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0086, Accuracy: 7314/10000 (73%)

----- Saving Pruned model0-------

--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0117, Accuracy: 6314/10000 (63%)

----- Saving Pruned model1-------
Rechecked accuracies are  [73.14, 63.14]
layer features.0.weight has #params  1728
layer features.3.weight has #params  73728
layer features.6.weight has #params  294912
layer features.8.weight has #params  589824
layer features.11.weight has #params  1179648
layer features.13.weight has #params  2359296
layer features.16.weight has #params  2359296
layer features.18.weight has #params  2359296
layer classifier.weight has #params  5120
Activation Timer start
Activation Timer ends
------- Geometric Ensembling -------
Timer start
Previous layer shape is  None
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
returns a uniform measure of cardinality:  64
returns a uniform measure of cardinality:  64
the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
Ratio of trace to the matrix sum:  tensor(0.0312, device='cuda:0')
Here, trace is 1.9999871253967285 and matrix sum is 63.99958801269531 
this is past correction for weight mode
Shape of aligned wt is  torch.Size([64, 3, 9])
Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])
Previous layer shape is  torch.Size([64, 3, 3, 3])
shape of layer: model 0 torch.Size([128, 64, 9])
shape of layer: model 1 torch.Size([128, 64, 9])
shape of previous transport map torch.Size([64, 64])
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
returns a uniform measure of cardinality:  128
returns a uniform measure of cardinality:  128
the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
Ratio of trace to the matrix sum:  tensor(0.0391, device='cuda:0')
Here, trace is 4.999936103820801 and matrix sum is 127.99836730957031 
this is past correction for weight mode
Shape of aligned wt is  torch.Size([128, 64, 9])
Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])
Previous layer shape is  torch.Size([128, 64, 3, 3])
shape of layer: model 0 torch.Size([256, 128, 9])
shape of layer: model 1 torch.Size([256, 128, 9])
shape of previous transport map torch.Size([128, 128])
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
returns a uniform measure of cardinality:  256
returns a uniform measure of cardinality:  256
the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')
Here, trace is 0.0 and matrix sum is 255.99343872070312 
this is past correction for weight mode
Shape of aligned wt is  torch.Size([256, 128, 9])
Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])
Previous layer shape is  torch.Size([256, 128, 3, 3])
shape of layer: model 0 torch.Size([256, 256, 9])
shape of layer: model 1 torch.Size([256, 256, 9])
shape of previous transport map torch.Size([256, 256])
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
returns a uniform measure of cardinality:  256
returns a uniform measure of cardinality:  256
the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
Ratio of trace to the matrix sum:  tensor(0.0117, device='cuda:0')
Here, trace is 2.9999232292175293 and matrix sum is 255.99343872070312 
this is past correction for weight mode
Shape of aligned wt is  torch.Size([256, 256, 9])
Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])
Previous layer shape is  torch.Size([256, 256, 3, 3])
shape of layer: model 0 torch.Size([512, 256, 9])
shape of layer: model 1 torch.Size([512, 256, 9])
shape of previous transport map torch.Size([256, 256])
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
returns a uniform measure of cardinality:  512
returns a uniform measure of cardinality:  512
the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')
Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 
this is past correction for weight mode
Shape of aligned wt is  torch.Size([512, 256, 9])
Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])
Previous layer shape is  torch.Size([512, 256, 3, 3])
shape of layer: model 0 torch.Size([512, 512, 9])
shape of layer: model 1 torch.Size([512, 512, 9])
shape of previous transport map torch.Size([512, 512])
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
returns a uniform measure of cardinality:  512
returns a uniform measure of cardinality:  512
the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')
Here, trace is 0.0 and matrix sum is 511.97381591796875 
this is past correction for weight mode
Shape of aligned wt is  torch.Size([512, 512, 9])
Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])
Previous layer shape is  torch.Size([512, 512, 3, 3])
shape of layer: model 0 torch.Size([512, 512, 9])
shape of layer: model 1 torch.Size([512, 512, 9])
shape of previous transport map torch.Size([512, 512])
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
returns a uniform measure of cardinality:  512
returns a uniform measure of cardinality:  512
the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
Ratio of trace to the matrix sum:  tensor(0.0059, device='cuda:0')
Here, trace is 2.9998464584350586 and matrix sum is 511.97381591796875 
this is past correction for weight mode
Shape of aligned wt is  torch.Size([512, 512, 9])
Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])
Previous layer shape is  torch.Size([512, 512, 3, 3])
shape of layer: model 0 torch.Size([512, 512, 9])
shape of layer: model 1 torch.Size([512, 512, 9])
shape of previous transport map torch.Size([512, 512])
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
returns a uniform measure of cardinality:  512
returns a uniform measure of cardinality:  512
the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')
Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')
Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 
this is past correction for weight mode
Shape of aligned wt is  torch.Size([512, 512, 9])
Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])
Previous layer shape is  torch.Size([512, 512, 3, 3])
shape of layer: model 0 torch.Size([10, 512])
shape of layer: model 1 torch.Size([10, 512])
shape of previous transport map torch.Size([512, 512])
Processing the coordinates to form ground_metric
dont leave off the squaring of the ground metric
ground metric is  tensor([[1.6642, 2.5029, 2.7458, 2.8476, 2.6476, 2.8511, 2.7391, 2.6419, 2.5849,
         2.5815],
        [2.4276, 1.4904, 2.6829, 2.5579, 2.7686, 2.5049, 2.4092, 2.5224, 2.3121,
         2.4361],
        [2.7418, 2.6812, 1.5030, 2.9236, 2.9285, 2.8685, 2.7146, 2.8069, 2.7496,
         2.7785],
        [2.8253, 2.6431, 2.9114, 1.7466, 2.9005, 2.8518, 2.7576, 2.7891, 2.7822,
         2.6045],
        [2.7659, 2.5396, 2.8283, 2.7942, 1.6583, 2.7929, 2.8016, 2.7036, 2.6801,
         2.7054],
        [2.8964, 2.5805, 2.9760, 2.9360, 2.7100, 1.5265, 2.7519, 2.7181, 2.7447,
         2.6453],
        [2.6492, 2.4401, 2.6981, 2.8682, 2.6679, 2.9072, 1.4653, 2.6803, 2.4996,
         2.5470],
        [2.7200, 2.5388, 2.7826, 2.7789, 2.6412, 2.6688, 2.6847, 1.5186, 2.5994,
         2.5427],
        [2.5940, 2.3419, 2.7475, 2.7111, 2.7295, 2.8373, 2.5433, 2.5943, 1.3688,
         2.3831],
        [2.6033, 2.2430, 2.7676, 2.7020, 2.6355, 2.7555, 2.5522, 2.5339, 2.5315,
         1.4286]], device='cuda:0', grad_fn=<PowBackward0>)
returns a uniform measure of cardinality:  10
returns a uniform measure of cardinality:  10
the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,
         0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.1000]], device='cuda:0')
Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')
Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 
this is past correction for weight mode
Shape of aligned wt is  torch.Size([10, 512])
Shape of fc_layer0_weight_data is  torch.Size([10, 512])
using independent method
in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]
Relu Inplace is  False
model parameters are 
 [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]

--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0230, Accuracy: 1050/10000 (10%)

len of model parameters and avg aligned layers is  9 9
len of model_state_dict is  9
len of param_list is  9

--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0173, Accuracy: 6211/10000 (62%)

Timer ends
Time taken for geometric ensembling is 8.746873122174293 seconds
------- Prediction based ensembling -------

Test set: Avg. loss: 0.0086, Accuracy: 7255/10000 (73%)

------- Naive ensembling of weights -------
[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]
torch.Size([64, 3, 3, 3])
[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]
torch.Size([128, 64, 3, 3])
[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]
torch.Size([256, 128, 3, 3])
[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]
torch.Size([256, 256, 3, 3])
[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]
torch.Size([512, 256, 3, 3])
[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]
torch.Size([512, 512, 3, 3])
[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]
torch.Size([512, 512, 3, 3])
[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]
torch.Size([512, 512, 3, 3])
[torch.Size([10, 512]), torch.Size([10, 512])]
torch.Size([10, 512])
in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]
Relu Inplace is  False
model parameters are 
 [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]

--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0230, Accuracy: 1000/10000 (10%)


--------- Testing in global mode ---------
size of test_loader dataset:  10000

Test set: Avg. loss: 0.0230, Accuracy: 1994/10000 (20%)

----- Saved results at sample_pruned_30_structured_vgg11.csv ------
{'exp_name': 'exp_2024-01-05_16-44-06_059512', 'model0_acc': 90.30999821424489, 'model1_acc': 90.4999980330467, 'geometric_acc': 62.11, 'prediction_acc': 72.55, 'naive_acc': 19.94, 'geometric_gain': -28.389998033046695, 'geometric_gain_%': -31.370164254235554, 'prediction_gain': -17.949998033046697, 'prediction_gain_%': -19.834252401300752, 'relative_loss_wrt_prediction': 11.535911852934802, 'geometric_time': 8.746873122174293}
FYI: the parameters were: 
 Namespace(n_epochs=300, batch_size_train=64, batch_size_test=1000, learning_rate=0.01, momentum=0.5, log_interval=100, to_download=False, disable_bias=True, dataset='Cifar10', num_models=2, model_name='vgg11_nobias', config_file=None, config_dir='/home/thmeier/otfusion/exp_sample_pruned_30_structured_vgg11/configurations', num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, sweep_id=90, gpu_id=0, skip_last_layer=False, skip_last_layer_type='average', debug=False, cifar_style_data=False, activation_histograms=False, act_num_samples=100, softmax_temperature=1, activation_mode=None, options_type='generic', deprecated=None, save_result_file='sample_pruned_30_structured_vgg11.csv', sweep_name='exp_sample_pruned_30_structured_vgg11', reg=0.01, reg_m=0.001, ground_metric='euclidean', ground_metric_normalize='none', not_squared=True, clip_gm=False, clip_min=0, clip_max=5, tmap_stats=False, ensemble_step=0.5, ground_metric_eff=True, retrain=0, retrain_lr_decay=-1, retrain_lr_decay_factor=None, retrain_lr_decay_epochs=None, retrain_avg_only=False, retrain_geometric_only=False, load_models='./cifar_models/', ckpt_type='best', recheck_cifar=True, recheck_acc=False, eval_aligned=False, enable_dropout=False, dump_model=False, dump_final_models=False, correction=True, activation_seed=21, weight_stats=True, sinkhorn_type='normal', geom_ensemble_type='wts', act_bug=False, standardize_acts=False, transform_acts=False, center_acts=False, prelu_acts=True, pool_acts=False, pool_relu=False, normalize_acts=False, normalize_wts=False, gromov=False, gromov_loss='square_loss', tensorboard_root='./tensorboard', tensorboard=False, same_model=-1, dist_normalize=False, update_acts=False, past_correction=True, partial_reshape=False, choice='0 2 4 6 8', diff_init=False, partition_type='labels', personal_class_idx=9, partition_dataloader=-1, personal_split_frac=0.1, exact=True, skip_personal_idx=False, prediction_wts=False, width_ratio=1, proper_marginals=False, retrain_seed=-1, no_random_trainloaders=False, reinit_trainloaders=False, second_model_name=None, print_distances=False, deterministic=False, skip_retrain=-1, importance=None, unbalanced=False, temperature=20, alpha=0.7, dist_epochs=60, handle_skips=False, prune_frac=0.3, prune_type='structured', timestamp='2024-01-05_16-44-06_059512', rootdir='/home/thmeier/otfusion/exp_sample_pruned_30_structured_vgg11', baseroot='/home/thmeier/otfusion', result_dir='/home/thmeier/otfusion/exp_sample_pruned_30_structured_vgg11/results', exp_name='exp_2024-01-05_16-44-06_059512', csv_dir='/home/thmeier/otfusion/exp_sample_pruned_30_structured_vgg11/csv', config={'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 300, 'seed': 42}, second_config={'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 300, 'seed': 42}, cifar_init_lr=0.05, activation_time=5.258992314338684e-05, params_model_0=9222848, params_model_1=9222848, geometric_time=8.746873122174293, params_geometric=9222848, **{'trace_sum_ratio_features.0.weight': 0.03125, 'trace_sum_ratio_features.3.weight': 0.0390625, 'trace_sum_ratio_features.6.weight': 0.0, 'trace_sum_ratio_features.8.weight': 0.011718750931322575, 'trace_sum_ratio_features.11.weight': 0.001953125, 'trace_sum_ratio_features.13.weight': 0.0, 'trace_sum_ratio_features.16.weight': 0.005859374534338713, 'trace_sum_ratio_features.18.weight': 0.001953125, 'trace_sum_ratio_classifier.weight': 1.0})
